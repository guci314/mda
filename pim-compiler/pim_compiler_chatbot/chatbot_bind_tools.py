#!/usr/bin/env python3
"""
PIM Compiler Chatbot - Bind Tools ç‰ˆæœ¬
ä½¿ç”¨ LangChain çš„ bind_tools æ–¹æ³•ï¼Œè¿™æ˜¯æœ€æ–°æ¨èçš„æ–¹å¼
"""

import os
import sys
from pathlib import Path
from typing import Dict, Any, Optional, List, Literal
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, ToolMessage
from langchain_core.tools import tool
from langchain_core.runnables import RunnablePassthrough
from pydantic import BaseModel, Field

from pim_compiler_chatbot.chatbot import PIMCompilerTools


# å®šä¹‰å·¥å…·çš„å‚æ•°æ¨¡å‹
class SearchPIMFiles(BaseModel):
    """æœç´¢ PIM æ–‡ä»¶çš„å‚æ•°"""
    query: str = Field(description="æœç´¢å…³é”®è¯ï¼Œå¦‚'åšå®¢'ã€'åšå®¢ç³»ç»Ÿ'ã€'åŒ»é™¢'ã€'ç”¨æˆ·ç®¡ç†'ç­‰")


class CompilePIM(BaseModel):
    """ç¼–è¯‘ PIM æ–‡ä»¶çš„å‚æ•°"""
    file_path: str = Field(description="PIMæ–‡ä»¶è·¯å¾„ï¼Œå¦‚ 'examples/blog.md'")


class CheckLog(BaseModel):
    """æŸ¥çœ‹ç¼–è¯‘æ—¥å¿—çš„å‚æ•°"""
    system_name: Optional[str] = Field(default=None, description="ç³»ç»Ÿåç§°ï¼Œå¦‚'blog'ã€'hospital'ã€‚ç•™ç©ºæ˜¾ç¤ºæ‰€æœ‰æ—¥å¿—")


class StopCompilation(BaseModel):
    """ç»ˆæ­¢ç¼–è¯‘çš„å‚æ•°"""
    system_name: Optional[str] = Field(default=None, description="è¦ç»ˆæ­¢çš„ç³»ç»Ÿåç§°ã€‚ç•™ç©ºç»ˆæ­¢æ‰€æœ‰")


# åˆå§‹åŒ–å·¥å…·å®ä¾‹ï¼Œç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„è·¯å¾„
# è·å– pim-compiler ç›®å½•è·¯å¾„ï¼ˆchatbot.py çš„çˆ¶ç›®å½•ï¼‰
pim_compiler_path = Path(__file__).parent.parent
tools_instance = PIMCompilerTools(pim_compiler_path=str(pim_compiler_path))


# ä½¿ç”¨ @tool è£…é¥°å™¨å®šä¹‰å·¥å…·
@tool
def search_pim_files(query: str) -> str:
    """æœç´¢ PIMï¼ˆå¹³å°æ— å…³æ¨¡å‹ï¼‰æ–‡ä»¶ã€‚æ”¯æŒä¸­æ–‡å’Œè‹±æ–‡å…³é”®è¯ã€‚"""
    return tools_instance.search_pim_files(query)


@tool
def compile_pim(file_path: str) -> str:
    """ç¼–è¯‘æŒ‡å®šçš„ PIM æ–‡ä»¶ï¼Œç”Ÿæˆå¯æ‰§è¡Œä»£ç ã€‚"""
    return tools_instance.compile_pim(file_path)


@tool
def check_log(system_name: Optional[str] = None) -> str:
    """æŸ¥çœ‹ç¼–è¯‘æ—¥å¿—å’Œè¿›åº¦ã€‚å¯ä»¥æŒ‡å®šç³»ç»Ÿåç§°ï¼Œæˆ–ç•™ç©ºæŸ¥çœ‹æ‰€æœ‰ã€‚"""
    return tools_instance.check_log(system_name)


@tool
def list_projects() -> str:
    """åˆ—å‡ºæ‰€æœ‰å·²ç¼–è¯‘çš„é¡¹ç›®ã€‚"""
    return tools_instance.list_compiled_projects("")


@tool
def stop_compilation(system_name: Optional[str] = None) -> str:
    """ç»ˆæ­¢æ­£åœ¨è¿è¡Œçš„ç¼–è¯‘è¿›ç¨‹ã€‚"""
    return tools_instance.stop_compilation(system_name)


class PIMCompilerAgent:
    """ä½¿ç”¨ bind_tools çš„ PIM ç¼–è¯‘å™¨æ™ºèƒ½ä½“"""
    
    def __init__(self, llm_config: Optional[Dict[str, Any]] = None):
        # é…ç½® LLM
        if llm_config is None:
            llm_config = {"model": "gpt-3.5-turbo", "temperature": 0.3}
        
        # å‚æ•°åè½¬æ¢
        if "openai_api_key" in llm_config:
            llm_config["api_key"] = llm_config.pop("openai_api_key")
        if "openai_api_base" in llm_config:
            llm_config["base_url"] = llm_config.pop("openai_api_base")
        
        # åˆ›å»º LLM
        self.llm = ChatOpenAI(**llm_config)
        
        # å®šä¹‰å·¥å…·
        self.tools = [
            search_pim_files,
            compile_pim,
            check_log,
            list_projects,
            stop_compilation
        ]
        
        # å°†å·¥å…·ç»‘å®šåˆ° LLM
        self.llm_with_tools = self.llm.bind_tools(self.tools)
        
        # ç³»ç»Ÿæç¤º
        self.system_prompt = """ä½ æ˜¯ PIM ç¼–è¯‘å™¨åŠ©æ‰‹ï¼Œä¸“é—¨å¸®åŠ©ç”¨æˆ·ç¼–è¯‘ PIMï¼ˆå¹³å°æ— å…³æ¨¡å‹ï¼‰æ–‡ä»¶ã€‚

ä½ çš„èƒ½åŠ›åŒ…æ‹¬ï¼š
1. æœç´¢ PIM æ–‡ä»¶ - æ”¯æŒä¸­æ–‡å’Œè‹±æ–‡å…³é”®è¯
2. ç¼–è¯‘ PIM æ–‡ä»¶ - å°†æ¨¡å‹è½¬æ¢ä¸ºå¯æ‰§è¡Œä»£ç 
3. æŸ¥çœ‹ç¼–è¯‘æ—¥å¿— - å®æ—¶ç›‘æ§ç¼–è¯‘è¿›åº¦
4. ç®¡ç†ç¼–è¯‘é¡¹ç›® - åˆ—å‡ºå’Œç®¡ç†å·²ç¼–è¯‘çš„é¡¹ç›®
5. ç»ˆæ­¢ç¼–è¯‘è¿›ç¨‹ - åœæ­¢æ­£åœ¨è¿è¡Œçš„ç¼–è¯‘ä»»åŠ¡

å·¥ä½œæµç¨‹ï¼š
- å½“ç”¨æˆ·è¯´"ç¼–è¯‘XXç³»ç»Ÿ"æ—¶ï¼Œå…ˆæœç´¢ç›¸å…³æ–‡ä»¶ï¼Œæ‰¾åˆ°åå†ç¼–è¯‘
- ç¼–è¯‘å¯åŠ¨åï¼Œæé†’ç”¨æˆ·å¯ä»¥æŸ¥çœ‹æ—¥å¿—
- ç”¨æˆ·å¯èƒ½ä½¿ç”¨å„ç§è¡¨è¾¾æ–¹å¼ï¼Œè¦çµæ´»ç†è§£æ„å›¾

æ³¨æ„ï¼š
- æ€»æ˜¯å…ˆæœç´¢æ–‡ä»¶ï¼Œç¡®è®¤æ–‡ä»¶å­˜åœ¨åå†ç¼–è¯‘
- æä¾›æ¸…æ™°ã€å‹å¥½çš„åé¦ˆ
- å¦‚æœæ“ä½œå¤±è´¥ï¼Œè§£é‡ŠåŸå› å¹¶æä¾›å»ºè®®"""
        
        # å¯¹è¯å†å²
        self.messages: List[Any] = [
            {"role": "system", "content": self.system_prompt}
        ]
    
    def _execute_tool_calls(self, tool_calls):
        """æ‰§è¡Œå·¥å…·è°ƒç”¨"""
        results = []
        for tool_call in tool_calls:
            tool_name = tool_call["name"]
            tool_args = tool_call["args"]
            
            # æ‰¾åˆ°å¯¹åº”çš„å·¥å…·
            tool_func = None
            for tool in self.tools:
                if tool.name == tool_name:
                    tool_func = tool
                    break
            
            if tool_func:
                try:
                    # æ‰§è¡Œå·¥å…·
                    result = tool_func.invoke(tool_args)
                    results.append({
                        "tool_call_id": tool_call["id"],
                        "content": result
                    })
                except Exception as e:
                    results.append({
                        "tool_call_id": tool_call["id"],
                        "content": f"å·¥å…·æ‰§è¡Œé”™è¯¯: {str(e)}"
                    })
            else:
                results.append({
                    "tool_call_id": tool_call["id"],
                    "content": f"æœªæ‰¾åˆ°å·¥å…·: {tool_name}"
                })
        
        return results
    
    def chat(self, message: str) -> str:
        """å¤„ç†ç”¨æˆ·æ¶ˆæ¯"""
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        self.messages.append({"role": "user", "content": message})
        
        # è°ƒç”¨ LLM
        response = self.llm_with_tools.invoke(self.messages)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
        if hasattr(response, 'tool_calls') and response.tool_calls:
            # æ·»åŠ åŠ©æ‰‹çš„å·¥å…·è°ƒç”¨æ¶ˆæ¯
            self.messages.append(response)
            
            # æ‰§è¡Œå·¥å…·è°ƒç”¨
            tool_results = self._execute_tool_calls(response.tool_calls)
            
            # æ·»åŠ å·¥å…·ç»“æœ
            for result in tool_results:
                tool_message = ToolMessage(
                    content=result["content"],
                    tool_call_id=result["tool_call_id"]
                )
                self.messages.append(tool_message)
            
            # å†æ¬¡è°ƒç”¨ LLM ç”Ÿæˆæœ€ç»ˆå“åº”
            final_response = self.llm_with_tools.invoke(self.messages)
            self.messages.append(final_response)
            
            return final_response.content
        else:
            # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç›´æ¥è¿”å›å“åº”
            self.messages.append(response)
            return response.content
    
    def clear_history(self):
        """æ¸…é™¤å¯¹è¯å†å²"""
        self.messages = [
            {"role": "system", "content": self.system_prompt}
        ]


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¤– PIM ç¼–è¯‘å™¨åŠ©æ‰‹ - Bind Tools ç‰ˆæœ¬")
    print("=" * 60)
    print("ä½¿ç”¨æœ€æ–°çš„ bind_tools APIï¼Œè‡ªåŠ¨å¤„ç†å‚æ•°è§£æ")
    print("æ”¯æŒä¸­æ–‡å…³é”®è¯ï¼Œå¦‚ï¼š'ç¼–è¯‘åšå®¢ç³»ç»Ÿ'ã€'æŸ¥çœ‹æ—¥å¿—' ç­‰")
    print()
    
    # é…ç½® LLM
    deepseek_key = os.getenv("DEEPSEEK_API_KEY")
    if deepseek_key:
        llm_config = {
            "api_key": deepseek_key,
            "base_url": "https://api.deepseek.com/v1",
            "model": "deepseek-chat",
            "temperature": 0.3
        }
        print("âœ… ä½¿ç”¨ DeepSeek æ¨¡å‹\n")
    else:
        print("âš ï¸  æœªè®¾ç½® DEEPSEEK_API_KEY\n")
        return
    
    # åˆ›å»ºæ™ºèƒ½ä½“
    agent = PIMCompilerAgent(llm_config)
    
    print("è¾“å…¥ 'exit' é€€å‡ºï¼Œ'clear' æ¸…é™¤å†å²")
    print("=" * 60)
    
    # å¯ç”¨å‘½ä»¤å†å²
    try:
        import readline
        history_file = Path.home() / ".pim_compiler_history"
        if history_file.exists():
            readline.read_history_file(str(history_file))
        
        import atexit
        atexit.register(readline.write_history_file, str(history_file))
    except ImportError:
        pass
    
    # äº¤äº’å¾ªç¯
    while True:
        try:
            user_input = input("\nä½ : ").strip()
            
            if user_input.lower() in ['exit', 'quit']:
                print("\nğŸ‘‹ å†è§ï¼")
                break
            
            if user_input.lower() == 'clear':
                agent.clear_history()
                print("âœ… å¯¹è¯å†å²å·²æ¸…é™¤")
                continue
            
            if not user_input:
                continue
            
            # è·å–å“åº”
            response = agent.chat(user_input)
            print(f"\nåŠ©æ‰‹: {response}")
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ å†è§ï¼")
            break
        except Exception as e:
            print(f"\nâŒ å‡ºé”™äº†: {str(e)}")
            import traceback
            traceback.print_exc()


if __name__ == "__main__":
    main()