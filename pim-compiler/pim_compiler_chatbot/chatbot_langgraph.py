#!/usr/bin/env python3
"""
PIM Compiler Chatbot - LangGraph ç‰ˆæœ¬
ä½¿ç”¨ LangGraph å®ç°æ›´å¤æ‚çš„å·¥ä½œæµå’ŒçŠ¶æ€ç®¡ç†
"""

import os
import sys
from pathlib import Path
from typing import Dict, Any, Optional, List, TypedDict, Annotated
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from langchain_openai import ChatOpenAI
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langchain.tools import Tool
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolExecutor, ToolInvocation
from langgraph.graph.message import add_messages

from pim_compiler_chatbot.chatbot import PIMCompilerTools


# å®šä¹‰çŠ¶æ€
class AgentState(TypedDict):
    """Agent çš„çŠ¶æ€å®šä¹‰"""
    messages: Annotated[List[BaseMessage], add_messages]
    current_task: str
    compilation_status: Dict[str, Any]
    error_count: int


class PIMCompilerGraph:
    """åŸºäº LangGraph çš„ PIM ç¼–è¯‘å™¨åŠ©æ‰‹"""
    
    def __init__(self, llm_config: Optional[Dict[str, Any]] = None):
        self.tools_instance = PIMCompilerTools()
        self.setup_tools()
        self.setup_llm(llm_config)
        self.build_graph()
    
    def setup_tools(self):
        """è®¾ç½®å·¥å…·"""
        self.tools = [
            Tool(
                name="search_pim_files",
                func=lambda q: self.tools_instance.search_pim_files(q.strip()),
                description="æœç´¢ PIM æ–‡ä»¶"
            ),
            Tool(
                name="compile_pim",
                func=lambda f: self.tools_instance.compile_pim(f.strip()),
                description="ç¼–è¯‘ PIM æ–‡ä»¶"
            ),
            Tool(
                name="check_log",
                func=lambda s: self.tools_instance.check_log(s.strip() if s and s.strip() else None),
                description="æŸ¥çœ‹ç¼–è¯‘æ—¥å¿—"
            ),
            Tool(
                name="list_projects",
                func=lambda _: self.tools_instance.list_compiled_projects(""),
                description="åˆ—å‡ºé¡¹ç›®"
            ),
            Tool(
                name="stop_compilation",
                func=lambda s: self.tools_instance.stop_compilation(s.strip() if s and s.strip() else None),
                description="ç»ˆæ­¢ç¼–è¯‘"
            )
        ]
        
        self.tool_executor = ToolExecutor(self.tools)
        self.tools_by_name = {tool.name: tool for tool in self.tools}
    
    def setup_llm(self, llm_config: Optional[Dict[str, Any]]):
        """è®¾ç½® LLM"""
        if llm_config is None:
            llm_config = {"model": "gpt-3.5-turbo", "temperature": 0.3}
        
        # å‚æ•°åè½¬æ¢
        if "openai_api_key" in llm_config:
            llm_config["api_key"] = llm_config.pop("openai_api_key")
        if "openai_api_base" in llm_config:
            llm_config["base_url"] = llm_config.pop("openai_api_base")
        
        self.llm = ChatOpenAI(**llm_config)
    
    def build_graph(self):
        """æ„å»º LangGraph å·¥ä½œæµ"""
        workflow = StateGraph(AgentState)
        
        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("analyze", self.analyze_request)
        workflow.add_node("plan", self.plan_actions)
        workflow.add_node("execute", self.execute_tools)
        workflow.add_node("check_result", self.check_result)
        workflow.add_node("respond", self.generate_response)
        
        # è®¾ç½®å…¥å£
        workflow.set_entry_point("analyze")
        
        # æ·»åŠ è¾¹
        workflow.add_edge("analyze", "plan")
        workflow.add_conditional_edges(
            "plan",
            self.should_execute,
            {
                "execute": "execute",
                "respond": "respond"
            }
        )
        workflow.add_edge("execute", "check_result")
        workflow.add_conditional_edges(
            "check_result",
            self.should_continue,
            {
                "plan": "plan",
                "respond": "respond"
            }
        )
        workflow.add_edge("respond", END)
        
        # ç¼–è¯‘å›¾
        self.app = workflow.compile()
    
    def analyze_request(self, state: AgentState) -> AgentState:
        """åˆ†æç”¨æˆ·è¯·æ±‚"""
        messages = state["messages"]
        last_message = messages[-1].content if messages else ""
        
        # ä½¿ç”¨ LLM åˆ†ææ„å›¾
        analysis_prompt = f"""åˆ†æç”¨æˆ·è¯·æ±‚å¹¶è¯†åˆ«ä»»åŠ¡ç±»å‹ï¼š

ç”¨æˆ·è¯·æ±‚ï¼š{last_message}

ä»»åŠ¡ç±»å‹åŒ…æ‹¬ï¼š
1. search - æœç´¢æ–‡ä»¶
2. compile - ç¼–è¯‘ç³»ç»Ÿ
3. check_log - æŸ¥çœ‹æ—¥å¿—
4. list - åˆ—å‡ºé¡¹ç›®
5. stop - ç»ˆæ­¢ç¼–è¯‘
6. complex - éœ€è¦å¤šæ­¥éª¤

è¿”å›æ ¼å¼ï¼šä»»åŠ¡ç±»å‹|å…³é”®ä¿¡æ¯
ä¾‹å¦‚ï¼šcompile|åšå®¢ç³»ç»Ÿ"""
        
        response = self.llm.invoke([HumanMessage(content=analysis_prompt)])
        task_info = response.content.strip()
        
        state["current_task"] = task_info
        return state
    
    def plan_actions(self, state: AgentState) -> AgentState:
        """è®¡åˆ’è¦æ‰§è¡Œçš„åŠ¨ä½œ"""
        task_info = state["current_task"]
        messages = state["messages"]
        
        # æ ¹æ®ä»»åŠ¡ç±»å‹è®¡åˆ’åŠ¨ä½œ
        task_type = task_info.split("|")[0] if "|" in task_info else "unknown"
        
        planning_prompt = f"""åŸºäºä»»åŠ¡ç±»å‹ '{task_type}'ï¼Œè®¡åˆ’éœ€è¦è°ƒç”¨çš„å·¥å…·ã€‚

å¯ç”¨å·¥å…·ï¼š
- search_pim_files: æœç´¢æ–‡ä»¶
- compile_pim: ç¼–è¯‘æ–‡ä»¶
- check_log: æŸ¥çœ‹æ—¥å¿—
- list_projects: åˆ—å‡ºé¡¹ç›®
- stop_compilation: ç»ˆæ­¢ç¼–è¯‘

ç”¨æˆ·è¯·æ±‚ï¼š{messages[-1].content if messages else ''}

è¿”å›æ ¼å¼ï¼šå·¥å…·å|å‚æ•°
å¦‚æœä¸éœ€è¦å·¥å…·ï¼Œè¿”å›ï¼šnone|none"""
        
        response = self.llm.invoke([HumanMessage(content=planning_prompt)])
        plan = response.content.strip()
        
        messages.append(AIMessage(content=f"è®¡åˆ’æ‰§è¡Œï¼š{plan}"))
        state["messages"] = messages
        return state
    
    def should_execute(self, state: AgentState) -> str:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦æ‰§è¡Œå·¥å…·"""
        last_message = state["messages"][-1].content
        if "none|none" in last_message:
            return "respond"
        return "execute"
    
    def execute_tools(self, state: AgentState) -> AgentState:
        """æ‰§è¡Œå·¥å…·"""
        messages = state["messages"]
        last_message = messages[-1].content
        
        # è§£æå·¥å…·è°ƒç”¨
        if "|" in last_message:
            parts = last_message.split("|")
            tool_name = parts[0].split("ï¼š")[-1].strip()
            tool_input = parts[1].strip() if len(parts) > 1 else ""
            
            if tool_name in self.tools_by_name:
                # æ‰§è¡Œå·¥å…·
                tool = self.tools_by_name[tool_name]
                try:
                    result = tool.func(tool_input)
                    messages.append(AIMessage(content=f"å·¥å…·æ‰§è¡Œç»“æœï¼š\n{result}"))
                except Exception as e:
                    messages.append(AIMessage(content=f"å·¥å…·æ‰§è¡Œé”™è¯¯ï¼š{str(e)}"))
                    state["error_count"] = state.get("error_count", 0) + 1
        
        state["messages"] = messages
        return state
    
    def check_result(self, state: AgentState) -> AgentState:
        """æ£€æŸ¥æ‰§è¡Œç»“æœ"""
        # è¿™é‡Œå¯ä»¥æ·»åŠ ç»“æœéªŒè¯é€»è¾‘
        return state
    
    def should_continue(self, state: AgentState) -> str:
        """åˆ¤æ–­æ˜¯å¦ç»§ç»­æ‰§è¡Œ"""
        # å¦‚æœæœ‰é”™è¯¯ä¸”é”™è¯¯æ¬¡æ•°è¿‡å¤šï¼Œåœæ­¢
        if state.get("error_count", 0) > 3:
            return "respond"
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ç»§ç»­æ‰§è¡Œ
        messages = state["messages"]
        if any("ç¼–è¯‘å·²å¯åŠ¨" in msg.content for msg in messages[-3:]):
            return "respond"
        
        return "respond"  # é»˜è®¤å“åº”
    
    def generate_response(self, state: AgentState) -> AgentState:
        """ç”Ÿæˆæœ€ç»ˆå“åº”"""
        messages = state["messages"]
        
        # æå–å…³é”®ä¿¡æ¯ç”Ÿæˆå“åº”
        summary_prompt = f"""åŸºäºä»¥ä¸‹å¯¹è¯å†å²ï¼Œç”Ÿæˆç®€æ´çš„å“åº”ï¼š

{chr(10).join([f"{msg.type}: {msg.content}" for msg in messages[-5:]])}

ç”Ÿæˆç”¨æˆ·å‹å¥½çš„å“åº”ï¼š"""
        
        response = self.llm.invoke([HumanMessage(content=summary_prompt)])
        messages.append(AIMessage(content=response.content))
        
        state["messages"] = messages
        return state
    
    def chat(self, user_input: str, history: List[BaseMessage] = None) -> str:
        """å¤„ç†ç”¨æˆ·è¾“å…¥"""
        if history is None:
            history = []
        
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        history.append(HumanMessage(content=user_input))
        
        # åˆå§‹çŠ¶æ€
        initial_state = {
            "messages": history,
            "current_task": "",
            "compilation_status": {},
            "error_count": 0
        }
        
        # è¿è¡Œå·¥ä½œæµ
        final_state = self.app.invoke(initial_state)
        
        # è¿”å›æœ€åçš„ AI å“åº”
        for msg in reversed(final_state["messages"]):
            if isinstance(msg, AIMessage) and not msg.content.startswith("è®¡åˆ’æ‰§è¡Œï¼š"):
                return msg.content
        
        return "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•å¤„ç†æ‚¨çš„è¯·æ±‚ã€‚"


def create_langgraph_agent(llm_config: Optional[Dict[str, Any]] = None):
    """åˆ›å»º LangGraph Agent"""
    graph = PIMCompilerGraph(llm_config)
    
    # åˆ›å»ºä¸€ä¸ªå…¼å®¹ AgentExecutor æ¥å£çš„åŒ…è£…å™¨
    class GraphWrapper:
        def __init__(self, graph):
            self.graph = graph
            self.history = []
        
        def invoke(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
            user_input = inputs.get("input", "")
            response = self.graph.chat(user_input, self.history)
            self.history.append(HumanMessage(content=user_input))
            self.history.append(AIMessage(content=response))
            return {"output": response}
    
    return GraphWrapper(graph)


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¤– PIM ç¼–è¯‘å™¨åŠ©æ‰‹ - LangGraph ç‰ˆæœ¬")
    print("=" * 60)
    print("è¿™æ˜¯ä¸€ä¸ªåŸºäº LangGraph çš„é«˜çº§ç‰ˆæœ¬ï¼Œæä¾›ï¼š")
    print("- æ›´å¥½çš„çŠ¶æ€ç®¡ç†")
    print("- é”™è¯¯å¤„ç†å’Œé‡è¯•")
    print("- å·¥ä½œæµå¯è§†åŒ–")
    print()
    
    # é…ç½® LLM
    deepseek_key = os.getenv("DEEPSEEK_API_KEY")
    if deepseek_key:
        llm_config = {
            "api_key": deepseek_key,
            "base_url": "https://api.deepseek.com/v1",
            "model": "deepseek-chat",
            "temperature": 0.3
        }
        print("âœ… ä½¿ç”¨ DeepSeek æ¨¡å‹\n")
    else:
        print("âš ï¸  æœªè®¾ç½® DEEPSEEK_API_KEY\n")
        return
    
    # åˆ›å»º agent
    agent = create_langgraph_agent(llm_config)
    
    print("è¾“å…¥ 'exit' é€€å‡º\n")
    
    # äº¤äº’å¾ªç¯
    while True:
        try:
            user_input = input("\nä½ : ").strip()
            
            if user_input.lower() in ['exit', 'quit']:
                print("\nğŸ‘‹ å†è§ï¼")
                break
            
            if not user_input:
                continue
            
            # æ‰§è¡Œ
            result = agent.invoke({"input": user_input})
            print(f"\nåŠ©æ‰‹: {result['output']}")
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ å†è§ï¼")
            break
        except Exception as e:
            print(f"\nâŒ å‡ºé”™äº†: {str(e)}")
            import traceback
            traceback.print_exc()


if __name__ == "__main__":
    main()