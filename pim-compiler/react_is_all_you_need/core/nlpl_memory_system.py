#!/usr/bin/env python3
"""
NLPL-based Memory System
åŸºäºæ–‡ä»¶ç³»ç»Ÿå’ŒNLPLçš„è®¤çŸ¥è®°å¿†ç³»ç»Ÿ
"""

import os
import json
import shutil
import hashlib
import subprocess
from pathlib import Path
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, asdict
import re

@dataclass
class MemoryEvent:
    """è®°å¿†äº‹ä»¶"""
    timestamp: str
    task_name: str
    task_type: str
    execution_rounds: int
    success: bool
    key_patterns: List[str]
    emotional_markers: Dict[str, float]
    cognitive_load: float
    innovations: List[str]
    errors: List[str]

class NLPLMemorySystem:
    """NLPLè®°å¿†ç³»ç»Ÿ"""
    
    def __init__(self, memory_dir: str = ".memory"):
        self.memory_dir = Path(memory_dir)
        self._initialize_structure()
        
    def _initialize_structure(self):
        """åˆå§‹åŒ–è®°å¿†ç›®å½•ç»“æ„"""
        directories = [
            "episodic",
            "semantic/concepts",
            "semantic/patterns",
            "procedural/skills",
            "procedural/habits",
            "working",
            "metacognitive",
            "archive"
        ]
        
        for dir_path in directories:
            (self.memory_dir / dir_path).mkdir(parents=True, exist_ok=True)
            
        # åˆ›å»ºç´¢å¼•æ–‡ä»¶
        index_files = [
            "episodic/index.nlpl",
            "semantic/relations.nlpl",
            "procedural/proficiency.nlpl",
            "metacognitive/self_knowledge.nlpl"
        ]
        
        for index_file in index_files:
            file_path = self.memory_dir / index_file
            if not file_path.exists():
                file_path.write_text(f"# {index_file}\nç”Ÿæˆæ—¶é—´ï¼š{datetime.now().isoformat()}\n\n")
    
    # ========== æƒ…æ™¯è®°å¿† ==========
    
    def create_episodic_memory(self, event: MemoryEvent) -> Tuple[str, str, str]:
        """åˆ›å»ºä¸‰å±‚æ¸…æ™°åº¦çš„æƒ…æ™¯è®°å¿†"""
        timestamp = datetime.now()
        date_dir = self.memory_dir / "episodic" / timestamp.strftime("%Y-%m-%d")
        date_dir.mkdir(exist_ok=True)
        
        base_name = timestamp.strftime("%H-%M-%S") + f"_{self._sanitize_filename(event.task_name)}"
        
        # ç”Ÿæˆä¸‰ä¸ªç‰ˆæœ¬
        detailed_path = date_dir / f"{base_name}_detailed.nlpl"
        summary_path = date_dir / f"{base_name}_summary.nlpl"
        gist_path = date_dir / f"{base_name}_gist.nlpl"
        
        # è¯¦ç»†ç‰ˆæœ¬
        detailed_content = self._generate_detailed_memory(event)
        detailed_path.write_text(detailed_content, encoding='utf-8')
        
        # æ‘˜è¦ç‰ˆæœ¬
        summary_content = self._generate_summary_memory(event)
        summary_path.write_text(summary_content, encoding='utf-8')
        
        # è¦ç‚¹ç‰ˆæœ¬
        gist_content = self._generate_gist_memory(event)
        gist_path.write_text(gist_content, encoding='utf-8')
        
        # æ›´æ–°ç´¢å¼•
        self._update_episodic_index(event, detailed_path, summary_path, gist_path)
        
        return str(detailed_path), str(summary_path), str(gist_path)
    
    def _generate_detailed_memory(self, event: MemoryEvent) -> str:
        """ç”Ÿæˆè¯¦ç»†ç‰ˆæœ¬çš„è®°å¿†"""
        return f"""# ä»»åŠ¡ï¼š{event.task_name}
æ‰§è¡Œæ—¶é—´ï¼š{event.timestamp}
æ€»è½®æ•°ï¼š{event.execution_rounds}
æˆåŠŸçŠ¶æ€ï¼š{'æˆåŠŸ' if event.success else 'å¤±è´¥'}

## å®Œæ•´æ‰§è¡Œåˆ†æ

### ä»»åŠ¡ç±»å‹è¯†åˆ«
- **ç±»å‹**ï¼š{event.task_type}
- **å¤æ‚åº¦**ï¼š{self._assess_complexity(event)}
- **æ–°é¢–åº¦**ï¼š{self._assess_novelty(event)}

### æ‰§è¡Œæ¨¡å¼åˆ†æ
è¯†åˆ«çš„æ¨¡å¼ï¼š
{self._format_patterns(event.key_patterns)}

### è®¤çŸ¥è´Ÿè·è¯„ä¼°
- **æ€»ä½“è´Ÿè·**ï¼š{event.cognitive_load:.2f}
- **å³°å€¼æ—¶åˆ»**ï¼š{self._identify_peak_moments(event)}
- **èµ„æºä½¿ç”¨**ï¼š{self._assess_resource_usage(event)}

### æƒ…ç»ªæ ‡è®°
{self._format_emotions(event.emotional_markers)}

### åˆ›æ–°ä¸å‘ç°
{self._format_innovations(event.innovations)}

### é”™è¯¯ä¸æ¢å¤
{self._format_errors(event.errors)}

## è¯¦ç»†æ—¶é—´çº¿
[è¿™é‡Œåº”åŒ…å«å®Œæ•´çš„æ‰§è¡Œæ­¥éª¤ï¼Œç”±è§‚å¯ŸAgentå¡«å……]

## æ€§èƒ½æŒ‡æ ‡
- **æ•ˆç‡è¯„åˆ†**ï¼š{self._calculate_efficiency(event)}/10
- **åˆ›æ–°æŒ‡æ•°**ï¼š{len(event.innovations)}/10
- **ç¨³å®šæ€§**ï¼š{self._calculate_stability(event)}/10

## ç»éªŒæå–
{self._extract_lessons(event)}
"""
    
    def _generate_summary_memory(self, event: MemoryEvent) -> str:
        """ç”Ÿæˆæ‘˜è¦ç‰ˆæœ¬çš„è®°å¿†"""
        return f"""# ä»»åŠ¡ï¼š{event.task_name}
æ—¶é—´ï¼š{event.timestamp[:10]}
ç»“æœï¼š{'æˆåŠŸ' if event.success else 'å¤±è´¥'}ï¼ˆ{event.execution_rounds}è½®ï¼‰

## å…³é”®ä¿¡æ¯
- **ä»»åŠ¡ç±»å‹**ï¼š{event.task_type}
- **ä¸»è¦æ¨¡å¼**ï¼š{', '.join(event.key_patterns[:3])}
- **è®¤çŸ¥è´Ÿè·**ï¼š{'é«˜' if event.cognitive_load > 0.7 else 'ä¸­' if event.cognitive_load > 0.4 else 'ä½'}

## æ‰§è¡Œç‰¹å¾
{self._summarize_execution(event)}

## é‡è¦å‘ç°
{self._summarize_findings(event)}

## æ ¸å¿ƒç»éªŒ
{self._extract_core_lessons(event)}
"""
    
    def _generate_gist_memory(self, event: MemoryEvent) -> str:
        """ç”Ÿæˆè¦ç‚¹ç‰ˆæœ¬çš„è®°å¿†"""
        return f"""# {event.task_name}
æ—¶é—´ï¼š{event.timestamp[:10]}
ç»“æœï¼š{'âœ“' if event.success else 'âœ—'}

## ä¸€å¥è¯æ€»ç»“
{self._one_line_summary(event)}

## æ ¸å¿ƒæ¨¡å¼
{event.key_patterns[0] if event.key_patterns else 'æ— ç‰¹å®šæ¨¡å¼'}

## è®°ä½
âœ“ {self._key_takeaway(event)}
"""
    
    # ========== è¯­ä¹‰è®°å¿† ==========
    
    def create_semantic_concept(self, concept_name: str, definition: str, 
                               features: Dict[str, List[str]], 
                               examples: Dict[str, str]) -> str:
        """åˆ›å»ºè¯­ä¹‰æ¦‚å¿µ"""
        concept_path = self.memory_dir / "semantic" / "concepts" / f"{concept_name}.nlpl"
        
        content = f"""# æ¦‚å¿µï¼š{concept_name}
ç”Ÿæˆæ—¶é—´ï¼š{datetime.now().isoformat()}

## å®šä¹‰
{definition}

## ç‰¹å¾
- **å¿…è¦ç‰¹å¾**ï¼š{', '.join(features.get('essential', []))}
- **å…¸å‹ç‰¹å¾**ï¼š{', '.join(features.get('typical', []))}
- **å¯é€‰ç‰¹å¾**ï¼š{', '.join(features.get('optional', []))}

## ç¤ºä¾‹
- **å…¸å‹ä¾‹å­**ï¼š{examples.get('typical', '')}
- **è¾¹ç•Œä¾‹å­**ï¼š{examples.get('boundary', '')}
- **åä¾‹**ï¼š{examples.get('counter', '')}

## å…³è”æ¦‚å¿µ
[ç”±æµ·é©¬ä½“Agentåç»­å¡«å……]
"""
        
        concept_path.write_text(content, encoding='utf-8')
        self._update_semantic_relations(concept_name)
        
        return str(concept_path)
    
    def create_semantic_pattern(self, pattern_name: str, trigger: str,
                               steps: List[str], success_rate: float) -> str:
        """åˆ›å»ºè¯­ä¹‰æ¨¡å¼"""
        pattern_path = self.memory_dir / "semantic" / "patterns" / f"{pattern_name}.nlpl"
        
        content = f"""# æ¨¡å¼ï¼š{pattern_name}
æå–æ—¶é—´ï¼š{datetime.now().isoformat()}
æˆåŠŸç‡ï¼š{success_rate:.1%}

## è§¦å‘æ¡ä»¶
{trigger}

## æ‰§è¡Œæ­¥éª¤
{self._format_steps(steps)}

## å‚æ•°é…ç½®
[å¯æ ¹æ®å…·ä½“æƒ…å†µè°ƒæ•´]

## ä½¿ç”¨ç»Ÿè®¡
- **ä½¿ç”¨æ¬¡æ•°**ï¼š0
- **æˆåŠŸæ¬¡æ•°**ï¼š0
- **å¹³å‡è½®æ•°**ï¼šN/A

## ä¼˜åŒ–å†å²
- v1.0ï¼šåˆå§‹ç‰ˆæœ¬
"""
        
        pattern_path.write_text(content, encoding='utf-8')
        return str(pattern_path)
    
    # ========== ç¨‹åºæ€§è®°å¿† ==========
    
    def create_procedural_skill(self, skill_name: str, trigger_conditions: str,
                               execution_steps: List[Dict[str, Any]], 
                               proficiency: float = 0.0) -> str:
        """åˆ›å»ºç¨‹åºæ€§æŠ€èƒ½"""
        skill_path = self.memory_dir / "procedural" / "skills" / f"{skill_name}.nlpl"
        
        content = f"""# æŠ€èƒ½ï¼š{skill_name}
åˆ›å»ºæ—¶é—´ï¼š{datetime.now().isoformat()}
ç†Ÿç»ƒåº¦ï¼š{proficiency:.1%}

## è§¦å‘æƒ…å¢ƒ
{trigger_conditions}

## æ‰§è¡Œæ­¥éª¤
{self._format_procedural_steps(execution_steps)}

## ç†Ÿç»ƒåº¦æŒ‡æ ‡
- **ä½¿ç”¨æ¬¡æ•°**ï¼š0
- **æˆåŠŸç‡**ï¼š0%
- **å¹³å‡ç”¨æ—¶**ï¼šN/A
- **è‡ªåŠ¨åŒ–ç¨‹åº¦**ï¼š{proficiency}

## ç›¸å…³æŠ€èƒ½
- **å‰ç½®æŠ€èƒ½**ï¼š[]
- **ç»„åˆæŠ€èƒ½**ï¼š[]
- **è¿›é˜¶æŠ€èƒ½**ï¼š[]

## ä¼˜åŒ–è®°å½•
[éšä½¿ç”¨é€æ­¥æ›´æ–°]
"""
        
        skill_path.write_text(content, encoding='utf-8')
        self._update_proficiency_index(skill_name, proficiency)
        
        return str(skill_path)
    
    # ========== å·¥ä½œè®°å¿† ==========
    
    def update_working_memory(self, task: str, state: Dict[str, Any], 
                             focus: str, activated_memories: List[str]):
        """æ›´æ–°å·¥ä½œè®°å¿†"""
        working_path = self.memory_dir / "working" / "current_context.nlpl"
        
        content = f"""# å½“å‰å·¥ä½œä¸Šä¸‹æ–‡
æ›´æ–°æ—¶é—´ï¼š{datetime.now().isoformat()}

## æ´»åŠ¨ä»»åŠ¡
**ä¸»ä»»åŠ¡**ï¼š{task}
**çŠ¶æ€**ï¼š{state.get('status', 'è¿›è¡Œä¸­')}

## æ³¨æ„åŠ›ç„¦ç‚¹
**å½“å‰å…³æ³¨**ï¼š{focus}

## ä¸´æ—¶çŠ¶æ€
{self._format_state(state)}

## æ¿€æ´»çš„è®°å¿†
{self._format_activated_memories(activated_memories)}
"""
        
        working_path.write_text(content, encoding='utf-8')
    
    # ========== å…ƒè®¤çŸ¥è®°å¿† ==========
    
    def update_metacognitive_assessment(self, metrics: Dict[str, float], 
                                       strategies: Dict[str, Dict],
                                       recommendations: List[str]):
        """æ›´æ–°å…ƒè®¤çŸ¥è¯„ä¼°"""
        meta_path = self.memory_dir / "metacognitive" / "assessment.nlpl"
        
        content = f"""# å…ƒè®¤çŸ¥è¯„ä¼°
è¯„ä¼°æ—¶é—´ï¼š{datetime.now().isoformat()}

## æ€§èƒ½æŒ‡æ ‡
{self._format_metrics(metrics)}

## ç­–ç•¥æ•ˆæœ
{self._format_strategies(strategies)}

## æ”¹è¿›å»ºè®®
{self._format_recommendations(recommendations)}

## ç³»ç»Ÿå¥åº·åº¦
{self._assess_system_health()}
"""
        
        meta_path.write_text(content, encoding='utf-8')
    
    # ========== è®°å¿†æ£€ç´¢ ==========
    
    def search_memories(self, query: str, memory_type: Optional[str] = None, 
                       limit: int = 10) -> List[str]:
        """ä½¿ç”¨grepæœç´¢è®°å¿†"""
        if memory_type:
            search_dir = self.memory_dir / memory_type
        else:
            search_dir = self.memory_dir
            
        try:
            # ä½¿ç”¨grepæœç´¢
            result = subprocess.run(
                ["grep", "-r", "-l", query, str(search_dir), "--include=*.nlpl"],
                capture_output=True,
                text=True,
                timeout=5
            )
            
            if result.returncode == 0:
                files = result.stdout.strip().split('\n')
                return files[:limit] if files[0] else []
            else:
                return []
                
        except (subprocess.TimeoutExpired, Exception):
            return []
    
    def get_recent_memories(self, days: int = 7) -> List[str]:
        """è·å–æœ€è¿‘Nå¤©çš„è®°å¿†"""
        cutoff = datetime.now() - timedelta(days=days)
        recent_memories = []
        
        episodic_dir = self.memory_dir / "episodic"
        for date_dir in episodic_dir.iterdir():
            if date_dir.is_dir():
                try:
                    dir_date = datetime.strptime(date_dir.name, "%Y-%m-%d")
                    if dir_date >= cutoff:
                        for memory_file in date_dir.glob("*.nlpl"):
                            recent_memories.append(str(memory_file))
                except ValueError:
                    continue
                    
        return sorted(recent_memories, reverse=True)
    
    # ========== è®°å¿†è¡°å‡ ==========
    
    def apply_temporal_decay(self):
        """åº”ç”¨æ—¶é—´è¡°å‡è§„åˆ™"""
        now = datetime.now()
        decay_log = []
        
        episodic_dir = self.memory_dir / "episodic"
        for date_dir in episodic_dir.iterdir():
            if not date_dir.is_dir():
                continue
                
            try:
                dir_date = datetime.strptime(date_dir.name, "%Y-%m-%d")
                age_days = (now - dir_date).days
                
                for memory_file in date_dir.glob("*.nlpl"):
                    if age_days > 7 and "detailed" in memory_file.name:
                        # 7å¤©ååˆ é™¤è¯¦ç»†ç‰ˆæœ¬
                        importance = self._assess_importance(memory_file)
                        if importance < 0.7:
                            memory_file.unlink()
                            decay_log.append(f"åˆ é™¤è¯¦ç»†ç‰ˆæœ¬ï¼š{memory_file.name}")
                            
                    elif age_days > 30 and "summary" in memory_file.name:
                        # 30å¤©ååˆ é™¤æ‘˜è¦ç‰ˆæœ¬
                        if not self._is_consolidated(memory_file):
                            memory_file.unlink()
                            decay_log.append(f"åˆ é™¤æ‘˜è¦ç‰ˆæœ¬ï¼š{memory_file.name}")
                            
                    elif age_days > 90 and "gist" in memory_file.name:
                        # 90å¤©åå½’æ¡£è¦ç‚¹ç‰ˆæœ¬
                        if self._should_archive(memory_file):
                            archive_dir = self.memory_dir / "archive" / date_dir.name
                            archive_dir.mkdir(parents=True, exist_ok=True)
                            shutil.move(str(memory_file), str(archive_dir))
                            decay_log.append(f"å½’æ¡£ï¼š{memory_file.name}")
                            
            except ValueError:
                continue
                
        return decay_log
    
    # ========== è¾…åŠ©æ–¹æ³• ==========
    
    def _sanitize_filename(self, name: str) -> str:
        """æ¸…ç†æ–‡ä»¶å"""
        return re.sub(r'[^\w\s-]', '', name)[:50]
    
    def _assess_complexity(self, event: MemoryEvent) -> str:
        """è¯„ä¼°ä»»åŠ¡å¤æ‚åº¦"""
        if event.execution_rounds > 20:
            return "é«˜"
        elif event.execution_rounds > 10:
            return "ä¸­"
        else:
            return "ä½"
    
    def _assess_novelty(self, event: MemoryEvent) -> str:
        """è¯„ä¼°ä»»åŠ¡æ–°é¢–åº¦"""
        # ç®€åŒ–å®ç°ï¼šåŸºäºåˆ›æ–°æ•°é‡
        if len(event.innovations) > 2:
            return "é«˜"
        elif len(event.innovations) > 0:
            return "ä¸­"
        else:
            return "ä½"
    
    def _format_patterns(self, patterns: List[str]) -> str:
        """æ ¼å¼åŒ–æ¨¡å¼åˆ—è¡¨"""
        if not patterns:
            return "- æ— ç‰¹å®šæ¨¡å¼"
        return "\n".join(f"- {p}" for p in patterns)
    
    def _format_emotions(self, emotions: Dict[str, float]) -> str:
        """æ ¼å¼åŒ–æƒ…ç»ªæ ‡è®°"""
        if not emotions:
            return "æ— ç‰¹æ®Šæƒ…ç»ªæ ‡è®°"
        
        lines = []
        for emotion, intensity in emotions.items():
            bars = "â– " * int(intensity * 10)
            lines.append(f"- **{emotion}**ï¼š{bars} ({intensity:.1f})")
        return "\n".join(lines)
    
    def _format_innovations(self, innovations: List[str]) -> str:
        """æ ¼å¼åŒ–åˆ›æ–°å‘ç°"""
        if not innovations:
            return "æ— åˆ›æ–°å‘ç°"
        return "\n".join(f"ğŸ’¡ {i}" for i in innovations)
    
    def _format_errors(self, errors: List[str]) -> str:
        """æ ¼å¼åŒ–é”™è¯¯è®°å½•"""
        if not errors:
            return "æ— é”™è¯¯å‘ç”Ÿ"
        return "\n".join(f"âš ï¸ {e}" for e in errors)
    
    def _calculate_efficiency(self, event: MemoryEvent) -> float:
        """è®¡ç®—æ•ˆç‡åˆ†æ•°"""
        base_score = 10.0
        if not event.success:
            base_score *= 0.5
        if event.execution_rounds > 20:
            base_score *= 0.8
        if event.cognitive_load > 0.8:
            base_score *= 0.9
        return min(10, max(0, base_score))
    
    def _calculate_stability(self, event: MemoryEvent) -> float:
        """è®¡ç®—ç¨³å®šæ€§åˆ†æ•°"""
        error_penalty = len(event.errors) * 1.5
        return max(0, 10 - error_penalty)
    
    def _extract_lessons(self, event: MemoryEvent) -> str:
        """æå–ç»éªŒæ•™è®­"""
        lessons = []
        
        if event.success:
            lessons.append(f"æˆåŠŸå› ç´ ï¼š{event.key_patterns[0] if event.key_patterns else 'ç¨³å®šæ‰§è¡Œ'}")
        else:
            lessons.append(f"å¤±è´¥åŸå› ï¼š{event.errors[0] if event.errors else 'æœªçŸ¥'}")
            
        if event.innovations:
            lessons.append(f"åˆ›æ–°ä»·å€¼ï¼š{event.innovations[0]}")
            
        return "\n".join(f"- {l}" for l in lessons)
    
    def _update_episodic_index(self, event: MemoryEvent, 
                               detailed_path: Path, 
                               summary_path: Path, 
                               gist_path: Path):
        """æ›´æ–°æƒ…æ™¯è®°å¿†ç´¢å¼•"""
        index_path = self.memory_dir / "episodic" / "index.nlpl"
        
        entry = f"""
## {event.timestamp}
- **ä»»åŠ¡ç±»å‹**ï¼š{event.task_type}
- **æ‰§è¡Œæ¨¡å¼**ï¼š{event.key_patterns[0] if event.key_patterns else 'é»˜è®¤'}
- **å…³é”®ç‰¹å¾**ï¼š[{', '.join(event.key_patterns[:3])}]
- **æ–‡ä»¶è·¯å¾„**ï¼š
  - è¯¦ç»†ï¼š{detailed_path}
  - æ‘˜è¦ï¼š{summary_path}
  - è¦ç‚¹ï¼š{gist_path}
"""
        
        with open(index_path, 'a', encoding='utf-8') as f:
            f.write(entry)
    
    def _update_semantic_relations(self, concept_name: str):
        """æ›´æ–°è¯­ä¹‰å…³ç³»ç½‘ç»œ"""
        relations_path = self.memory_dir / "semantic" / "relations.nlpl"
        
        # ç®€åŒ–å®ç°ï¼šè¿½åŠ æ–°æ¦‚å¿µ
        with open(relations_path, 'a', encoding='utf-8') as f:
            f.write(f"\n- æ–°æ¦‚å¿µï¼š{concept_name} (å¾…å»ºç«‹å…³ç³»)\n")
    
    def _update_proficiency_index(self, skill_name: str, proficiency: float):
        """æ›´æ–°æŠ€èƒ½ç†Ÿç»ƒåº¦ç´¢å¼•"""
        proficiency_path = self.memory_dir / "procedural" / "proficiency.nlpl"
        
        with open(proficiency_path, 'a', encoding='utf-8') as f:
            f.write(f"\n- {skill_name}ï¼š{proficiency:.1%}\n")
    
    def _assess_importance(self, file_path: Path) -> float:
        """è¯„ä¼°è®°å¿†é‡è¦æ€§"""
        # ç®€åŒ–å®ç°ï¼šåŸºäºæ–‡ä»¶å¤§å°å’Œè®¿é—®æ—¶é—´
        try:
            stat = file_path.stat()
            size_factor = min(1.0, stat.st_size / 10000)
            
            # æœ€è¿‘è®¿é—®æ—¶é—´
            last_access = datetime.fromtimestamp(stat.st_atime)
            days_since_access = (datetime.now() - last_access).days
            access_factor = max(0, 1 - days_since_access / 30)
            
            return (size_factor + access_factor) / 2
        except:
            return 0.5
    
    def _is_consolidated(self, file_path: Path) -> bool:
        """æ£€æŸ¥è®°å¿†æ˜¯å¦å·²å·©å›º"""
        # ç®€åŒ–å®ç°ï¼šæ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”çš„è¯­ä¹‰æˆ–ç¨‹åºæ€§è®°å¿†
        memory_name = file_path.stem.split('_')[0]
        
        semantic_exists = any(
            (self.memory_dir / "semantic" / "concepts").glob(f"*{memory_name}*")
        )
        procedural_exists = any(
            (self.memory_dir / "procedural" / "skills").glob(f"*{memory_name}*")
        )
        
        return semantic_exists or procedural_exists
    
    def _should_archive(self, file_path: Path) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥å½’æ¡£"""
        importance = self._assess_importance(file_path)
        return importance < 0.3
    
    # ç®€åŒ–çš„è¾…åŠ©æ–¹æ³•
    def _identify_peak_moments(self, event):
        return "æ‰§è¡Œä¸­æœŸ"
    
    def _assess_resource_usage(self, event):
        return "ä¸­ç­‰"
    
    def _summarize_execution(self, event):
        return f"é‡‡ç”¨{event.key_patterns[0] if event.key_patterns else 'é»˜è®¤'}æ¨¡å¼æ‰§è¡Œ"
    
    def _summarize_findings(self, event):
        if event.innovations:
            return f"å‘ç°{len(event.innovations)}ä¸ªåˆ›æ–°ç‚¹"
        return "æ— ç‰¹æ®Šå‘ç°"
    
    def _extract_core_lessons(self, event):
        if event.success:
            return "ä»»åŠ¡æˆåŠŸå®Œæˆï¼Œæ¨¡å¼æœ‰æ•ˆ"
        return "éœ€è¦æ”¹è¿›æ‰§è¡Œç­–ç•¥"
    
    def _one_line_summary(self, event):
        return f"ä½¿ç”¨{event.execution_rounds}è½®{'æˆåŠŸ' if event.success else 'å¤±è´¥'}å®Œæˆ{event.task_type}ä»»åŠ¡"
    
    def _key_takeaway(self, event):
        if event.innovations:
            return event.innovations[0]
        elif event.key_patterns:
            return f"{event.key_patterns[0]}æ¨¡å¼æœ‰æ•ˆ"
        else:
            return "ä¿æŒç¨³å®šæ‰§è¡Œ"
    
    def _format_steps(self, steps):
        return "\n".join(f"{i+1}. {step}" for i, step in enumerate(steps))
    
    def _format_procedural_steps(self, steps):
        formatted = []
        for i, step in enumerate(steps, 1):
            formatted.append(f"### æ­¥éª¤{i}ï¼š{step.get('name', 'æœªå‘½å')}")
            if 'action' in step:
                formatted.append(f"- **åŠ¨ä½œ**ï¼š{step['action']}")
            if 'tool' in step:
                formatted.append(f"- **å·¥å…·**ï¼š{step['tool']}")
            if 'validation' in step:
                formatted.append(f"- **éªŒè¯**ï¼š{step['validation']}")
        return "\n".join(formatted)
    
    def _format_state(self, state):
        lines = []
        for key, value in state.items():
            if isinstance(value, dict):
                lines.append(f"**{key}**ï¼š")
                for k, v in value.items():
                    lines.append(f"  - {k}: {v}")
            else:
                lines.append(f"**{key}**ï¼š{value}")
        return "\n".join(lines)
    
    def _format_activated_memories(self, memories):
        if not memories:
            return "æ— æ¿€æ´»è®°å¿†"
        return "\n".join(f"- {m}" for m in memories[:5])
    
    def _format_metrics(self, metrics):
        lines = []
        for metric, value in metrics.items():
            percentage = int(value * 100)
            bars = "â– " * (percentage // 10) + "â–¡" * (10 - percentage // 10)
            lines.append(f"- **{metric}**ï¼š{bars} {percentage}%")
        return "\n".join(lines)
    
    def _format_strategies(self, strategies):
        lines = []
        for name, stats in strategies.items():
            lines.append(f"### {name}")
            lines.append(f"- æˆåŠŸç‡ï¼š{stats.get('success_rate', 0):.1%}")
            lines.append(f"- ä½¿ç”¨é¢‘ç‡ï¼š{stats.get('usage_frequency', 0):.1%}")
            lines.append(f"- æ•ˆç‡å¾—åˆ†ï¼š{stats.get('efficiency', 0):.2f}")
        return "\n".join(lines)
    
    def _format_recommendations(self, recommendations):
        return "\n".join(f"{i+1}. {rec}" for i, rec in enumerate(recommendations))
    
    def _assess_system_health(self):
        """è¯„ä¼°ç³»ç»Ÿå¥åº·åº¦"""
        # ç®€åŒ–å®ç°
        return """
- è®°å¿†å®¹é‡ï¼šæ­£å¸¸
- æ£€ç´¢æ•ˆç‡ï¼šè‰¯å¥½
- çŸ¥è¯†å¢é•¿ï¼šç¨³å®š
- ç³»ç»Ÿè´Ÿè½½ï¼šé€‚ä¸­
"""


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ
    memory = NLPLMemorySystem()
    
    # åˆ›å»ºä¸€ä¸ªæƒ…æ™¯è®°å¿†
    event = MemoryEvent(
        timestamp=datetime.now().isoformat(),
        task_name="åˆ›å»ºè®¡ç®—å™¨æ¨¡å—",
        task_type="æ–‡ä»¶åˆ›å»º",
        execution_rounds=12,
        success=True,
        key_patterns=["å¿«é€ŸåŸå‹", "è¿­ä»£å¼€å‘"],
        emotional_markers={"æ»¡æ„": 0.8, "è‡ªä¿¡": 0.7},
        cognitive_load=0.4,
        innovations=["ä½¿ç”¨ç±»å‹æ³¨è§£æé«˜ä»£ç è´¨é‡"],
        errors=[]
    )
    
    paths = memory.create_episodic_memory(event)
    print(f"åˆ›å»ºæƒ…æ™¯è®°å¿†ï¼š\n  è¯¦ç»†ï¼š{paths[0]}\n  æ‘˜è¦ï¼š{paths[1]}\n  è¦ç‚¹ï¼š{paths[2]}")
    
    # åˆ›å»ºè¯­ä¹‰æ¦‚å¿µ
    concept_path = memory.create_semantic_concept(
        "æ–‡ä»¶æ“ä½œ",
        "ä¸æ–‡ä»¶ç³»ç»Ÿäº¤äº’çš„è¡Œä¸º",
        {
            "essential": ["è·¯å¾„", "å†…å®¹"],
            "typical": ["ç¼–ç ", "æƒé™"],
            "optional": ["ç¼“å†²", "é”"]
        },
        {
            "typical": "è¯»å–é…ç½®æ–‡ä»¶",
            "boundary": "è¯»å–ç½‘ç»œæ–‡ä»¶",
            "counter": "å†…å­˜æ“ä½œ"
        }
    )
    print(f"åˆ›å»ºè¯­ä¹‰æ¦‚å¿µï¼š{concept_path}")
    
    # åˆ›å»ºç¨‹åºæ€§æŠ€èƒ½
    skill_path = memory.create_procedural_skill(
        "è°ƒè¯•Pythoné”™è¯¯",
        "å½“å‡ºç°Pythonå¼‚å¸¸æ—¶",
        [
            {"name": "è¯»å–é”™è¯¯", "action": "æå–é”™è¯¯ä¿¡æ¯", "tool": "parse_traceback"},
            {"name": "å®šä½é—®é¢˜", "action": "æ‰¾åˆ°é”™è¯¯ä½ç½®", "tool": "read_file"},
            {"name": "ä¿®å¤é”™è¯¯", "action": "åº”ç”¨ä¿®å¤æ–¹æ¡ˆ", "tool": "edit_file"}
        ],
        proficiency=0.7
    )
    print(f"åˆ›å»ºç¨‹åºæ€§æŠ€èƒ½ï¼š{skill_path}")
    
    # æœç´¢è®°å¿†
    results = memory.search_memories("è®¡ç®—å™¨")
    print(f"æœç´¢ç»“æœï¼š{results}")
    
    # åº”ç”¨æ—¶é—´è¡°å‡
    decay_log = memory.apply_temporal_decay()
    if decay_log:
        print(f"è¡°å‡å¤„ç†ï¼š{decay_log}")