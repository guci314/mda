#!/usr/bin/env python3
"""é€šç”¨ ReactAgent v4 - é¢†åŸŸæ— å…³ç‰ˆæœ¬ï¼Œæ”¯æŒå…ˆéªŒçŸ¥è¯†æ³¨å…¥

è¿™æ˜¯ä¸€ä¸ªåŸºäº LangChain çš„é€šç”¨ React Agent å®ç°ï¼Œä¸»è¦ç‰¹æ€§ï¼š
1. æ”¯æŒä¸‰çº§è®°å¿†ç³»ç»Ÿï¼ˆæ— è®°å¿†ã€æ™ºèƒ½ç¼“å†²ã€æŒä¹…å­˜å‚¨ï¼‰
2. æ”¯æŒå…ˆéªŒçŸ¥è¯†æ³¨å…¥å’Œ include æœºåˆ¶
3. é¢†åŸŸæ— å…³çš„é€šç”¨è®¾è®¡
4. é›†æˆæ–‡ä»¶æ“ä½œã€å‘½ä»¤æ‰§è¡Œç­‰åŸºç¡€å·¥å…·
5. æ”¯æŒè‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯
6. æ”¯æŒé…ç½®ä»»ä½• OpenAI å…¼å®¹çš„ LLMï¼ˆé»˜è®¤ä½¿ç”¨ DeepSeekï¼‰

ä½¿ç”¨ç¤ºä¾‹ï¼š
    # ä½¿ç”¨é»˜è®¤ DeepSeek
    config = ReactAgentConfig(
        work_dir="output",
        memory_level=MemoryLevel.SMART,
        knowledge_files=["knowledge/experimental/ç»¼åˆçŸ¥è¯†.md"]
    )
    agent = GenericReactAgent(config, name="my_agent")
    agent.execute_task("åˆ›å»ºä¸€ä¸ªç”¨æˆ·ç®¡ç†ç³»ç»Ÿ")
    
    # ä½¿ç”¨è‡ªå®šä¹‰ LLM
    config = ReactAgentConfig(
        work_dir="output",
        llm_model="gpt-4",
        llm_base_url="https://api.openai.com/v1",
        llm_api_key_env="OPENAI_API_KEY"
    )
    agent = GenericReactAgent(config, name="gpt4_agent")
    agent.execute_task("åˆ›å»ºä¸€ä¸ªç”¨æˆ·ç®¡ç†ç³»ç»Ÿ")
"""

import os
import sys
import time
import logging
import re
import warnings
import asyncio
import threading
import atexit
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Optional

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

# è¿‡æ»¤ LangChain å¼ƒç”¨è­¦å‘Š - å¿…é¡»åœ¨å¯¼å…¥ä¹‹å‰è®¾ç½®
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=FutureWarning)

# è®¾ç½®æ—¥å¿— - é»˜è®¤åªæ˜¾ç¤ºè­¦å‘Šå’Œé”™è¯¯
logging.basicConfig(
    level=logging.WARNING,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ç¦ç”¨ httpx çš„ INFO æ—¥å¿—
logging.getLogger("httpx").setLevel(logging.WARNING)

# æ‰‹åŠ¨å¯¼å…¥å¿…è¦çš„å†…å®¹
from dotenv import load_dotenv
load_dotenv()

# å¤„ç†ä»£ç†è®¾ç½®é—®é¢˜ - ä¿ç•™ä»£ç†è®¾ç½®ç”¨äº Google æœç´¢
# ä½†æŸäº› API å¯èƒ½éœ€è¦ä¸åŒçš„å¤„ç†
# os.environ.pop('http_proxy', None)
# os.environ.pop('https_proxy', None)
# os.environ.pop('all_proxy', None)

# ç›´æ¥å¤åˆ¶ReactAgentGeneratorçš„ä»£ç ï¼Œé¿å…å¯¼å…¥é—®é¢˜
from enum import Enum

# åœ¨å¯¼å…¥ langchain ä¹‹å‰å†æ¬¡è®¾ç½®è­¦å‘Šè¿‡æ»¤
import warnings as _warnings
_warnings.filterwarnings("ignore", category=DeprecationWarning)

from langchain_openai import ChatOpenAI
from langchain.agents import AgentExecutor
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

# ä½¿ç”¨ try-except å¤„ç†å¯¼å…¥ï¼Œé¿å… Pylance é”™è¯¯
try:
    from langgraph.prebuilt import create_react_agent  # type: ignore
except ImportError:
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œæä¾›ä¸€ä¸ªå›é€€æ–¹æ¡ˆ
    raise ImportError(
        "LangGraph is required. Please install it with: pip install langgraph>=0.2.0"
    )
from langchain_core.tools import tool
from langchain_community.cache import SQLiteCache
from langchain_core.globals import set_llm_cache
from langchain.memory import (
    ConversationBufferMemory,
    ConversationBufferWindowMemory,
)
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain.memory.chat_memory import BaseChatMemory
from langchain.schema import get_buffer_string
from langchain_core.runnables import RunnableConfig
from langchain_community.chat_message_histories import (
    SQLChatMessageHistory,
)
from pydantic import BaseModel, Field

# å¯é€‰ï¼šå¯¼å…¥ç‰¹å®š LLM çš„ token è®¡æ•°ä¿®å¤
try:
    from deepseek_token_counter import patch_deepseek_token_counting  # type: ignore
except ImportError:
    patch_deepseek_token_counting = None

# è®¾ç½®é»˜è®¤ç¼“å­˜ - ä½¿ç”¨ç»å¯¹è·¯å¾„ç¡®ä¿ç¼“å­˜ç”Ÿæ•ˆ
# å¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡ DISABLE_LANGCHAIN_CACHE=true æ¥ç¦ç”¨ç¼“å­˜
default_cache_path = os.path.join(os.path.dirname(__file__), ".langchain.db")
if os.environ.get('DISABLE_LANGCHAIN_CACHE', '').lower() != 'true':
    set_llm_cache(SQLiteCache(database_path=default_cache_path))
    
    # å¦‚æœå¯ç”¨è°ƒè¯•æ¨¡å¼ï¼Œè®°å½•ç¼“å­˜è·¯å¾„
    if os.environ.get('DEBUG'):
        logger.info(f"LangChain SQLite cache enabled at: {default_cache_path}")
        if os.path.exists(default_cache_path):
            logger.info(f"Cache file size: {os.path.getsize(default_cache_path)} bytes")
else:
    if os.environ.get('DEBUG'):
        logger.info("LangChain cache disabled by environment variable")

class MemoryLevel(str, Enum):
    """è®°å¿†çº§åˆ«æšä¸¾
    
    å®šä¹‰ Agent çš„è®°å¿†ç®¡ç†ç­–ç•¥ï¼š
    - NONE: æ— çŠ¶æ€æ¨¡å¼ï¼Œé€‚åˆç®€å•ä»»åŠ¡
    - SMART: æ‘˜è¦ç¼“å†²åŒºï¼Œè‡ªåŠ¨æ‘˜è¦è¶…å‡ºé™åˆ¶çš„å†å²å¯¹è¯
    - PRO: SQLite æŒä¹…åŒ–ï¼Œæ”¯æŒçŸ¥è¯†æå–
    """
    NONE = "none"              # æ— è®°å¿† - å¿«é€Ÿç®€å•
    SMART = "summary_buffer"   # æ™ºèƒ½æ‘˜è¦ç¼“å†² - è‡ªåŠ¨æ‘˜è¦æ—§å¯¹è¯ï¼Œä¿ç•™è¿‘æœŸåŸæ–‡
    PRO = "sqlite"            # æŒä¹…å­˜å‚¨ - ä¸“ä¸šé¡¹ç›®

# ä» tools æ¨¡å—å¯¼å…¥å·¥å…·åˆ›å»ºå‡½æ•°
try:
    # å°è¯•ç›¸å¯¹å¯¼å…¥ï¼ˆä½œä¸ºåŒ…çš„ä¸€éƒ¨åˆ†è¿è¡Œï¼‰
    from .tools import create_tools  # type: ignore
except ImportError:
    # å°è¯•ç»å¯¹å¯¼å…¥ï¼ˆç›´æ¥è¿è¡Œè„šæœ¬ï¼‰
    from .tools import create_tools  # type: ignore

# å…¨å±€çº¿ç¨‹è·Ÿè¸ª
_memory_update_threads = []
_exploration_threads = []
_shutdown_registered = False

def _wait_for_memory_updates():
    """ç­‰å¾…æ‰€æœ‰åå°çº¿ç¨‹å®Œæˆ"""
    # ç­‰å¾…çŸ¥è¯†æå–çº¿ç¨‹
    if _memory_update_threads:
        active_threads = [t for t in _memory_update_threads if t.is_alive()]
        if active_threads:
            print(f"\nç­‰å¾… {len(active_threads)} ä¸ªçŸ¥è¯†æå–ä»»åŠ¡å®Œæˆ...")
            for i, thread in enumerate(active_threads, 1):
                if thread.is_alive():
                    print(f"  [{i}/{len(active_threads)}] ç­‰å¾… {thread.name}...")
                    thread.join(timeout=30)  # æœ€å¤šç­‰å¾…30ç§’
                    if thread.is_alive():
                        print(f"  è­¦å‘Šï¼š{thread.name} è¶…æ—¶æœªå®Œæˆ")
                    else:
                        print(f"  [{i}/{len(active_threads)}] {thread.name} å·²å®Œæˆ")
            print("æ‰€æœ‰è®°å¿†æ›´æ–°å·²å®Œæˆã€‚")
        _memory_update_threads.clear()
    
    # ç­‰å¾…é¡¹ç›®æ¢ç´¢çº¿ç¨‹
    if _exploration_threads:
        active_threads = [t for t in _exploration_threads if t.is_alive()]
        if active_threads:
            print(f"\nç­‰å¾… {len(active_threads)} ä¸ªé¡¹ç›®æ¢ç´¢ä»»åŠ¡å®Œæˆ...")
            for i, thread in enumerate(active_threads, 1):
                if thread.is_alive():
                    print(f"  [{i}/{len(active_threads)}] ç­‰å¾… {thread.name}...")
                    thread.join(timeout=60)  # é¡¹ç›®æ¢ç´¢å¯èƒ½éœ€è¦æ›´é•¿æ—¶é—´
                    if thread.is_alive():
                        print(f"  è­¦å‘Šï¼š{thread.name} è¶…æ—¶æœªå®Œæˆ")
                    else:
                        print(f"  [{i}/{len(active_threads)}] {thread.name} å·²å®Œæˆ")
            print("æ‰€æœ‰é¡¹ç›®æ¢ç´¢å·²å®Œæˆã€‚")
        _exploration_threads.clear()

# æ³¨å†Œé€€å‡ºå¤„ç†å‡½æ•°
def _register_shutdown_handler():
    """æ³¨å†Œç¨‹åºé€€å‡ºæ—¶çš„å¤„ç†å‡½æ•°"""
    global _shutdown_registered
    if not _shutdown_registered:
        atexit.register(_wait_for_memory_updates)
        _shutdown_registered = True

class CustomSummaryBufferMemory(ConversationBufferMemory):
    """è‡ªå®šä¹‰çš„æ‘˜è¦ç¼“å†²å†…å­˜å®ç°
    
    æ›¿ä»£å·²å¼ƒç”¨çš„ ConversationSummaryBufferMemoryï¼Œå®ç°ç›¸ä¼¼åŠŸèƒ½ï¼š
    - ä¿ç•™æœ€è¿‘çš„å¯¹è¯åœ¨ç¼“å†²åŒº
    - å½“è¶…è¿‡ token é™åˆ¶æ—¶ï¼Œå¯¹æ—§æ¶ˆæ¯è¿›è¡Œæ‘˜è¦
    - ä½¿ç”¨ LLM ç”Ÿæˆæ‘˜è¦
    """
    
    llm: Any = None
    max_token_limit: int = 2000
    summary: str = ""
    
    def __init__(
        self,
        llm,
        max_token_limit: int = 2000,
        memory_key: str = "chat_history",
        return_messages: bool = True,
        **kwargs
    ):
        super().__init__(memory_key=memory_key, return_messages=return_messages, **kwargs)
        self.llm = llm
        self.max_token_limit = max_token_limit
        self.summary = ""
        
    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:
        """ä¿å­˜ä¸Šä¸‹æ–‡å¹¶åœ¨éœ€è¦æ—¶è¿›è¡Œæ‘˜è¦"""
        # ç¡®ä¿è¾“å‡ºä¸ä¸ºç©º
        if not outputs or not any(outputs.values()):
            outputs = {"output": "No response"}
        # å…ˆä¿å­˜æ–°çš„å¯¹è¯
        super().save_context(inputs, outputs)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦å‹ç¼©
        self._prune_memory_if_needed()
        
    def _prune_memory_if_needed(self) -> None:
        """å¦‚æœè¶…è¿‡ token é™åˆ¶ï¼Œåˆ™ä¿®å‰ªå†…å­˜"""
        # è·å–å½“å‰æ¶ˆæ¯çš„ token æ•°
        buffer_string = get_buffer_string(
            self.chat_memory.messages,
            human_prefix="Human",
            ai_prefix="Assistant"
        )
        
        # ç®€å•çš„ token ä¼°ç®—ï¼ˆå¤§çº¦ 4 ä¸ªå­—ç¬¦ = 1 ä¸ª tokenï¼‰
        current_tokens = len(buffer_string) // 4
        
        if current_tokens > self.max_token_limit:
            # éœ€è¦è¿›è¡Œæ‘˜è¦
            print(f"\nğŸ’­ [SMARTè®°å¿†] å½“å‰å¯¹è¯å†å²è¶…è¿‡é™åˆ¶ ({current_tokens} > {self.max_token_limit} tokens)")
            print(f"   æ­£åœ¨å‹ç¼©æ—©æœŸå¯¹è¯å†å²...")
            
            messages_to_summarize = []
            remaining_messages = []
            accumulated_tokens = 0
            
            # ä»æœ€æ–°çš„æ¶ˆæ¯å¼€å§‹ç´¯ç§¯ï¼Œç›´åˆ°è¾¾åˆ°é™åˆ¶
            for message in reversed(self.chat_memory.messages):
                message_tokens = len(message.content) // 4
                if accumulated_tokens + message_tokens < self.max_token_limit * 0.8:  # ä¿ç•™ 80% ç©ºé—´
                    remaining_messages.insert(0, message)
                    accumulated_tokens += message_tokens
                else:
                    messages_to_summarize.insert(0, message)
            
            if messages_to_summarize:
                # ç”Ÿæˆæ‘˜è¦
                print(f"   å°† {len(messages_to_summarize)} æ¡æ—©æœŸæ¶ˆæ¯å‹ç¼©ä¸ºæ‘˜è¦...")
                self._generate_summary(messages_to_summarize)
                
                # æ›´æ–°æ¶ˆæ¯å†å²
                self.chat_memory.messages = remaining_messages
                print(f"   âœ… å‹ç¼©å®Œæˆï¼ä¿ç•™æœ€è¿‘ {len(remaining_messages)} æ¡æ¶ˆæ¯")
    
    def _generate_summary(self, messages: List[BaseMessage]) -> None:
        """ä½¿ç”¨ LLM ç”Ÿæˆæ¶ˆæ¯æ‘˜è¦"""
        # æ„å»ºæ‘˜è¦æç¤º
        conversation = get_buffer_string(messages, human_prefix="Human", ai_prefix="Assistant")
        
        if self.summary:
            prompt = f"""Current summary:
{self.summary}

New conversation to incorporate:
{conversation}

Please provide a concise summary that captures the key points from both the existing summary and the new conversation."""
        else:
            prompt = f"""Please summarize the following conversation concisely:

{conversation}"""
        
        # ç”Ÿæˆæ‘˜è¦
        summary_message = self.llm.invoke(prompt)
        # å¤„ç†å¯èƒ½çš„ç©ºå†…å®¹æˆ–åˆ—è¡¨æ ¼å¼
        if hasattr(summary_message, 'content'):
            if isinstance(summary_message.content, str):
                self.summary = summary_message.content
            elif isinstance(summary_message.content, list) and summary_message.content:
                self.summary = str(summary_message.content[0])
            else:
                self.summary = "No summary available"
        else:
            self.summary = "No summary available"
        
    def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """åŠ è½½å†…å­˜å˜é‡ï¼ŒåŒ…æ‹¬æ‘˜è¦"""
        memory_vars = super().load_memory_variables(inputs)
        
        if self.summary and self.return_messages:
            # åœ¨æ¶ˆæ¯å†å²å‰æ·»åŠ æ‘˜è¦ä½œä¸ºç³»ç»Ÿæ¶ˆæ¯
            summary_message = SystemMessage(content=f"Previous conversation summary: {self.summary}")
            if self.memory_key in memory_vars and isinstance(memory_vars[self.memory_key], list):
                memory_vars[self.memory_key] = [summary_message] + memory_vars[self.memory_key]
        elif self.summary and not self.return_messages:
            # åœ¨å­—ç¬¦ä¸²æ ¼å¼ä¸­åŒ…å«æ‘˜è¦
            if self.memory_key in memory_vars:
                memory_vars[self.memory_key] = f"Summary: {self.summary}\n\nRecent conversation:\n{memory_vars[self.memory_key]}"
        
        return memory_vars

# é»˜è®¤æ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£å¤§å°ï¼ˆå•ä½ï¼štokensï¼‰
DEFAULT_CONTEXT_WINDOWS = {
    # DeepSeek
    "deepseek-chat": 32768,
    "deepseek-coder": 16384,
    
    
    # OpenAI
    "gpt-4": 8192,
    "gpt-4-32k": 32768,
    "gpt-4-turbo": 128000,
    "gpt-4-turbo-preview": 128000,
    "gpt-4o": 128000,
    "gpt-3.5-turbo": 16385,
    "gpt-3.5-turbo-16k": 16385,
    
    # Claude (Anthropic)
    "claude-3-opus-20240229": 200000,
    "claude-3-sonnet-20240229": 200000,
    "claude-3-haiku-20240307": 200000,
    "claude-2.1": 200000,
    "claude-2": 100000,
    
    # Default for unknown models
    "default": 4096
}

# é»˜è®¤æ¨¡å‹çš„çŸ¥è¯†æå–æ–‡ä»¶å¤§å°é™åˆ¶ï¼ˆå•ä½ï¼šbytesï¼‰
DEFAULT_KNOWLEDGE_EXTRACTION_LIMITS = {
    # DeepSeek - è¾ƒå°çš„ä¸Šä¸‹æ–‡ï¼Œä½¿ç”¨è¾ƒå°çš„è®°å¿†
    "deepseek-chat": 50 * 1024,      # 50KB
    "deepseek-coder": 50 * 1024,     # 50KB
    
    # Gemini - å¤§ä¸Šä¸‹æ–‡ï¼Œå¯ä»¥æœ‰æ›´å¤§çš„è®°å¿†
    "gemini-2.5-flash": 200 * 1024,  # 200KB
    "gemini-1.5-pro": 150 * 1024,    # 150KB
    
    # OpenAI
    "gpt-4": 100 * 1024,             # 100KB
    "gpt-4-turbo": 150 * 1024,       # 150KB
    "gpt-3.5-turbo": 80 * 1024,      # 80KB
    
    # Claude - å¤§ä¸Šä¸‹æ–‡
    "claude-3-opus-20240229": 150 * 1024,    # 150KB
    "claude-3-sonnet-20240229": 150 * 1024,  # 150KB
    "claude-2.1": 150 * 1024,        # 150KB
    
    # Default
    "default": 100 * 1024             # 100KB
}


class ReactAgentConfig:
    """React Agent é…ç½®ç±»
    
    ç®¡ç† Agent çš„æ‰€æœ‰é…ç½®å‚æ•°ï¼ŒåŒ…æ‹¬ï¼š
    - å·¥ä½œç›®å½•å’Œæ–‡ä»¶è·¯å¾„
    - è®°å¿†ç³»ç»Ÿé…ç½®
    - çŸ¥è¯†æ–‡ä»¶é…ç½®
    - ç³»ç»Ÿæç¤ºè¯é…ç½®
    - LLM é…ç½®
    
    Args:
        work_dir: å·¥ä½œç›®å½•è·¯å¾„ï¼ˆAgent çš„å·¥ä½œç©ºé—´ï¼‰
        additional_config: é¢å¤–çš„é…ç½®å­—å…¸
        memory_level: è®°å¿†çº§åˆ«ï¼ˆNONE/SMART/PROï¼‰
        session_id: ä¼šè¯IDï¼Œç”¨äºåŒºåˆ†ä¸åŒå¯¹è¯
        max_token_limit: æœ€å¤§ token é™åˆ¶ï¼ˆç”¨äº SMART æ¨¡å¼ï¼‰
        db_path: SQLite æ•°æ®åº“è·¯å¾„ï¼ˆç”¨äº PRO æ¨¡å¼ï¼‰
        knowledge_file: å•ä¸ªçŸ¥è¯†æ–‡ä»¶è·¯å¾„ï¼ˆå·²åºŸå¼ƒï¼Œå»ºè®®ä½¿ç”¨ knowledge_filesï¼‰
        knowledge_files: çŸ¥è¯†æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        knowledge_strings: çŸ¥è¯†å­—ç¬¦ä¸²åˆ—è¡¨ï¼Œç›´æ¥æä¾›çŸ¥è¯†å†…å®¹è€Œéæ–‡ä»¶è·¯å¾„
        interface: Agent æ¥å£å£°æ˜
        system_prompt_file: ç³»ç»Ÿæç¤ºè¯æ–‡ä»¶è·¯å¾„
        llm_model: LLM æ¨¡å‹åç§°ï¼ˆé»˜è®¤: "deepseek-chat"ï¼‰
        llm_base_url: LLM API åŸºç¡€ URLï¼ˆé»˜è®¤: "https://api.deepseek.com/v1"ï¼‰
        llm_api_key_env: LLM API å¯†é’¥çš„ç¯å¢ƒå˜é‡åï¼ˆé»˜è®¤: "DEEPSEEK_API_KEY"ï¼‰
        llm_temperature: LLM æ¸©åº¦å‚æ•°ï¼ˆé»˜è®¤: 0ï¼‰
        context_window: æ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£å¤§å°ï¼ˆå•ä½ï¼štokensï¼‰ã€‚å¦‚æœæœªæŒ‡å®šï¼Œå°†æ ¹æ®æ¨¡å‹åç§°è‡ªåŠ¨æ¨æ–­
        cache_path: è‡ªå®šä¹‰ç¼“å­˜è·¯å¾„ï¼ˆé»˜è®¤: Noneï¼Œä½¿ç”¨å…¨å±€ç¼“å­˜ï¼‰
        enable_cache: æ˜¯å¦å¯ç”¨ LLM ç¼“å­˜ï¼ˆé»˜è®¤: Trueï¼‰
        knowledge_extraction_limit: çŸ¥è¯†æå–æ–‡ä»¶å¤§å°é™åˆ¶ï¼ˆå•ä½ï¼šbytesï¼‰ã€‚å¦‚æœæœªæŒ‡å®šï¼Œå°†æ ¹æ®æ¨¡å‹åç§°è‡ªåŠ¨æ¨æ–­
        show_memory_updates: æ˜¯å¦æ˜¾ç¤ºè®°å¿†æ›´æ–°é€šçŸ¥ï¼ˆé»˜è®¤: Trueï¼‰ã€‚é”™è¯¯çº æ­£å§‹ç»ˆæ˜¾ç¤º
        enable_project_exploration: æ˜¯å¦å¯ç”¨é¡¹ç›®æ¢ç´¢åŠŸèƒ½ï¼ˆé»˜è®¤: Trueï¼‰
        exploration_interval: é¡¹ç›®æ¢ç´¢é—´éš”æ—¶é—´ï¼ˆç§’ï¼‰ï¼ˆé»˜è®¤: 86400ï¼Œå³24å°æ—¶ï¼‰
        exploration_prompt: é¡¹ç›®æ¢ç´¢æç¤ºè¯ï¼ˆå¯é€‰ï¼‰
        exploration_prompt_file: é¡¹ç›®æ¢ç´¢æç¤ºè¯æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        auto_reload_on_exploration: é¡¹ç›®æ¢ç´¢å®Œæˆåæ˜¯å¦è‡ªåŠ¨é‡è½½ï¼ˆé»˜è®¤: Trueï¼‰
        llm_max_tokens: LLMè¾“å‡ºçš„æœ€å¤§tokenæ•°ï¼ˆé»˜è®¤: 16384ï¼‰
    """
    def __init__(self, work_dir, additional_config=None, 
                 memory_level=MemoryLevel.SMART, session_id=None, 
                 max_token_limit=30000, db_path=None,
                 knowledge_file=None, knowledge_files=None, knowledge_strings=None, interface=None,
                 system_prompt_file="knowledge/core/system_prompt.md",
                 llm_model=None,
                 llm_base_url=None,
                 llm_api_key_env=None,
                 llm_temperature=0,
                 context_window=None,
                 cache_path=None,
                 enable_cache=True,
                 knowledge_extraction_limit=None,
                 http_client=None,
                 agent_home=None,
                 enable_world_overview=True,
                 enable_perspective=False,
                 show_memory_updates=True,
                 enable_project_exploration=False,
                 exploration_interval=86400,
                 exploration_prompt=None,
                 exploration_prompt_file=None,
                 auto_reload_on_exploration=True):
        # å°†å·¥ä½œç›®å½•è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
        self.work_dir = os.path.abspath(work_dir) if work_dir else os.getcwd()
        self.additional_config = additional_config or {}
        # Agent å†…éƒ¨å­˜å‚¨ç›®å½•ï¼ˆç‹¬ç«‹äºå·¥ä½œç›®å½•ï¼‰
        self.agent_home = agent_home
        # è®°å¿†é…ç½®
        self.memory_level = memory_level
        self.session_id = session_id or f"session_{int(time.time())}"
        self.max_token_limit = max_token_limit
        self.db_path = db_path or os.path.join(os.path.dirname(__file__), "memory.db")
        
        # çŸ¥è¯†æ–‡ä»¶è·¯å¾„ - æ”¯æŒå•ä¸ªæ–‡ä»¶æˆ–å¤šä¸ªæ–‡ä»¶
        if knowledge_files is not None:
            # ä¼˜å…ˆä½¿ç”¨ knowledge_filesï¼ˆåˆ—è¡¨ï¼‰
            self.knowledge_files = knowledge_files if isinstance(knowledge_files, list) else [knowledge_files]
        elif knowledge_file is not None:
            # å‘åå…¼å®¹ï¼šå¦‚æœåªæä¾›äº† knowledge_fileï¼Œè½¬æ¢ä¸ºåˆ—è¡¨
            self.knowledge_files = [knowledge_file]
        else:
            # é»˜è®¤ä½¿ç”¨ç‹¬ç«‹çš„çŸ¥è¯†æ–‡ä»¶ï¼Œé¿å…ä¸ç³»ç»Ÿæç¤ºè¯é‡å¤
            self.knowledge_files = ["knowledge/core/default_knowledge.md"]
        
        # çŸ¥è¯†å­—ç¬¦ä¸² - ç›´æ¥æä¾›çŸ¥è¯†å†…å®¹
        if knowledge_strings is not None:
            self.knowledge_strings = knowledge_strings if isinstance(knowledge_strings, list) else [knowledge_strings]
        else:
            self.knowledge_strings = []
        
        # ç³»ç»Ÿæç¤ºè¯æ–‡ä»¶è·¯å¾„
        self.system_prompt_file = system_prompt_file
        # å·¥å…·è§„èŒƒå’Œç”¨é€”æè¿°
        self.interface = interface
        
        # LLM é…ç½® - å¦‚æœæœªæä¾›ï¼Œä½¿ç”¨ DeepSeek é»˜è®¤å€¼
        self.llm_model = llm_model or "deepseek-chat"
        self.llm_base_url = llm_base_url or "https://api.deepseek.com/v1"
        self.llm_api_key_env = llm_api_key_env or "DEEPSEEK_API_KEY"
        self.llm_temperature = llm_temperature
        self.http_client = http_client
        
        # è®¾ç½®ä¸Šä¸‹æ–‡çª—å£å¤§å°
        if context_window is not None:
            self.context_window = context_window
        else:
            # æ ¹æ®æ¨¡å‹åç§°è‡ªåŠ¨æ¨æ–­ä¸Šä¸‹æ–‡çª—å£å¤§å°
            self.context_window = DEFAULT_CONTEXT_WINDOWS.get(
                self.llm_model, 
                DEFAULT_CONTEXT_WINDOWS["default"]
            )
        
        # æ ¹æ®ä¸Šä¸‹æ–‡çª—å£è‡ªåŠ¨è°ƒæ•´ max_token_limitï¼ˆå¦‚æœä½¿ç”¨é»˜è®¤å€¼ï¼‰
        if max_token_limit == 30000 and self.context_window != DEFAULT_CONTEXT_WINDOWS["default"]:
            # ä½¿ç”¨ä¸Šä¸‹æ–‡çª—å£çš„ 80% ä½œä¸º max_token_limit
            # é¢„ç•™ 20% ç»™ç³»ç»Ÿæç¤ºè¯ã€å·¥å…·è¾“å‡ºç­‰
            self.max_token_limit = int(self.context_window * 0.8)
            if os.environ.get('DEBUG'):
                logger.info(f"Auto-adjusted max_token_limit to {self.max_token_limit} based on context window {self.context_window}")
        
        # ç¼“å­˜é…ç½®
        self.cache_path = cache_path
        self.enable_cache = enable_cache
        
        # çŸ¥è¯†æå–é™åˆ¶
        if knowledge_extraction_limit is not None:
            self.knowledge_extraction_limit = knowledge_extraction_limit
        else:
            # æ ¹æ®æ¨¡å‹åç§°è‡ªåŠ¨æ¨æ–­çŸ¥è¯†æå–æ–‡ä»¶å¤§å°é™åˆ¶
            self.knowledge_extraction_limit = DEFAULT_KNOWLEDGE_EXTRACTION_LIMITS.get(
                self.llm_model,
                DEFAULT_KNOWLEDGE_EXTRACTION_LIMITS["default"]
            )
        
        # è§†å›¾åŠŸèƒ½å¼€å…³
        # enable_world_overview å·²åºŸå¼ƒï¼Œä¿ç•™å‚æ•°æ˜¯ä¸ºäº†å‘åå…¼å®¹
        # self.enable_world_overview = enable_world_overview
        self.enable_perspective = enable_perspective
        self.show_memory_updates = show_memory_updates
        
        # é¡¹ç›®æ¢ç´¢é…ç½®
        self.enable_project_exploration = enable_project_exploration
        self.exploration_interval = exploration_interval
        self.exploration_prompt = exploration_prompt
        self.exploration_prompt_file = exploration_prompt_file
        self.auto_reload_on_exploration = auto_reload_on_exploration

class GenericReactAgent:
    """é€šç”¨ React Agent - é¢†åŸŸæ— å…³
    
    åŸºäº LangChain çš„ Reactï¼ˆReasoning and Actingï¼‰Agent å®ç°ã€‚
    æ”¯æŒæ–‡ä»¶æ“ä½œã€å‘½ä»¤æ‰§è¡Œã€çŸ¥è¯†æ³¨å…¥ç­‰åŠŸèƒ½ã€‚
    
    ä¸»è¦ç»„ä»¶ï¼š
    - LLM: ä½¿ç”¨é…ç½®çš„è¯­è¨€æ¨¡å‹
    - å·¥å…·é›†: æ–‡ä»¶è¯»å†™ã€ç›®å½•æ“ä½œã€å‘½ä»¤æ‰§è¡Œã€æ–‡ä»¶æœç´¢
    - è®°å¿†ç³»ç»Ÿ: æ”¯æŒä¸‰çº§è®°å¿†ç®¡ç†
    - çŸ¥è¯†ç³»ç»Ÿ: æ”¯æŒå¤šæ–‡ä»¶å’Œ include æœºåˆ¶
    
    Attributes:
        config: Agent é…ç½®å¯¹è±¡
        work_dir: å·¥ä½œç›®å½•è·¯å¾„
        llm: è¯­è¨€æ¨¡å‹å®ä¾‹
        memory: è®°å¿†ç³»ç»Ÿå®ä¾‹
        prior_knowledge: åŠ è½½çš„å…ˆéªŒçŸ¥è¯†
        system_prompt_template: ç³»ç»Ÿæç¤ºè¯æ¨¡æ¿
    """
    
    def __init__(self, config: ReactAgentConfig, name: Optional[str] = None, custom_tools: Optional[List[Any]] = None):
        self.config = config
        # ç¡®ä¿å·¥ä½œç›®å½•æ˜¯ç»å¯¹è·¯å¾„ï¼ˆconfig.work_dirå·²ç»æ˜¯ç»å¯¹è·¯å¾„ï¼‰
        self.work_dir = Path(config.work_dir).resolve()
        self.name = name or f"Agent-{config.llm_model}"  # Agent åç§°ï¼Œå¦‚æœæœªæä¾›åˆ™ä½¿ç”¨é»˜è®¤å€¼
        
        # éªŒè¯å·¥ä½œç›®å½•å¿…é¡»å­˜åœ¨
        if not self.work_dir.exists():
            raise ValueError(
                f"å·¥ä½œç›®å½• '{self.work_dir}' ä¸å­˜åœ¨ã€‚\n"
                "å·¥ä½œç›®å½•ä»£è¡¨å¤–éƒ¨ä¸–ç•Œï¼ˆå¦‚é¡¹ç›®ä»£ç åº“ï¼‰ï¼Œå¿…é¡»é¢„å…ˆå­˜åœ¨ã€‚\n"
                "Agent ä¸èƒ½åˆ›å»ºå·¥ä½œç›®å½•ï¼Œåªèƒ½åœ¨å…¶ä¸­æ“ä½œæ–‡ä»¶ã€‚"
            )
        
        # åˆå§‹åŒ–ç¯å¢ƒè®¤çŸ¥ï¼ˆå–ä»£ world_overviewï¼‰
        
        # è®¾ç½® Agent å†…éƒ¨å­˜å‚¨ç›®å½•ï¼ˆç‹¬ç«‹äºå·¥ä½œç›®å½•ï¼‰
        if config.agent_home:
            self.agent_home = Path(config.agent_home).expanduser()
        else:
            # é»˜è®¤ä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•çš„ .agents ç›®å½•
            self.agent_home = Path(__file__).parent / ".agents"
        
        # åˆ›å»º agent ç›®å½•
        self.agent_dir = self.agent_home / self.name
        self.agent_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºçŸ­æœŸæ•°æ®ç›®å½•
        self.data_dir = self.agent_dir / "short_term_data"
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºé•¿æœŸæ•°æ®ç›®å½•
        self.knowledge_dir = self.agent_dir / "long_term_data"
        self.knowledge_dir.mkdir(parents=True, exist_ok=True)
        self.knowledge_file = self.knowledge_dir / "extracted_knowledge.md"
        
        # æ³¨å†Œé€€å‡ºå¤„ç†å‡½æ•°ï¼ˆç¡®ä¿è®°å¿†æ›´æ–°å®Œæˆï¼‰
        _register_shutdown_handler()
        
        # åº”ç”¨ç¼“å­˜é…ç½®
        self._setup_cache()
        
        # åˆå§‹åŒ–æ›´æ–°æ ‡è®°
        self._pending_reload = False
        
        self.llm = self._create_llm()
        self.memory = self._create_memory()
        self.prior_knowledge = self._load_prior_knowledge()  # ç°åœ¨åŒ…å«äº†é¡¹ç›®ç†è§£
        self.system_prompt_template = self._load_system_prompt()
        self.interface = config.interface or self._get_default_interface()
        
        # åˆå§‹åŒ– agent å’Œ executor
        self._agent = None
        self._executor = None
        self._tools: Optional[List[Any]] = custom_tools  # æ”¯æŒè‡ªå®šä¹‰å·¥å…·
        self._system_prompt = None  # ç³»ç»Ÿæç¤ºè¯
        self._create_and_setup_agent()
        
        # åªåœ¨è°ƒè¯•æ¨¡å¼ä¸‹æ˜¾ç¤ºåˆå§‹åŒ–ä¿¡æ¯
        if os.environ.get('DEBUG'):
            logger.info(f"[{self.name}] initialized with memory level: {config.memory_level}")
            logger.info(f"[{self.name}] LLM model: {config.llm_model}, context window: {config.context_window} tokens")
            logger.info(f"[{self.name}] Max token limit for memory: {config.max_token_limit}")
            if self.prior_knowledge:
                logger.info(f"Loaded prior knowledge from: {config.knowledge_files}")
            if self.system_prompt_template:
                logger.info(f"Loaded system prompt from: {config.system_prompt_file}")
    
    def _setup_cache(self) -> None:
        """è®¾ç½®ç¼“å­˜é…ç½®
        
        æ ¹æ®é…ç½®å†³å®šæ˜¯å¦å¯ç”¨ç¼“å­˜ï¼Œä»¥åŠä½¿ç”¨å“ªä¸ªç¼“å­˜è·¯å¾„ã€‚
        æ”¯æŒä¸‰ç§æ¨¡å¼ï¼š
        1. ä½¿ç”¨å…¨å±€é»˜è®¤ç¼“å­˜ï¼ˆé»˜è®¤ï¼‰
        2. ä½¿ç”¨è‡ªå®šä¹‰ç¼“å­˜è·¯å¾„
        3. ç¦ç”¨ç¼“å­˜
        """
        # å¦‚æœé…ç½®ç¦ç”¨ç¼“å­˜
        if not self.config.enable_cache:
            set_llm_cache(None)
            if os.environ.get('DEBUG'):
                logger.info("LLM cache disabled for this agent")
            return
        
        # å¦‚æœæŒ‡å®šäº†è‡ªå®šä¹‰ç¼“å­˜è·¯å¾„
        if self.config.cache_path:
            # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
            cache_dir = os.path.dirname(self.config.cache_path)
            if cache_dir:
                os.makedirs(cache_dir, exist_ok=True)
            
            # è®¾ç½®è‡ªå®šä¹‰ç¼“å­˜
            set_llm_cache(SQLiteCache(database_path=self.config.cache_path))
            if os.environ.get('DEBUG'):
                logger.info(f"Using custom cache path: {self.config.cache_path}")
        else:
            # ä½¿ç”¨é»˜è®¤å…¨å±€ç¼“å­˜ï¼ˆå·²åœ¨æ¨¡å—çº§åˆ«è®¾ç½®ï¼‰
            if os.environ.get('DEBUG'):
                logger.info(f"Using default global cache at: {default_cache_path}")
    
    def _get_default_interface(self) -> str:
        """è·å–é»˜è®¤çš„å·¥å…·è§„èŒƒæè¿°"""
        return """GenericReactAgent - é€šç”¨ä»»åŠ¡æ‰§è¡Œå·¥å…·
        
ç”¨é€”ï¼š
- æ‰§è¡Œå„ç§ç¼–ç¨‹å’Œæ–‡ä»¶æ“ä½œä»»åŠ¡
- åˆ›å»ºã€è¯»å–ã€ä¿®æ”¹æ–‡ä»¶å’Œç›®å½•
- æ‰§è¡Œç³»ç»Ÿå‘½ä»¤
- æ ¹æ®å…ˆéªŒçŸ¥è¯†ç”Ÿæˆç‰¹å®šé¢†åŸŸçš„ä»£ç 

èƒ½åŠ›ï¼š
- æ–‡ä»¶ç³»ç»Ÿæ“ä½œï¼ˆåˆ›å»ºã€è¯»å–ã€å†™å…¥æ–‡ä»¶ï¼‰
- ç›®å½•ç®¡ç†ï¼ˆåˆ›å»ºã€åˆ—å‡ºç›®å½•å†…å®¹ï¼‰
- å‘½ä»¤æ‰§è¡Œï¼ˆè¿è¡Œç³»ç»Ÿå‘½ä»¤ï¼‰
- æ–‡ä»¶æœç´¢ï¼ˆæŒ‰æ¨¡å¼æœç´¢æ–‡ä»¶ï¼‰

é€‚ç”¨åœºæ™¯ï¼š
- ä»£ç ç”Ÿæˆ
- é¡¹ç›®åˆå§‹åŒ–
- æ–‡ä»¶æ‰¹é‡å¤„ç†
- è‡ªåŠ¨åŒ–è„šæœ¬ç¼–å†™
"""
        
    def _create_llm(self):
        """åˆ›å»ºè¯­è¨€æ¨¡å‹
        
        åˆå§‹åŒ–è¯­è¨€æ¨¡å‹ï¼Œæ”¯æŒä»»ä½• OpenAI å…¼å®¹çš„ APIï¼š
        1. ä»ç¯å¢ƒå˜é‡åŠ è½½ API å¯†é’¥
        2. ä½¿ç”¨é…ç½®çš„æ¨¡å‹å‚æ•°
        
        Returns:
            ChatOpenAI: é…ç½®å¥½çš„è¯­è¨€æ¨¡å‹å®ä¾‹
            
        Raises:
            ValueError: å¦‚æœ API å¯†é’¥æœªè®¾ç½®
        """
        # ä»é…ç½®çš„ç¯å¢ƒå˜é‡åè·å– API å¯†é’¥
        api_key = os.getenv(self.config.llm_api_key_env)
        if not api_key:
            raise ValueError(f"{self.config.llm_api_key_env} not set")
        
        # åˆ›å»ºChatOpenAIå®ä¾‹ï¼Œæ”¯æŒä»»ä½•å…¼å®¹çš„API
        llm_kwargs = {
            "model": self.config.llm_model,
            "api_key": api_key,  # type: ignore
            "base_url": self.config.llm_base_url,
            "temperature": self.config.llm_temperature,
        }
        
        # è®¾ç½®max_tokens
        llm_kwargs["max_tokens"] = 16384  # é»˜è®¤16kï¼Œé€‚åˆå¤§å¤šæ•°æ¨¡å‹
        
        # å¦‚æœæä¾›äº† http_clientï¼Œæ·»åŠ åˆ°å‚æ•°ä¸­
        if self.config.http_client:
            llm_kwargs["http_client"] = self.config.http_client
            
        llm = ChatOpenAI(**llm_kwargs)
        
        if os.environ.get('DEBUG'):
            logger.info(f"LLM initialized: model={self.config.llm_model}, base_url={self.config.llm_base_url}")
        
        return llm
    
    def _create_memory(self):
        """æ ¹æ®é…ç½®åˆ›å»ºè®°å¿†ç³»ç»Ÿ
        
        åŸºäº memory_level åˆ›å»ºä¸åŒç±»å‹çš„è®°å¿†ç³»ç»Ÿï¼š
        - NONE: è¿”å› Noneï¼Œæ— çŠ¶æ€å¯¹è¯
        - SMART: åˆ›å»ºçª—å£ç¼“å†²è®°å¿†ï¼Œä¿æŒæœ€è¿‘ k è½®å¯¹è¯
        - PRO: åˆ›å»º SQLite æŒä¹…åŒ–è®°å¿†ï¼Œæ”¯æŒè·¨ä¼šè¯
        
        Returns:
            Optional[ConversationBufferMemory]: è®°å¿†ç³»ç»Ÿå®ä¾‹æˆ– None
        """
        if self.config.memory_level == MemoryLevel.NONE:
            if os.environ.get('DEBUG'):
                logger.info("Memory disabled - using stateless mode")
            return None
            
        elif self.config.memory_level == MemoryLevel.SMART:
            # ä½¿ç”¨è‡ªå®šä¹‰çš„æ‘˜è¦ç¼“å†²è®°å¿†ï¼Œè‡ªåŠ¨ç®¡ç†å†å²å¯¹è¯
            # ä¿ç•™æœ€è¿‘çš„å¯¹è¯åŸæ–‡ï¼Œå¯¹è¶…å‡ºé™åˆ¶çš„éƒ¨åˆ†è¿›è¡Œæ‘˜è¦
            if os.environ.get('DEBUG'):
                logger.info(f"Using smart memory (custom summary buffer) with max_token_limit={self.config.max_token_limit}")
            
            return CustomSummaryBufferMemory(
                llm=self.llm,
                max_token_limit=self.config.max_token_limit,
                memory_key="chat_history",
                return_messages=True
            )
            
        elif self.config.memory_level == MemoryLevel.PRO:
            if os.environ.get('DEBUG'):
                logger.info(f"Using persistent memory (SQLite) - session: {self.config.session_id}")
            # ç¡®ä¿æ•°æ®åº“ç›®å½•å­˜åœ¨
            db_dir = os.path.dirname(self.config.db_path)
            if db_dir:
                os.makedirs(db_dir, exist_ok=True)
                
            message_history = SQLChatMessageHistory(
                session_id=self.config.session_id,
                connection_string=f"sqlite:///{self.config.db_path}"
            )
            return ConversationBufferMemory(
                memory_key="chat_history",
                chat_memory=message_history,
                return_messages=True
            )
    
    def _load_prior_knowledge(self) -> str:
        """åŠ è½½å…ˆéªŒçŸ¥è¯†ï¼Œæ”¯æŒå¤šä¸ªæ–‡ä»¶å’Œ include æœºåˆ¶
        
        åŠŸèƒ½ï¼š
        1. æ”¯æŒåŠ è½½å¤šä¸ªçŸ¥è¯†æ–‡ä»¶
        2. æ”¯æŒ @include è¯­æ³•å¼•å…¥å…¶ä»–æ–‡ä»¶
        3. è‡ªåŠ¨å¤„ç†è·¯å¾„è§£æï¼ˆç»å¯¹è·¯å¾„ã€ç›¸å¯¹è·¯å¾„ï¼‰
        4. é˜²æ­¢å¾ªç¯å¼•ç”¨
        5. è½¬ä¹‰å¤§æ‹¬å·é¿å…æ¨¡æ¿å†²çª
        
        Returns:
            str: åˆå¹¶åçš„æ‰€æœ‰çŸ¥è¯†å†…å®¹
        """
        def load_file_with_includes(file_path: Path, loaded_files: Optional[set] = None) -> str:
            """é€’å½’åŠ è½½æ–‡ä»¶ï¼Œå¤„ç† include è¯­å¥
            
            Args:
                file_path: è¦åŠ è½½çš„æ–‡ä»¶è·¯å¾„
                loaded_files: å·²åŠ è½½æ–‡ä»¶é›†åˆï¼Œç”¨äºé˜²æ­¢å¾ªç¯å¼•ç”¨
                
            Returns:
                str: æ–‡ä»¶å†…å®¹ï¼Œinclude è¯­å¥è¢«æ›¿æ¢ä¸ºå®é™…å†…å®¹
            """
            if loaded_files is None:
                loaded_files = set()
            
            # é˜²æ­¢å¾ªç¯å¼•ç”¨
            abs_path = file_path.resolve()
            if abs_path in loaded_files:
                logger.warning(f"Circular reference detected: {file_path}")
                return f"[Circular reference: {file_path}]"
            
            loaded_files.add(abs_path)
            
            if not file_path.exists():
                return f"[File not found: {file_path}]"
            
            try:
                content = file_path.read_text(encoding='utf-8')
                lines = content.split('\n')
                result_lines = []
                
                import re
                # åŒ¹é… include è¯­å¥çš„æ¨¡å¼ï¼š@include [æ–‡ä»¶å.md]
                include_pattern = re.compile(r'@include\s*\[([^\]]+\.md)\]')
                
                for line in lines:
                    matches = include_pattern.findall(line)
                    if matches:
                        # å¤„ç† include
                        for match in matches:
                            included_path = Path(match)
                            
                            # å°è¯•å¤šç§è·¯å¾„è§£ææ–¹å¼
                            paths_to_try = []
                            
                            # 1. å¦‚æœæ˜¯ç»å¯¹è·¯å¾„ï¼Œç›´æ¥ä½¿ç”¨
                            if included_path.is_absolute():
                                paths_to_try.append(included_path)
                            else:
                                # 2. ç›¸å¯¹äºå½“å‰æ–‡ä»¶çš„ç›®å½•
                                paths_to_try.append(file_path.parent / match)
                                
                                # 3. ç›¸å¯¹äºè„šæœ¬ç›®å½•
                                script_dir = Path(__file__).parent
                                paths_to_try.append(script_dir / match)
                                
                                # 4. ç›¸å¯¹äºå½“å‰å·¥ä½œç›®å½•
                                paths_to_try.append(Path.cwd() / match)
                            
                            # å°è¯•æ‰¾åˆ°æ–‡ä»¶
                            found_path = None
                            for path in paths_to_try:
                                if path.exists():
                                    found_path = path
                                    break
                            
                            if found_path:
                                # æ›¿æ¢ include è¯­å¥ä¸ºæ–‡ä»¶å†…å®¹
                                included_content = load_file_with_includes(found_path, loaded_files)
                                # æ·»åŠ æ ‡è®°è¡¨æ˜è¿™æ˜¯å¼•å…¥çš„å†…å®¹
                                line = line.replace(f'[{match}]', f'\n<!-- Included from {match} -->\n{included_content}\n<!-- End of {match} -->')
                            else:
                                line = line.replace(f'[{match}]', f'[File not found: {match}]')
                    
                    result_lines.append(line)
                
                return '\n'.join(result_lines)
                
            except Exception as e:
                logger.warning(f"Failed to load file {file_path}: {e}")
                return f"[Error loading file: {file_path}]"
        
        # åŠ è½½æ‰€æœ‰çŸ¥è¯†æ–‡ä»¶
        all_knowledge = []
        
        for knowledge_file in self.config.knowledge_files:
            knowledge_path = Path(knowledge_file)
            
            # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œå…ˆå°è¯•è„šæœ¬ç›®å½•
            if not knowledge_path.is_absolute() and not knowledge_path.exists():
                script_dir = Path(__file__).parent
                knowledge_path = script_dir / knowledge_file
            
            if knowledge_path.exists():
                try:
                    content = load_file_with_includes(knowledge_path)
                    if content:
                        # æ·»åŠ æ–‡ä»¶æ ‡è®°
                        all_knowledge.append(f"<!-- Knowledge from {knowledge_file} -->\n{content}")
                        if os.environ.get('DEBUG'):
                            logger.info(f"Loaded knowledge from: {knowledge_file}")
                except Exception as e:
                    logger.warning(f"Failed to load knowledge from {knowledge_file}: {e}")
            else:
                if os.environ.get('DEBUG'):
                    logger.info(f"Knowledge file not found: {knowledge_file}")
        
        # æ·»åŠ çŸ¥è¯†å­—ç¬¦ä¸²
        for i, knowledge_string in enumerate(self.config.knowledge_strings):
            if knowledge_string:
                # æ·»åŠ å­—ç¬¦ä¸²æ ‡è®°
                all_knowledge.append(f"<!-- Knowledge from string {i+1} -->\n{knowledge_string}")
                if os.environ.get('DEBUG'):
                    logger.info(f"Loaded knowledge from string {i+1}")
        
        # åŠ è½½é¡¹ç›®ç†è§£ä½œä¸ºæ ¸å¿ƒçŸ¥è¯†ï¼ˆæ¨¡å‹é©±åŠ¨å¼€å‘ç†å¿µï¼‰
        understanding_file = self.knowledge_dir / "project_understanding.md"
        if understanding_file.exists():
            try:
                project_understanding = understanding_file.read_text(encoding='utf-8')
                if project_understanding.strip():
                    # é¡¹ç›®ç†è§£ï¼ˆUMLæ¨¡å‹ï¼‰æ˜¯æœ€é«˜ä»·å€¼çš„çŸ¥è¯†
                    all_knowledge.insert(0, f"<!-- Project Architecture Model (Core Knowledge) -->\n{project_understanding}\n<!-- End of Project Architecture Model -->")
                    if os.environ.get('DEBUG'):
                        logger.info(f"Loaded project understanding as core knowledge ({len(project_understanding)} chars)")
            except Exception as e:
                logger.warning(f"Failed to load project understanding: {e}")
        
        # åˆå¹¶æ‰€æœ‰çŸ¥è¯†å†…å®¹
        if all_knowledge:
            combined_content = "\n\n".join(all_knowledge)
            
            # è½¬ä¹‰å•ä¸ªå¤§æ‹¬å·ä»¥é¿å…è¢«è¯¯è®¤ä¸ºæ˜¯æ¨¡æ¿å˜é‡
            # åªè½¬ä¹‰æœªæˆå¯¹çš„å¤§æ‹¬å·
            import re
            # å…ˆä¿æŠ¤å·²ç»è½¬ä¹‰çš„å¤§æ‹¬å·
            combined_content = combined_content.replace('{{', '\x00DOUBLE_OPEN\x00')
            combined_content = combined_content.replace('}}', '\x00DOUBLE_CLOSE\x00')
            # è½¬ä¹‰å•ä¸ªå¤§æ‹¬å·
            combined_content = combined_content.replace('{', '{{')
            combined_content = combined_content.replace('}', '}}')
            # æ¢å¤å·²ç»è½¬ä¹‰çš„å¤§æ‹¬å·
            combined_content = combined_content.replace('\x00DOUBLE_OPEN\x00', '{{')
            combined_content = combined_content.replace('\x00DOUBLE_CLOSE\x00', '}}')
            
            return combined_content
        else:
            return ""
    
    
    def _load_system_prompt(self) -> str:
        """åŠ è½½ç³»ç»Ÿæç¤ºè¯æ¨¡æ¿
        
        å°è¯•ä»ä»¥ä¸‹ä½ç½®åŠ è½½ç³»ç»Ÿæç¤ºè¯ï¼š
        1. æŒ‡å®šçš„æ–‡ä»¶è·¯å¾„
        2. è„šæœ¬æ‰€åœ¨ç›®å½•
        3. å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼Œä½¿ç”¨å†…ç½®é»˜è®¤æç¤ºè¯
        
        Returns:
            str: ç³»ç»Ÿæç¤ºè¯æ¨¡æ¿å†…å®¹
        """
        prompt_path = Path(self.config.system_prompt_file)
        if prompt_path.exists():
            try:
                content = prompt_path.read_text(encoding='utf-8')
                return content
            except Exception as e:
                logger.warning(f"Failed to load system prompt: {e}")
                # è¿”å›é»˜è®¤çš„ç³»ç»Ÿæç¤ºè¯
                return self._get_default_system_prompt()
        else:
            # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•ä»è„šæœ¬æ‰€åœ¨ç›®å½•æŸ¥æ‰¾
            script_dir = Path(__file__).parent
            prompt_path = script_dir / self.config.system_prompt_file
            if prompt_path.exists():
                try:
                    content = prompt_path.read_text(encoding='utf-8')
                    return content
                except Exception as e:
                    logger.warning(f"Failed to load system prompt from script dir: {e}")
            
            if os.environ.get('DEBUG'):
                logger.info(f"No system prompt file found, using default")
            return self._get_default_system_prompt()
    
    def _get_default_system_prompt(self) -> str:
        """è·å–é»˜è®¤çš„ç³»ç»Ÿæç¤ºè¯"""
        return """ä½ æ˜¯ä¸€ä¸ªä»»åŠ¡æ‰§è¡ŒåŠ©æ‰‹ï¼Œèƒ½å¤Ÿä½¿ç”¨å„ç§å·¥å…·å®Œæˆä»»åŠ¡ã€‚

## æ ¸å¿ƒåŸåˆ™

1. **ç«‹å³æ‰§è¡Œ**ï¼šæ”¶åˆ°ä»»åŠ¡åç›´æ¥æ‰§è¡Œï¼Œé¿å…è¿‡åº¦åˆ†æ
2. **å·¥å…·ä¼˜å…ˆ**ï¼šä½¿ç”¨å·¥å…·å®Œæˆå®é™…å·¥ä½œï¼Œè€Œä¸æ˜¯åªæè¿°æ­¥éª¤
3. **ç®€æ´æ²Ÿé€š**ï¼šä¿æŒå›å¤ç®€æ´ï¼Œé‡ç‚¹æ˜¯æ‰§è¡Œç»“æœ

## å·¥ä½œæµç¨‹

1. **ç†è§£ä»»åŠ¡**ï¼ˆ1-2å¥è¯ï¼‰
2. **æ‰§è¡Œæ“ä½œ**ï¼ˆä½¿ç”¨ç›¸å…³å·¥å…·ï¼‰
3. **æŠ¥å‘Šç»“æœ**ï¼ˆç®€è¦è¯´æ˜ï¼‰
4. **å¤„ç†å¼‚å¸¸**ï¼ˆé‡åˆ°é”™è¯¯æ—¶åˆ†æå¹¶é‡è¯•ï¼‰

## é—®é¢˜è§£å†³ç­–ç•¥

### é‡åˆ°å›°éš¾æ—¶
- å°è¯•å¤šä¸ªè§£å†³æ–¹æ¡ˆï¼ˆ2-3ä¸ªï¼‰
- å¦‚æœéƒ½å¤±è´¥ï¼Œé‡æ–°å®¡è§†ç›®æ ‡
- è€ƒè™‘æ›¿ä»£æ–¹æ¡ˆ
- å¿…è¦æ—¶å¯»æ±‚æ›´å¤šä¿¡æ¯

## å·¥ä½œç©ºé—´

### å¤–éƒ¨ä¸–ç•Œ
- å·¥ä½œç›®å½•ï¼š{work_dir}
- ä»£è¡¨éœ€è¦æ“ä½œçš„å¤–éƒ¨ç¯å¢ƒ
- åªèƒ½ä¿®æ”¹å†…å®¹ï¼Œä¸èƒ½åˆ é™¤æˆ–æ¸…ç†æ•´ä¸ªç›®å½•

### ä»»åŠ¡ä¸´æ—¶åŒº
- ä½ç½®ï¼š.agents/<agent_name>/short_term_data/
- ç”¨äºå­˜å‚¨ä»»åŠ¡æ‰§è¡Œçš„ä¸´æ—¶æ•°æ®
- æ¯ä¸ªä»»åŠ¡å¼€å§‹æ—¶ä¼šè¢«æ¸…ç©º

## æ³¨æ„äº‹é¡¹

- ä¿æŒä¸“æ³¨äºå½“å‰ä»»åŠ¡
- é¿å…åˆ›å»ºä¸å¿…è¦çš„æ–‡ä»¶
- ä¼˜å…ˆä¿®æ”¹ç°æœ‰æ–‡ä»¶è€Œéåˆ›å»ºæ–°æ–‡ä»¶
- ç¡®ä¿äº§ç”Ÿå®é™…è¾“å‡º"""
    
    def _create_agent(self):
        """åˆ›å»º Agent
        
        åˆ›å»º LangGraph React Agentï¼ŒåŒ…æ‹¬ï¼š
        1. åˆ›å»ºæ‰€æœ‰å·¥å…·å®ä¾‹
        2. æ„å»ºç³»ç»Ÿæç¤ºè¯ï¼ˆç»“åˆæ¨¡æ¿å’ŒçŸ¥è¯†ï¼‰
        3. åˆ›å»º LangGraph React Agent
        
        Returns:
            Agent: é…ç½®å¥½çš„ LangGraph Agent
        """
        
        # åˆ›å»ºå·¥å…·é›†ï¼šé»˜è®¤å·¥å…· + è‡ªå®šä¹‰å·¥å…·
        default_tools = create_tools(self.config.work_dir)
        
        if self._tools is None:
            # æ²¡æœ‰æä¾›è‡ªå®šä¹‰å·¥å…·ï¼Œåªä½¿ç”¨é»˜è®¤å·¥å…·é›†
            tools = default_tools
        else:
            # åˆå¹¶é»˜è®¤å·¥å…·å’Œè‡ªå®šä¹‰å·¥å…·
            tools = default_tools + self._tools
        
        # ä½¿ç”¨åŠ è½½çš„ç³»ç»Ÿæç¤ºè¯æ¨¡æ¿ï¼ˆä¸å†éœ€è¦ task_descriptionï¼‰
        system_prompt = self.system_prompt_template.format(
            work_dir=self.config.work_dir
        )
        
        
        # æ³¨å…¥æ•°æ®ç›®å½•ä¿¡æ¯
        data_dir_info = f"""
## å·¥ä½œåŒºåŸŸ
- å…±äº«å·¥ä½œåŒºï¼š{self.config.work_dir}
- ä½ çš„çŸ­æœŸæ•°æ®å­˜å‚¨åŒºï¼š{self.data_dir}
"""
        system_prompt += data_dir_info
        
        # æ³¨å…¥æå–çš„çŸ¥è¯†
        knowledge_file = self.knowledge_dir / "extracted_knowledge.md"
        if knowledge_file.exists():
            try:
                knowledge_content = knowledge_file.read_text(encoding='utf-8')
                if knowledge_content.strip():
                    system_prompt += f"""
## æå–çš„çŸ¥è¯†

ä»¥ä¸‹æ˜¯ä½ ä¹‹å‰ç§¯ç´¯çš„ç»éªŒå’ŒçŸ¥è¯†ï¼Œè¯·åœ¨æ‰§è¡Œä»»åŠ¡æ—¶å‚è€ƒï¼š

{knowledge_content}
"""
            except Exception as e:
                print(f"è¯»å–æå–çš„çŸ¥è¯†å¤±è´¥: {e}")
        
        # å¦‚æœæœ‰å…ˆéªŒçŸ¥è¯†ï¼Œæ·»åŠ åˆ°æç¤ºè¯ä¸­
        if self.prior_knowledge:
            system_prompt += f"""
## æ ¸å¿ƒçŸ¥è¯†ï¼ˆæ¨¡å‹é©±åŠ¨å¼€å‘ï¼‰

é‡è¦åŸåˆ™ï¼šè½¯ä»¶å¼€å‘çš„æœ¬è´¨æ˜¯å°†UMLæ¨¡å‹ç¿»è¯‘æˆä»£ç ã€‚é¡¹ç›®ç†è§£æ–‡æ¡£ï¼ˆProject Architecture Modelï¼‰æ˜¯æœ€é«˜ä»·å€¼çš„çŸ¥è¯†ï¼ŒåŒ…å«äº†å®Œæ•´çš„UMLè§†å›¾ã€‚ä½ çš„ä»»åŠ¡æ˜¯åŸºäºè¿™äº›æ¨¡å‹ç”Ÿæˆå’Œç»´æŠ¤ä»£ç ã€‚

ä»¥ä¸‹æ˜¯ç›¸å…³çš„é¢†åŸŸçŸ¥è¯†å’Œé¡¹ç›®æ¨¡å‹ï¼Œè¯·åœ¨æ‰§è¡Œä»»åŠ¡æ—¶ä¸¥æ ¼å‚è€ƒï¼š

{self.prior_knowledge}
"""

        # LangGraph ä½¿ç”¨ä¸åŒçš„æ–¹å¼å¤„ç†ç³»ç»Ÿæç¤ºè¯
        # æˆ‘ä»¬å°†åœ¨æ‰§è¡Œæ—¶å°†ç³»ç»Ÿæç¤ºè¯ä½œä¸ºç¬¬ä¸€æ¡æ¶ˆæ¯æ³¨å…¥
        self._system_prompt = system_prompt
        
        # åˆ›å»ºagent - ä½¿ç”¨ LangGraph çš„ create_react_agent
        # LangGraph çš„ create_react_agent æ”¯æŒ prompt å‚æ•°
        # å¯ä»¥ä¼ å…¥å­—ç¬¦ä¸²æˆ– SystemMessageï¼Œå®ƒä¼šè¢«æ·»åŠ åˆ°æ¶ˆæ¯åˆ—è¡¨çš„å¼€å¤´
        agent = create_react_agent(
            model=self.llm,
            tools=tools,
            prompt=system_prompt if self.prior_knowledge else None,  # å¦‚æœæœ‰çŸ¥è¯†ï¼ˆåŒ…å«é¡¹ç›®ç†è§£ï¼‰ï¼Œä½¿ç”¨è‡ªå®šä¹‰æç¤ºè¯
            checkpointer=None  # æš‚æ—¶ä¸ä½¿ç”¨ checkpointer
        )
        
        # ä¿å­˜å·¥å…·åˆ—è¡¨åˆ°å®ä¾‹å±æ€§ï¼Œä¾›åç»­ä½¿ç”¨
        self._tools = tools
        
        return agent
    
    def _create_and_setup_agent(self):
        """åˆ›å»ºå¹¶è®¾ç½® agent
        
        åœ¨ LangGraph ä¸­ï¼Œagent æœ¬èº«å°±æ˜¯ executorï¼Œä¸éœ€è¦é¢å¤–åŒ…è£…ã€‚
        """
        # åˆ›å»º agent
        self._agent = self._create_agent()
        
        # ç¡®ä¿å·¥å…·å·²åˆå§‹åŒ–
        if self._tools is None:
            raise RuntimeError("Tools not initialized properly in _create_agent")
        
        # åœ¨ LangGraph ä¸­ï¼Œagent å°±æ˜¯ executor
        self._executor = self._agent
        
        if os.environ.get('DEBUG'):
            logger.info("Created new LangGraph React Agent")
    
    def load_knowledge(self, knowledge_content: str) -> None:
        """åŠ¨æ€åŠ è½½çŸ¥è¯†å†…å®¹
        
        å°†æ–°çš„çŸ¥è¯†å†…å®¹æ·»åŠ åˆ°ç°æœ‰çš„å…ˆéªŒçŸ¥è¯†ä¸­ã€‚
        åŠ è½½åä¼šé‡å»º agent ä»¥ä½¿æ–°çŸ¥è¯†ç”Ÿæ•ˆï¼Œä½†ä¼šä¿ç•™ memoryã€‚
        
        Args:
            knowledge_content: è¦åŠ è½½çš„çŸ¥è¯†æ–‡æœ¬
            
        Example:
            agent.load_knowledge("Python æœ€ä½³å®è·µï¼šä½¿ç”¨è™šæ‹Ÿç¯å¢ƒç®¡ç†ä¾èµ–")
        """
        if self.prior_knowledge:
            # å¦‚æœå·²æœ‰çŸ¥è¯†ï¼Œè¿½åŠ æ–°å†…å®¹
            self.prior_knowledge += f"\n\n<!-- åŠ¨æ€åŠ è½½çš„çŸ¥è¯† -->\n{knowledge_content}"
        else:
            # å¦‚æœæ²¡æœ‰çŸ¥è¯†ï¼Œç›´æ¥è®¾ç½®
            self.prior_knowledge = knowledge_content
        
        # é‡æ–°åˆ›å»º agentï¼ˆä¼šä½¿ç”¨æ–°çš„çŸ¥è¯†ï¼Œä½†ä¿ç•™ memoryï¼‰
        self._create_and_setup_agent()
        
        if os.environ.get('DEBUG'):
            logger.info(f"Loaded additional knowledge: {len(knowledge_content)} characters")
    
    def _update_extracted_knowledge_sync(self, messages: List[BaseMessage]) -> None:
        """åŒæ­¥æ–¹æ³•ï¼šæ›´æ–°æå–çš„çŸ¥è¯†ï¼ˆåœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œï¼‰"""
        try:
            # è¯»å–ç°æœ‰çŸ¥è¯†
            existing_knowledge = ""
            if self.knowledge_file.exists():
                existing_knowledge = self.knowledge_file.read_text(encoding='utf-8')
            
            # æ„å»ºä»»åŠ¡å†å²
            task_history = self._format_messages_for_memory(messages)
            
            # æ„å»ºæ›´æ–°çŸ¥è¯†çš„æç¤ºè¯
            knowledge_prompt = self._build_knowledge_extraction_prompt(existing_knowledge, task_history)
            
            # è°ƒç”¨ LLM æå–çŸ¥è¯†
            response = self.llm.invoke(knowledge_prompt)
            extracted_knowledge = response.content if isinstance(response.content, str) else str(response.content[0] if response.content else "")
            
            # æ£€æŸ¥çŸ¥è¯†æ–‡ä»¶å¤§å°
            knowledge_size = len(extracted_knowledge.encode('utf-8'))
            if knowledge_size > self.config.knowledge_extraction_limit:
                # å¦‚æœè¶…è¿‡é™åˆ¶ï¼Œè¦æ±‚ LLM è¿›ä¸€æ­¥å‹ç¼©
                print(f"\nğŸ“š [çŸ¥è¯†æå–] çŸ¥è¯†æ–‡ä»¶è¶…è¿‡é™åˆ¶ ({knowledge_size/1024:.1f}KB > {self.config.knowledge_extraction_limit/1024:.1f}KB)")
                print(f"   æ­£åœ¨å‹ç¼©çŸ¥è¯†å†…å®¹...")
                
                compress_prompt = self._build_knowledge_compression_prompt(
                    extracted_knowledge, 
                    knowledge_size, 
                    self.config.knowledge_extraction_limit
                )
                compress_response = self.llm.invoke(compress_prompt)
                extracted_knowledge = compress_response.content if isinstance(compress_response.content, str) else str(compress_response.content[0] if compress_response.content else "")
                
                compressed_size = len(extracted_knowledge.encode('utf-8'))
                print(f"   âœ… å‹ç¼©å®Œæˆï¼ä» {knowledge_size/1024:.1f}KB å‹ç¼©åˆ° {compressed_size/1024:.1f}KB")
            
            # ä¿å­˜æå–çš„çŸ¥è¯†
            self.knowledge_file.write_text(extracted_knowledge, encoding='utf-8')
            
            # é€šçŸ¥ç”¨æˆ·çŸ¥è¯†æå–å®Œæˆï¼ˆç»éªŒä¸»ä¹‰ï¼šå…ˆç®€å•æ‰“å°ï¼‰
            # è·å–çŸ¥è¯†çš„å‰å‡ è¡Œä½œä¸ºé¢„è§ˆ
            knowledge_preview = extracted_knowledge.split('\n')[0][:100]
            if len(knowledge_preview) == 100:
                knowledge_preview += "..."
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«é”™è¯¯çº æ­£ï¼ˆé‡è¦ä¿¡æ¯åº”è¯¥æ›´é†’ç›®ï¼‰
            if any(word in extracted_knowledge.lower() for word in ['é”™è¯¯', 'error', 'ä¿®æ­£', 'fix', 'å®é™…ä¸Š']):
                # é”™è¯¯çº æ­£å§‹ç»ˆæ˜¾ç¤ºï¼ˆç»éªŒæ•™è®­ï¼šé”™è¯¯ä¿¡æ¯å¿…é¡»è®©ç”¨æˆ·çŸ¥é“ï¼‰
                print(f"\nğŸš¨ [è®°å¿†æ›´æ–°] å‘ç°é”™è¯¯çº æ­£ï¼š{knowledge_preview}\n")
            elif self.config.show_memory_updates:
                # æ™®é€šå­¦ä¹ æ›´æ–°ï¼ˆæ ¹æ®é…ç½®å†³å®šæ˜¯å¦æ˜¾ç¤ºï¼‰
                print(f"\nğŸ’­ [è®°å¿†æ›´æ–°] {knowledge_preview}\n")
            
            if os.environ.get('DEBUG'):
                logger.info(f"[{self.name}] Knowledge extracted successfully")
                
        except Exception as e:
            # çŸ¥è¯†æå–å¤±è´¥ä¸åº”å½±å“ä¸»ä»»åŠ¡
            if os.environ.get('DEBUG'):
                logger.error(f"[{self.name}] Failed to extract knowledge: {e}")
    
    def _format_messages_for_memory(self, messages: List[BaseMessage]) -> str:
        """æ ¼å¼åŒ–æ¶ˆæ¯åˆ—è¡¨ä¸ºçŸ¥è¯†æå–æ‰€éœ€çš„æ ¼å¼"""
        formatted = []
        for msg in messages:
            if isinstance(msg, SystemMessage):
                continue  # è·³è¿‡ç³»ç»Ÿæ¶ˆæ¯
            elif isinstance(msg, HumanMessage):
                formatted.append(f"ç”¨æˆ·: {msg.content}")
            elif hasattr(msg, 'name') and msg.name:  # å·¥å…·æ¶ˆæ¯
                formatted.append(f"å·¥å…· {msg.name}: {msg.content[:500]}...")  # é™åˆ¶å·¥å…·è¾“å‡ºé•¿åº¦
            else:  # AI æ¶ˆæ¯
                formatted.append(f"AI: {msg.content}")
        return "\n".join(formatted)
    
    def _build_knowledge_extraction_prompt(self, existing_knowledge: str, task_history: str) -> str:
        """æ„å»ºçŸ¥è¯†æå–çš„æç¤ºè¯"""
        knowledge_limit_kb = self.config.knowledge_extraction_limit // 1024
        
        prompt = f"""# é•¿æœŸè®°å¿†æ›´æ–°ä»»åŠ¡

## ä½ çš„è§’è‰²
ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†ç®¡ç†åŠ©æ‰‹ï¼Œè´Ÿè´£ä» agent çš„ä»»åŠ¡æ‰§è¡Œä¸­æå–æœ‰ä»·å€¼çš„çŸ¥è¯†å’Œç»éªŒï¼Œå¹¶æŒ‰ç…§å››å±‚è®°å¿†æ¶æ„ç»„ç»‡ã€‚

## å››å±‚è®°å¿†æ¶æ„
1. **å…ƒçŸ¥è¯†å±‚**ï¼šå¦‚ä½•å­¦ä¹ ã€æŸ¥æ‰¾ã€éªŒè¯çš„æ–¹æ³•ï¼ˆæå°‘å˜åŒ–ï¼‰
2. **åŸç†å±‚**ï¼šè®¾è®¡ç†å¿µã€æ¶æ„å†³ç­–ã€æ ¸å¿ƒæ¦‚å¿µï¼ˆä»…é‡å¤§é‡æ„æ—¶æ›´æ–°ï¼‰
3. **æ¥å£å±‚**ï¼šAPIã€é…ç½®é¡¹ã€å…¬å…±æ–¹æ³•ï¼ˆç‰ˆæœ¬æ›´æ–°æ—¶éªŒè¯ï¼‰
4. **å®ç°å±‚**ï¼šå…·ä½“ä»£ç ä½ç½®ã€å†…éƒ¨å®ç°ï¼ˆæ¯æ¬¡ä½¿ç”¨å‰éªŒè¯ï¼‰

## è¾“å…¥
1. **å·²æœ‰çŸ¥è¯†**ï¼š
{existing_knowledge if existing_knowledge else "ï¼ˆç©ºï¼‰"}

2. **æœ¬æ¬¡ä»»åŠ¡æ‰§è¡Œå†å²**ï¼š
{task_history}

## ä»»åŠ¡
åŸºäºæœ¬æ¬¡ä»»åŠ¡çš„æ‰§è¡Œå†å²ï¼Œæ›´æ–°é•¿æœŸè®°å¿†ã€‚

**é‡è¦**ï¼šä½ å¿…é¡»æ•´åˆè€Œéè¦†ç›–ç°æœ‰çŸ¥è¯†ï¼
- å¦‚æœ"å·²æœ‰çŸ¥è¯†"ä¸ä¸ºç©ºï¼Œä½ å¿…é¡»ä¿ç•™å…¶ä¸­ä»ç„¶æœ‰æ•ˆçš„å†…å®¹
- åœ¨å·²æœ‰çŸ¥è¯†çš„åŸºç¡€ä¸Šæ·»åŠ æ–°å­¦åˆ°çš„å†…å®¹
- åªåˆ é™¤æ˜ç¡®è¿‡æ—¶æˆ–é”™è¯¯çš„éƒ¨åˆ†

å…·ä½“è¦æ±‚ï¼š

1. **æŒ‰å±‚çº§æå–ç»éªŒ**ï¼š
   - **å…ƒçŸ¥è¯†**ï¼šå‘ç°äº†ä»€ä¹ˆæ–°çš„æŸ¥æ‰¾ã€éªŒè¯æ–¹æ³•ï¼Ÿ
   - **åŸç†**ï¼šç†è§£äº†ä»€ä¹ˆè®¾è®¡ç†å¿µæˆ–æ¶æ„å†³ç­–ï¼Ÿ
   - **æ¥å£**ï¼šä½¿ç”¨äº†å“ªäº›APIã€é…ç½®é¡¹ã€å·¥å…·ï¼Ÿ
   - **å®ç°**ï¼šå…·ä½“ä»£ç ä½ç½®ã€æ–‡ä»¶ç»“æ„ï¼ˆæ ‡æ³¨ä¸ç¡®å®šæ€§ï¼‰

2. **å¤„ç†ä¸åŒå±‚çº§çš„æ›´æ–°**ï¼š
   - å…ƒçŸ¥è¯†å’ŒåŸç†å±‚ï¼šåªæœ‰é‡å¤§å‘ç°æ‰æ›´æ–°
   - æ¥å£å±‚ï¼šè®°å½•å‡†ç¡®çš„APIç­¾åå’Œç”¨æ³•
   - å®ç°å±‚ï¼šæ ‡æ³¨"å¯èƒ½å·²å˜åŒ–"ï¼Œè®°å½•æŸ¥æ‰¾æ¨¡å¼è€Œéè¡Œå·

3. **æ•´åˆæ–°æ—§è®°å¿†**ï¼ˆæœ€é‡è¦ï¼‰ï¼š
   - **å¿…é¡»ä¿ç•™**ï¼šå·²æœ‰çŸ¥è¯†ä¸­ä»ç„¶æœ‰æ•ˆçš„æ‰€æœ‰å†…å®¹
   - **éœ€è¦æ·»åŠ **ï¼šæœ¬æ¬¡ä»»åŠ¡ä¸­å­¦åˆ°çš„æ–°çŸ¥è¯†
   - **å¯ä»¥æ›´æ–°**ï¼šå‘ç°é”™è¯¯çš„éƒ¨åˆ†ï¼ˆæ˜ç¡®æ ‡æ³¨"æ›´æ­£ï¼š"ï¼‰
   - **å¯ä»¥åˆ é™¤**ï¼šç¡®è®¤å·²è¿‡æ—¶çš„å®ç°ç»†èŠ‚
   - **åˆå¹¶ç›¸ä¼¼**ï¼šç›¸åŒä¸»é¢˜çš„ç»éªŒå¯ä»¥åˆå¹¶

4. **ä¿æŒç²¾ç‚¼**ï¼š
   - ä½¿ç”¨ç®€æ´çš„è¦ç‚¹å½¢å¼
   - æ–¹æ³•ä¼˜äºç»“æœï¼ˆå¦‚ä½•æ‰¾åˆ° > åœ¨å“ªé‡Œï¼‰
   - ç¨³å®šä¼˜äºç²¾ç¡®ï¼ˆåŸç† > ç»†èŠ‚ï¼‰
   - æ³¨æ„è¾“å‡ºå¤§å°é™åˆ¶ï¼šçº¦ {knowledge_limit_kb}KB

## è¾“å‡ºæ ¼å¼
ä½¿ç”¨ä»¥ä¸‹Markdownç»“æ„ç»„ç»‡çŸ¥è¯†ï¼š

```markdown
# çŸ¥è¯†åº“

## å…ƒçŸ¥è¯†
- æŸ¥æ‰¾æ–¹æ³•ã€éªŒè¯æŠ€å·§ã€å­¦ä¹ æ¨¡å¼

## åŸç†ä¸è®¾è®¡
- æ ¸å¿ƒæ¦‚å¿µã€æ¶æ„å†³ç­–ã€è®¾è®¡ç†å¿µ

## æ¥å£ä¸API
- å·¥å…·ç”¨æ³•ã€é…ç½®é¡¹ã€å…¬å…±æ–¹æ³•

## å®ç°ç»†èŠ‚ï¼ˆéœ€éªŒè¯ï¼‰
- ä»£ç ä½ç½®ã€æ–‡ä»¶ç»“æ„ã€å†…éƒ¨å®ç°
- æ³¨ï¼šå®ç°ç»†èŠ‚å¯èƒ½å·²å˜åŒ–ï¼Œä½¿ç”¨å‰éœ€éªŒè¯

## ç”¨æˆ·åå¥½ä¸é¡¹ç›®ç‰¹ç‚¹
- ç‰¹å®šåå¥½ã€é¡¹ç›®çº¦å®šã€å¸¸è§æ¨¡å¼
```

è¯·ç›´æ¥è¾“å‡ºæå–çš„çŸ¥è¯†å†…å®¹ï¼Œä¸è¦æœ‰ä»»ä½•è§£é‡Šæˆ–å…ƒä¿¡æ¯ã€‚"""
        
        return prompt
    
    def _build_knowledge_compression_prompt(self, knowledge_content: str, 
                                        current_size: int, 
                                        limit: int) -> str:
        """æ„å»ºçŸ¥è¯†å‹ç¼©çš„æç¤ºè¯ï¼ˆçŸ¥è¯†ç²¾ç‚¼ç³»ç»Ÿï¼‰"""
        current_kb = current_size / 1024
        limit_kb = limit / 1024
        target_kb = limit_kb * 0.85  # ç›®æ ‡å¤§å°ä¸ºé™åˆ¶çš„85%ï¼Œç•™å‡ºä½™é‡
        
        prompt = f"""# çŸ¥è¯†å‹ç¼©ä»»åŠ¡ï¼ˆçŸ¥è¯†ç²¾ç‚¼ç³»ç»Ÿï¼‰

## å½“å‰çŠ¶å†µ
- çŸ¥è¯†æ–‡ä»¶å¤§å°ï¼š{current_kb:.1f}KB ({current_size} å­—èŠ‚)
- å¤§å°é™åˆ¶ï¼š{limit_kb:.1f}KB ({limit} å­—èŠ‚)
- ç›®æ ‡å¤§å°ï¼š{target_kb:.1f}KB ä»¥å†…

## çŸ¥è¯†å†…å®¹
{knowledge_content}

## å‹ç¼©ç­–ç•¥

è¯·ä½¿ç”¨ä»¥ä¸‹ç²¾ç‚¼ç­–ç•¥æ¥å‹ç¼©çŸ¥è¯†ï¼š

### 1. æ—¶é—´è¡°å‡åŸåˆ™
- ä¿ç•™æœ€è¿‘çš„ç»éªŒå’Œæ•™è®­
- åˆ é™¤è¿‡äºä¹…è¿œæˆ–å·²è¢«æ›´æ–°çš„ä¿¡æ¯
- åˆå¹¶é‡å¤å‡ºç°çš„æ¨¡å¼

### 2. é‡è¦æ€§è¯„ä¼°
ä¿ç•™ä»¥ä¸‹ç±»å‹çš„ä¿¡æ¯ï¼ˆä¼˜å…ˆçº§ä»é«˜åˆ°ä½ï¼‰ï¼š
- **å…³é”®é”™è¯¯å’Œè§£å†³æ–¹æ¡ˆ**ï¼šé¿å…é‡å¤çŠ¯é”™
- **ç”¨æˆ·åå¥½å’Œé¡¹ç›®ç‰¹ç‚¹**ï¼šä¸ªæ€§åŒ–æœåŠ¡
- **æœ‰æ•ˆçš„å·¥ä½œæ¨¡å¼**ï¼šæé«˜æ•ˆç‡
- **ç‰¹æ®Šæ¡ˆä¾‹å¤„ç†**ï¼šè¾¹ç•Œæƒ…å†µ
- **å¸¸ç”¨æŠ€æœ¯æ ˆä¿¡æ¯**ï¼šæŠ€æœ¯åå¥½

åˆ é™¤ä»¥ä¸‹ç±»å‹çš„ä¿¡æ¯ï¼š
- å…·ä½“çš„æ–‡ä»¶å†…å®¹å’Œä»£ç ç»†èŠ‚
- ä¸€æ¬¡æ€§çš„ä¸´æ—¶è§£å†³æ–¹æ¡ˆ
- å·²ç»è¿‡æ—¶çš„æŠ€æœ¯ä¿¡æ¯
- å†—é•¿çš„æ‰§è¡Œè¿‡ç¨‹æè¿°
- é‡å¤çš„æˆåŠŸç»éªŒ

### 3. æŠ½è±¡åŒ–åŸåˆ™
- å°†å…·ä½“æ¡ˆä¾‹æŠ½è±¡ä¸ºé€šç”¨æ¨¡å¼
- åˆå¹¶ç›¸ä¼¼ç»éªŒä¸ºä¸€èˆ¬æ€§åŸåˆ™
- ä¿ç•™æ¨¡å¼è€Œéå®ä¾‹

### 4. ç»“æ„åŒ–ç»„ç»‡
ä½¿ç”¨æ¸…æ™°çš„æ ‡é¢˜ç»“æ„ï¼Œå¦‚ï¼š
- ## æ ¸å¿ƒç»éªŒ
- ## å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ
- ## ç”¨æˆ·åå¥½
- ## æŠ€æœ¯æ ˆç‰¹ç‚¹

## è¾“å‡ºè¦æ±‚
1. è¾“å‡ºå‹ç¼©åçš„çŸ¥è¯†å†…å®¹
2. ä½¿ç”¨Markdownæ ¼å¼
3. ç¡®ä¿å¤§å°ä¸è¶…è¿‡ {target_kb:.1f}KB
4. ä¿æŒå†…å®¹çš„è¿è´¯æ€§å’Œå¯è¯»æ€§
5. ä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæˆ–å…ƒä¿¡æ¯

è¯·ç›´æ¥è¾“å‡ºå‹ç¼©åçš„è®°å¿†å†…å®¹ï¼š"""
        
        return prompt
    
    def _clean_data_directory(self) -> None:
        """æ¸…ç†å·¥ä½œç›®å½•å’Œç§æœ‰æ•°æ®åŒºåŸŸï¼Œä¸ºæ–°ä»»åŠ¡å‡†å¤‡å¹²å‡€ç¯å¢ƒ
        
        æ¸…ç†ç­–ç•¥ï¼š
        - TODOæ–‡ä»¶ï¼šå½’æ¡£åˆ° archive ç›®å½•ï¼ˆä¿ç•™ä»»åŠ¡è®¡åˆ’å†å²ï¼‰
        - BPMNæ–‡ä»¶ï¼šå½’æ¡£åˆ° archive ç›®å½•ï¼ˆä¿ç•™æµç¨‹æ‰§è¡Œå†å²ï¼‰
        - å…¶ä»–æ–‡ä»¶ï¼šåˆ é™¤
        """
        import shutil
        from datetime import datetime
        
        # æ¸…ç†ç§æœ‰æ•°æ®ç›®å½•
        if self.data_dir.exists():
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            
            # åˆ›å»ºå½’æ¡£ç›®å½•
            archive_dir = self.data_dir / "archive"
            archive_dir.mkdir(exist_ok=True)
            
            # å½’æ¡£å·¥ä½œæµç›¸å…³æ–‡ä»¶ï¼ˆTODOå’ŒBPMNï¼‰
            for item in self.data_dir.iterdir():
                # è·³è¿‡ archive ç›®å½•æœ¬èº«
                if item.is_dir() and item.name == "archive":
                    continue
                    
                if item.is_file():
                    # å½’æ¡£TODOæ–‡ä»¶
                    if item.name.upper() == 'TODO.MD' or item.name.upper().startswith('TODO'):
                        archive_name = f"{timestamp}_{item.name}"
                        archive_path = archive_dir / archive_name
                        try:
                            item.rename(archive_path)
                            if os.environ.get('DEBUG'):
                                logger.info(f"[{self.name}] Archived TODO file: {item.name} -> archive/{archive_name}")
                        except Exception as e:
                            logger.warning(f"[{self.name}] Failed to archive {item}: {e}")
                    
                    # å½’æ¡£BPMNæ–‡ä»¶
                    elif item.name.endswith('.bpmn'):
                        archive_name = f"{timestamp}_{item.name}"
                        archive_path = archive_dir / archive_name
                        try:
                            item.rename(archive_path)
                            if os.environ.get('DEBUG'):
                                logger.info(f"[{self.name}] Archived BPMN file: {item.name} -> archive/{archive_name}")
                        except Exception as e:
                            logger.warning(f"[{self.name}] Failed to archive {item}: {e}")
                    
                    # åˆ é™¤å…¶ä»–æ–‡ä»¶
                    else:
                        try:
                            item.unlink()
                            if os.environ.get('DEBUG'):
                                logger.info(f"[{self.name}] Removed file from data directory: {item}")
                        except Exception as e:
                            logger.warning(f"[{self.name}] Failed to remove {item}: {e}")
                
                # åˆ é™¤éå½’æ¡£ç›®å½•
                elif item.is_dir():
                    try:
                        shutil.rmtree(item)
                        if os.environ.get('DEBUG'):
                            logger.info(f"[{self.name}] Removed directory from data directory: {item}")
                    except Exception as e:
                        logger.warning(f"[{self.name}] Failed to remove {item}: {e}")
        
        # æ³¨é‡Šæ‰æ¸…ç†å…±äº«å·¥ä½œç›®å½•çš„ä»£ç ï¼Œä»¥æ”¯æŒå¤š Agent æ–‡ä»¶å…±äº«
        # åœ¨å¤š Agent åä½œåœºæ™¯ä¸‹ï¼Œä¸åº”è¯¥æ¸…ç†å…±äº«å·¥ä½œç›®å½•
        # æ¯ä¸ª Agent åº”è¯¥åªç®¡ç†è‡ªå·±çš„ç§æœ‰æ•°æ®ç›®å½• (.agent_data/{agent_name})
        
        # # æ¸…ç†å…±äº«å·¥ä½œç›®å½•ä¸­çš„æ–‡ä»¶ï¼Œä½†ä¿ç•™éšè—ç›®å½•ï¼ˆ.agent_data, .agent_memoryï¼‰
        # if self.work_dir.exists():
        #     for item in self.work_dir.iterdir():
        #         # è·³è¿‡éšè—ç›®å½•ï¼ˆ.agent_data, .agent_memoryï¼‰
        #         if item.name.startswith('.'):
        #             continue
        #             
        #         # åˆ é™¤æ–‡ä»¶
        #         if item.is_file():
        #             try:
        #                 item.unlink()
        #                 if os.environ.get('DEBUG'):
        #                     logger.info(f"[{self.name}] Removed file: {item}")
        #             except Exception as e:
        #                 logger.warning(f"[{self.name}] Failed to remove {item}: {e}")
        #         
        #         # åˆ é™¤ééšè—ç›®å½•
        #         elif item.is_dir():
        #             try:
        #                 shutil.rmtree(item)
        #                 if os.environ.get('DEBUG'):
        #                     logger.info(f"[{self.name}] Removed directory: {item}")
        #             except Exception as e:
        #                 logger.warning(f"[{self.name}] Failed to remove {item}: {e}")
    
    def _check_and_trigger_exploration(self) -> None:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦è§¦å‘å®šæœŸæ¢ç´¢"""
        import time
        
        # è·å–ä¸Šæ¬¡æ¢ç´¢æ—¶é—´
        last_exploration_time = self._get_last_exploration_time()
        current_time = time.time()
        
        # å¦‚æœä»æœªæ¢ç´¢è¿‡æˆ–è¶…è¿‡é—´éš”æ—¶é—´
        if last_exploration_time is None or (current_time - last_exploration_time) > self.config.exploration_interval:
            print(f"\nğŸ” [é¡¹ç›®æ¢ç´¢] è¶…è¿‡{self.config.exploration_interval/3600:.0f}å°æ—¶æœªæ¢ç´¢ï¼Œå¯åŠ¨åå°æ¢ç´¢...")
            self._trigger_project_exploration()
    
    def _get_last_exploration_time(self) -> Optional[float]:
        """è·å–ä¸Šæ¬¡æ¢ç´¢æ—¶é—´"""
        try:
            from project_explorer_segmented import SegmentedProjectExplorer as ProjectExplorer
            explorer = ProjectExplorer(self.name, self.work_dir, self.llm, self.config)
            return explorer.get_last_exploration_time()
        except Exception as e:
            if os.environ.get('DEBUG'):
                logger.warning(f"Failed to get last exploration time: {e}")
            return None
    
    def _trigger_project_exploration(self) -> None:
        """è§¦å‘é¡¹ç›®æ¢ç´¢ï¼ˆå¼‚æ­¥ï¼‰"""
        import threading
        import asyncio
        
        # é¿å…é‡å¤æ¢ç´¢
        if hasattr(self, '_exploration_in_progress') and self._exploration_in_progress:
            print("âš ï¸ é¡¹ç›®æ¢ç´¢å·²åœ¨è¿›è¡Œä¸­ï¼Œè·³è¿‡æ­¤æ¬¡è§¦å‘")
            return
        
        self._exploration_in_progress = True
        
        # å®šä¹‰æ¢ç´¢å®Œæˆçš„å›è°ƒ
        def on_exploration_complete():
            """æ¢ç´¢å®Œæˆåçš„å›è°ƒ"""
            if self.config.auto_reload_on_exploration:
                # è®¾ç½®å¾…æ›´æ–°æ ‡è®°
                self._pending_reload = True
                print("ğŸ’¡ [é¡¹ç›®ç†è§£] æ¢ç´¢å®Œæˆï¼Œå°†åœ¨ä¸‹æ¬¡ä»»åŠ¡æ—¶æ›´æ–°æ¨¡å‹")
        
        # åˆ›å»ºå¼‚æ­¥æ¢ç´¢ä»»åŠ¡
        def explore_async():
            try:
                from project_explorer_segmented import SegmentedProjectExplorer as ProjectExplorer
                explorer = ProjectExplorer(
                    self.name, 
                    self.work_dir, 
                    self.llm, 
                    self.config,
                    on_complete_callback=on_exploration_complete
                )
                
                # åœ¨æ–°çš„äº‹ä»¶å¾ªç¯ä¸­è¿è¡Œå¼‚æ­¥ä»»åŠ¡
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                loop.run_until_complete(explorer.explore_project())
                loop.close()
                
            except Exception as e:
                print(f"âš ï¸ [é¡¹ç›®æ¢ç´¢] æ¢ç´¢è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
                if os.environ.get('DEBUG'):
                    import traceback
                    traceback.print_exc()
            finally:
                self._exploration_in_progress = False
        
        # åˆ›å»ºå¹¶å¯åŠ¨çº¿ç¨‹
        explorer_thread = threading.Thread(
            target=explore_async,
            daemon=False,  # éå®ˆæŠ¤çº¿ç¨‹ï¼Œç¡®ä¿å®Œæˆ
            name=f"project_explorer_{self.name}"
        )
        explorer_thread.start()
        
        # è·Ÿè¸ªçº¿ç¨‹ï¼ˆä¾›é€€å‡ºæ—¶ç­‰å¾…ï¼‰
        global _exploration_threads
        if '_exploration_threads' not in globals():
            _exploration_threads = []
        _exploration_threads.append(explorer_thread)
    
    def _save_current_messages(self) -> List[BaseMessage]:
        """ä¿å­˜å½“å‰æ¶ˆæ¯å†å²
        
        Returns:
            List[BaseMessage]: å½“å‰çš„æ¶ˆæ¯åˆ—è¡¨
        """
        if self.memory and hasattr(self.memory, 'chat_memory'):
            # å¤åˆ¶æ¶ˆæ¯åˆ—è¡¨ï¼Œé¿å…å¼•ç”¨é—®é¢˜
            return list(self.memory.chat_memory.messages)
        return []
    
    def _restore_messages(self, messages: List[BaseMessage]) -> None:
        """æ¢å¤æ¶ˆæ¯å†å²
        
        Args:
            messages: è¦æ¢å¤çš„æ¶ˆæ¯åˆ—è¡¨
        """
        if self.memory and hasattr(self.memory, 'chat_memory'):
            # æ¸…ç©ºå½“å‰æ¶ˆæ¯
            self.memory.chat_memory.clear()
            
            # æ¢å¤æ¶ˆæ¯ï¼ˆè·³è¿‡ç¬¬ä¸€æ¡ç³»ç»Ÿæ¶ˆæ¯ï¼Œå› ä¸ºæ–°çš„ç³»ç»Ÿæ¶ˆæ¯å·²ç»åŒ…å«æ›´æ–°çš„çŸ¥è¯†ï¼‰
            for i, msg in enumerate(messages):
                # ç¬¬ä¸€æ¡é€šå¸¸æ˜¯ç³»ç»Ÿæ¶ˆæ¯ï¼ŒåŒ…å«æ—§çš„çŸ¥è¯†ï¼Œè·³è¿‡
                if i == 0 and isinstance(msg, SystemMessage):
                    continue
                self.memory.chat_memory.add_message(msg)
    
    def _reload_with_new_understanding(self) -> None:
        """é‡æ–°åŠ è½½é¡¹ç›®ç†è§£å¹¶ä¿ç•™æ¶ˆæ¯å†å²
        
        ç»éªŒä¸»ä¹‰å®ç°ï¼šé‡æ–°åˆå§‹åŒ–Agentï¼Œç„¶åæ¢å¤æ¶ˆæ¯
        """
        print("\nğŸ”„ [é¡¹ç›®ç†è§£] æ£€æµ‹åˆ°æ–°çš„é¡¹ç›®ç†è§£ï¼Œæ­£åœ¨æ›´æ–°...")
        self.hot_reload_knowledge()
        print("ğŸ’¡ ç°åœ¨AgentåŸºäºæœ€æ–°çš„UMLæ¨¡å‹å·¥ä½œ\n")
    
    def hot_reload_knowledge(self, knowledge_files: Optional[List[str]] = None, 
                           knowledge_strings: Optional[List[str]] = None,
                           notify: bool = True) -> None:
        """çŸ¥è¯†çƒ­åŠ è½½ - åŠ¨æ€åŠ è½½çŸ¥è¯†æ–‡ä»¶ï¼Œç±»ä¼¼åŠ¨æ€é“¾æ¥åº“
        
        ç»éªŒä¸»ä¹‰è®¾è®¡ï¼šé€šè¿‡é‡æ–°åˆå§‹åŒ–å®ç°çƒ­åŠ è½½ï¼Œç®€å•æœ‰æ•ˆ
        
        Args:
            knowledge_files: æ–°çš„çŸ¥è¯†æ–‡ä»¶åˆ—è¡¨ï¼ˆå¦‚æœä¸ºNoneï¼Œåˆ™é‡æ–°åŠ è½½å½“å‰é…ç½®çš„æ–‡ä»¶ï¼‰
            knowledge_strings: æ–°çš„çŸ¥è¯†å­—ç¬¦ä¸²åˆ—è¡¨
            notify: æ˜¯å¦æ˜¾ç¤ºé€šçŸ¥æ¶ˆæ¯
            
        Example:
            # çƒ­åŠ è½½æ–°çš„çŸ¥è¯†æ–‡ä»¶
            agent.hot_reload_knowledge(["knowledge/new_domain.md"])
            
            # é‡æ–°åŠ è½½å½“å‰çŸ¥è¯†ï¼ˆç”¨äºæ–‡ä»¶å†…å®¹æ›´æ–°ï¼‰
            agent.hot_reload_knowledge()
            
            # åŠ è½½å­—ç¬¦ä¸²çŸ¥è¯†
            agent.hot_reload_knowledge(knowledge_strings=["æ–°çš„é¢†åŸŸçŸ¥è¯†..."])
        """
        if notify:
            print("\nâ™»ï¸ [çŸ¥è¯†çƒ­åŠ è½½] æ­£åœ¨æ›´æ–°çŸ¥è¯†ç³»ç»Ÿ...")
        
        # 1. ä¿å­˜å½“å‰æ¶ˆæ¯å†å²
        saved_messages = self._save_current_messages()
        
        # 2. ä¿å­˜å½“å‰é…ç½®
        saved_config = self.config
        saved_name = self.name
        saved_tools = self._tools
        
        # 3. æ›´æ–°é…ç½®ä¸­çš„çŸ¥è¯†æ–‡ä»¶ï¼ˆå¦‚æœæä¾›äº†æ–°çš„ï¼‰
        if knowledge_files is not None:
            saved_config.knowledge_files = knowledge_files
        if knowledge_strings is not None:
            saved_config.knowledge_strings = knowledge_strings
        
        # 4. é‡æ–°åˆå§‹åŒ–ï¼ˆä¼šåŠ è½½æ–°çš„çŸ¥è¯†ï¼‰
        self.__init__(saved_config, saved_name, saved_tools)
        
        # 5. æ¢å¤æ¶ˆæ¯å†å²
        self._restore_messages(saved_messages)
        
        if notify:
            loaded_count = len(self.config.knowledge_files) + len(self.config.knowledge_strings)
            print(f"âœ… [çŸ¥è¯†çƒ­åŠ è½½] å®Œæˆï¼å·²åŠ è½½ {loaded_count} ä¸ªçŸ¥è¯†æºï¼Œå¯¹è¯å†å²å·²ä¿ç•™")
    
    def _execute_unix_command(self, knowledge_file_name: str, parameter: str) -> Optional[str]:
        """æ‰§è¡ŒUnixå‘½ä»¤æ ¼å¼çš„çŸ¥è¯†æ–‡ä»¶è°ƒç”¨
        
        Args:
            knowledge_file_name: çŸ¥è¯†æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
            parameter: ä¼ é€’ç»™çŸ¥è¯†ç¨‹åºçš„å‚æ•°
            
        Returns:
            Optional[str]: æ‰§è¡Œç»“æœï¼Œå¦‚æœæ‰¾ä¸åˆ°çŸ¥è¯†æ–‡ä»¶åˆ™è¿”å›None
        """
        # æŸ¥æ‰¾çŸ¥è¯†æ–‡ä»¶
        knowledge_content = None
        knowledge_file_path = None
        
        # 1. å…ˆåœ¨é•¿æœŸæ•°æ®ç›®å½•æŸ¥æ‰¾
        for ext in ['.md', '.txt', '']:
            candidate = self.knowledge_dir / f"{knowledge_file_name}{ext}"
            if candidate.exists():
                knowledge_file_path = candidate
                break
        
        # 2. å¦‚æœæ²¡æ‰¾åˆ°ï¼Œåœ¨knowledgeç›®å½•æŸ¥æ‰¾
        if knowledge_file_path is None:
            knowledge_base = Path(__file__).parent / "knowledge"
            if knowledge_base.exists():
                for ext in ['.md', '.txt', '']:
                    candidate = knowledge_base / f"{knowledge_file_name}{ext}"
                    if candidate.exists():
                        knowledge_file_path = candidate
                        break
        
        # 3. å¦‚æœè¿˜æ²¡æ‰¾åˆ°ï¼Œæ£€æŸ¥é…ç½®çš„çŸ¥è¯†æ–‡ä»¶
        if knowledge_file_path is None:
            for kf in self.config.knowledge_files:
                kf_path = Path(kf)
                if kf_path.stem == knowledge_file_name or kf_path.name == knowledge_file_name:
                    if kf_path.exists():
                        knowledge_file_path = kf_path
                        break
        
        # å¦‚æœæ‰¾ä¸åˆ°çŸ¥è¯†æ–‡ä»¶
        if knowledge_file_path is None:
            print(f"âš ï¸ [Unixå‘½ä»¤] æ‰¾ä¸åˆ°çŸ¥è¯†æ–‡ä»¶: {knowledge_file_name}")
            return None
        
        # è¯»å–çŸ¥è¯†æ–‡ä»¶å†…å®¹
        try:
            knowledge_content = knowledge_file_path.read_text(encoding='utf-8')
        except Exception as e:
            print(f"âŒ [Unixå‘½ä»¤] è¯»å–çŸ¥è¯†æ–‡ä»¶å¤±è´¥: {e}")
            return None
        
        # æ„é€ æç¤ºè¯
        # çŸ¥è¯†æœ¬è´¨æ˜¯ç¨‹åºçš„å“²å­¦ï¼šçŸ¥è¯†æ–‡ä»¶æ˜¯ç¨‹åºï¼Œå‚æ•°æ˜¯è¾“å…¥
        prompt = f"""ä½ ç°åœ¨è¦æ‰§è¡Œä¸€ä¸ªçŸ¥è¯†ç¨‹åºã€‚

## ç¨‹åºï¼ˆæ¥è‡ª {knowledge_file_path.name}ï¼‰ï¼š
{knowledge_content}

## å‚æ•°ï¼š
{parameter if parameter else "ï¼ˆæ— å‚æ•°ï¼‰"}

è¯·æ ¹æ®ä¸Šè¿°ç¨‹åºå†…å®¹å’Œå‚æ•°ï¼Œæ‰§è¡Œç›¸åº”çš„ä»»åŠ¡ã€‚
"""
        
        print(f"\nğŸ–¥ï¸ [Unixå‘½ä»¤] æ‰§è¡Œ: /{knowledge_file_name} {parameter}")
        print(f"ğŸ“„ åŠ è½½çŸ¥è¯†ç¨‹åº: {knowledge_file_path}")
        
        # ä½¿ç”¨å†…éƒ¨æ‰§è¡Œæ–¹æ³•
        return self._execute_internal_task(prompt)
    
    def clear_long_term_memory(self, confirm: bool = False) -> None:
        """æ¸…ç©ºé•¿æœŸè®°å¿†ï¼ˆåˆ é™¤long_term_dataç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶ï¼‰
        
        Args:
            confirm: æ˜¯å¦ç¡®è®¤åˆ é™¤ï¼Œé»˜è®¤Falseéœ€è¦ç¡®è®¤
            
        è­¦å‘Šï¼šæ­¤æ“ä½œä¸å¯æ¢å¤ï¼å°†åˆ é™¤æ‰€æœ‰ï¼š
        - æå–çš„çŸ¥è¯† (extracted_knowledge.md)
        - é¡¹ç›®ç†è§£ (project_understanding.md)
        - ç¯å¢ƒè®¤çŸ¥ (environment_cognition.json)
        - æ¢ç´¢å†å² (exploration_log.json)
        """
        import shutil
        
        if not confirm:
            print("âš ï¸ è­¦å‘Šï¼šæ­¤æ“ä½œå°†æ°¸ä¹…åˆ é™¤æ‰€æœ‰é•¿æœŸè®°å¿†ï¼")
            print("   åŒ…æ‹¬ï¼šçŸ¥è¯†ã€é¡¹ç›®ç†è§£ã€ç¯å¢ƒè®¤çŸ¥ç­‰")
            print("   å¦‚æœç¡®å®šè¦ç»§ç»­ï¼Œè¯·ä½¿ç”¨ clear_long_term_memory(confirm=True)")
            return
        
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            if not self.knowledge_dir.exists():
                print("ğŸ“­ é•¿æœŸè®°å¿†ç›®å½•ä¸å­˜åœ¨ï¼Œæ— éœ€æ¸…ç†")
                return
            
            # ç»Ÿè®¡æ–‡ä»¶æ•°é‡
            files = list(self.knowledge_dir.glob("*"))
            file_count = len(files)
            
            if file_count == 0:
                print("ğŸ“­ é•¿æœŸè®°å¿†å·²ç»æ˜¯ç©ºçš„")
                return
            
            print(f"ğŸ—‘ï¸ æ­£åœ¨æ¸…ç©ºé•¿æœŸè®°å¿†...")
            print(f"   å°†åˆ é™¤ {file_count} ä¸ªæ–‡ä»¶ï¼š")
            
            # åˆ—å‡ºå°†è¦åˆ é™¤çš„æ–‡ä»¶
            for f in files:
                print(f"   - {f.name}")
            
            # åˆ é™¤æ‰€æœ‰æ–‡ä»¶
            for f in files:
                try:
                    if f.is_file():
                        f.unlink()
                    elif f.is_dir():
                        shutil.rmtree(f)
                except Exception as e:
                    print(f"   âš ï¸ åˆ é™¤ {f.name} å¤±è´¥: {e}")
            
            # é‡æ–°åˆå§‹åŒ–å¿…è¦çš„æ–‡ä»¶
            self.knowledge_file.touch()  # åˆ›å»ºç©ºçš„çŸ¥è¯†æ–‡ä»¶
            
            print("âœ… é•¿æœŸè®°å¿†å·²æ¸…ç©ºï¼")
            print("   Agentå°†ä»é›¶å¼€å§‹å­¦ä¹ ")
            
            # æ¸…ç©ºå†…å­˜ä¸­çš„ç¼“å­˜
            self.prior_knowledge = ""
            self.project_understanding = ""
            
        except Exception as e:
            print(f"âŒ æ¸…ç©ºé•¿æœŸè®°å¿†å¤±è´¥: {e}")
            if os.environ.get('DEBUG'):
                import traceback
                traceback.print_exc()
    
    
    def execute_task(self, task: str) -> str:
        """æ‰§è¡Œä»»åŠ¡
        
        ä¸»è¦æ‰§è¡Œå…¥å£ï¼Œä½¿ç”¨ç¼“å­˜çš„ executor æ‰§è¡Œä»»åŠ¡ã€‚
        
        Args:
            task: è¦æ‰§è¡Œçš„ä»»åŠ¡æè¿°
            
        Returns:
            str: æœ€åä¸€æ¡AIæ¶ˆæ¯å†…å®¹
        """
        # ç¡®ä¿ executor å·²åˆå§‹åŒ–
        if self._executor is None:
            raise RuntimeError("Executor not initialized. This should not happen.")
        
        # æ¸…ç†æ•°æ®ç›®å½•ï¼ˆä¿ç•™æå–çš„çŸ¥è¯†ï¼‰
        self._clean_data_directory()
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¾…å¤„ç†çš„é¡¹ç›®ç†è§£æ›´æ–°
        if self._pending_reload:
            self._reload_with_new_understanding()
            self._pending_reload = False
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯Unixå‘½ä»¤æ ¼å¼: /çŸ¥è¯†æ–‡ä»¶å å‚æ•°
        if task.strip().startswith('/'):
            parts = task.strip().split(maxsplit=1)
            if len(parts) >= 1:
                knowledge_file_name = parts[0][1:]  # å»æ‰å¼€å¤´çš„ /
                parameter = parts[1] if len(parts) > 1 else ""
                
                # å¤„ç†Unixå‘½ä»¤
                result = self._execute_unix_command(knowledge_file_name, parameter)
                if result is not None:
                    return result
                # å¦‚æœè¿”å›Noneï¼Œè¯´æ˜æ²¡æ‰¾åˆ°å¯¹åº”çš„çŸ¥è¯†æ–‡ä»¶ï¼Œç»§ç»­æ­£å¸¸å¤„ç†
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ¢ç´¢è¯·æ±‚
        exploration_keywords = [
            "æ¢ç´¢é¡¹ç›®", "åˆ†æé¡¹ç›®", "æ‰«æä»£ç ", 
            "ç†è§£æ¶æ„", "é¡¹ç›®ç»“æ„", "explore project",
            "analyze project", "scan code", "understand architecture"
        ]
        
        if any(keyword in task.lower() for keyword in exploration_keywords):
            # ç”¨æˆ·ä¸»åŠ¨è¯·æ±‚æ¢ç´¢
            print("\nğŸ” æ­£åœ¨åå°æ¢ç´¢é¡¹ç›®ç»“æ„...")
            self._trigger_project_exploration()
            return "å·²å¯åŠ¨é¡¹ç›®æ¢ç´¢ï¼Œå°†åœ¨åå°å¼‚æ­¥æ‰§è¡Œã€‚æ¢ç´¢å®Œæˆåä¼šé€šçŸ¥æ‚¨ã€‚"
        
        # æ‰§è¡Œä¸»ä»»åŠ¡
        result = self._execute_internal_task(task)
        
        # æ£€æŸ¥å®šæœŸè§¦å‘ï¼ˆå¦‚æœå¯ç”¨äº†é¡¹ç›®æ¢ç´¢ï¼‰
        if self.config.enable_project_exploration:
            self._check_and_trigger_exploration()
        
        return result
    
    def _execute_internal_task(self, task: str) -> str:
        """æ‰§è¡Œå†…éƒ¨ä»»åŠ¡çš„é€šç”¨æ–¹æ³•
        
        Returns:
            str: æœ€åä¸€æ¡AIæ¶ˆæ¯å†…å®¹
        """
        # ä½¿ç”¨ LangGraph agent æ‰§è¡Œä»»åŠ¡
        print(f"\n[{self.name}] > Executing task...")
        
        # å‡†å¤‡è¾“å…¥æ¶ˆæ¯
        messages = []
        
        # æ·»åŠ ç³»ç»Ÿæç¤ºè¯ä½œä¸ºç¬¬ä¸€æ¡æ¶ˆæ¯
        if hasattr(self, '_system_prompt') and self._system_prompt:
            messages.append(SystemMessage(content=self._system_prompt))
        
        # å¦‚æœæœ‰è®°å¿†ï¼ŒåŠ è½½å†å²æ¶ˆæ¯
        if self.memory is not None:
            memory_vars = self.memory.load_memory_variables({})
            if "chat_history" in memory_vars:
                # æ·»åŠ å†å²æ¶ˆæ¯ï¼ˆä½†ä¸åŒ…æ‹¬ä¹‹å‰çš„ç³»ç»Ÿæ¶ˆæ¯ï¼‰
                for msg in memory_vars["chat_history"]:
                    if not isinstance(msg, SystemMessage):
                        messages.append(msg)
        
        # æ·»åŠ å½“å‰ä»»åŠ¡
        messages.append(HumanMessage(content=task))
        
        inputs = {"messages": messages}
        
        # æ‰§è¡Œä»»åŠ¡ï¼Œè®¾ç½®æ›´é«˜çš„é€’å½’é™åˆ¶å’Œé…ç½®
        invoke_config = RunnableConfig(
            recursion_limit=300,  # å¢åŠ é€’å½’é™åˆ¶
            max_concurrency=5,    # é™åˆ¶å¹¶å‘
            configurable={}
        )
        
        # ä½¿ç”¨ stream æ–¹æ³•æ˜¾ç¤ºè¯¦ç»†çš„æ‰§è¡Œè¿‡ç¨‹
        try:
            # æ”¶é›†æ‰€æœ‰æ¶ˆæ¯ç”¨äºæœ€ç»ˆè¾“å‡º
            all_messages = []
            # è®°å½•å·²æ‰“å°çš„æ¶ˆæ¯å†…å®¹å“ˆå¸Œï¼Œé¿å…é‡å¤
            printed_messages = set()
            
            # ä½¿ç”¨ stream æ–¹æ³•è·å–ä¸­é—´æ­¥éª¤
            if self._executor is None:
                raise RuntimeError("Executor not initialized")
            for event in self._executor.stream(inputs, config=invoke_config, stream_mode="values"):
                messages = event.get("messages", [])
                if messages:
                    # è·å–æœ€åä¸€æ¡æ¶ˆæ¯
                    last_message = messages[-1]
                    all_messages = messages
                    
                    # ç”Ÿæˆæ¶ˆæ¯å†…å®¹çš„å”¯ä¸€æ ‡è¯†ï¼ˆåŸºäºå†…å®¹è€Œä¸æ˜¯å¯¹è±¡IDï¼‰
                    msg_content = ""
                    if hasattr(last_message, 'content'):
                        msg_content = str(last_message.content)
                    if hasattr(last_message, 'tool_calls'):
                        msg_content += str(last_message.tool_calls)
                    if hasattr(last_message, 'name'):
                        msg_content += str(last_message.name)
                    
                    msg_hash = hash(msg_content)
                    if msg_hash in printed_messages:
                        continue  # è·³è¿‡å·²æ‰“å°çš„æ¶ˆæ¯
                    printed_messages.add(msg_hash)
                    
                    # æ˜¾ç¤ºä¸åŒç±»å‹çš„æ¶ˆæ¯
                    if hasattr(last_message, 'content'):
                        if isinstance(last_message, HumanMessage):
                            print(f"\n\U0001f464 ç”¨æˆ·: {last_message.content}")
                        elif isinstance(last_message, SystemMessage):
                            if os.environ.get('DEBUG'):
                                print(f"\n\U0001f4bb ç³»ç»Ÿ: {last_message.content[:100]}...")
                        elif hasattr(last_message, 'tool_calls') and last_message.tool_calls:
                            # AI å†³å®šè°ƒç”¨å·¥å…·
                            print(f"\n\U0001f914 [{self.name}] AI æ€è€ƒ: éœ€è¦ä½¿ç”¨å·¥å…·æ¥å®Œæˆä»»åŠ¡")
                            for tool_call in last_message.tool_calls:
                                print(f"\U0001f527 è°ƒç”¨å·¥å…·: {tool_call.get('name', 'unknown')}")
                                if 'args' in tool_call:
                                    print(f"   å‚æ•°: {tool_call['args']}")
                        elif hasattr(last_message, 'name'):
                            # å·¥å…·è¿”å›ç»“æœ
                            print(f"\n\U0001f4ac å·¥å…·ç»“æœ ({last_message.name}):")
                            content = last_message.content
                            # é™åˆ¶è¾“å‡ºé•¿åº¦
                            if len(content) > 500:
                                print(f"   {content[:500]}...")
                                print(f"   [çœç•¥ {len(content)-500} å­—ç¬¦]")
                            else:
                                print(f"   {content}")
                        else:
                            # AI çš„æœ€ç»ˆå›ç­” - é™åˆ¶é•¿åº¦é¿å…é‡å¤
                            if last_message.content:
                                content = last_message.content
                                if len(content) > 200:
                                    print(f"\n\U0001f916 [{self.name}] AI å›ç­”: {content[:200]}... [å·²æˆªæ–­]")
                                else:
                                    print(f"\n\U0001f916 [{self.name}] AI å›ç­”: {content}")
            
            # æ„å»ºç»“æœ
            result = {"messages": all_messages}
            
        except Exception as e:
            # å¦‚æœå‡ºé”™ï¼Œå°è¯•ç®€åŒ–è¾“å…¥
            if "recursion" in str(e).lower():
                print("æç¤ºï¼šä»»åŠ¡å¯èƒ½è¿‡äºå¤æ‚ï¼Œå°è¯•ç®€åŒ–...")
                # åªä¿ç•™ç”¨æˆ·æ¶ˆæ¯ï¼Œå»æ‰ç³»ç»Ÿæç¤ºè¯
                simple_inputs = {"messages": [HumanMessage(content=task)]}
                if self._executor is None:
                    raise RuntimeError("Executor not initialized")
                result = self._executor.invoke(simple_inputs, config=invoke_config)
            else:
                raise
        
        # è·å–æœ€ç»ˆè¾“å‡º
        if "messages" in result:
            # è·å–æœ€åä¸€æ¡ AI æ¶ˆæ¯ä½œä¸ºè¾“å‡º
            output_message = result["messages"][-1]
            
            
            output = output_message.content if hasattr(output_message, 'content') else str(output_message)
            
            # å¦‚æœæœ‰è®°å¿†ï¼Œä¿å­˜å¯¹è¯
            if self.memory is not None:
                self.memory.save_context({"input": task}, {"output": str(output)})
        else:
            output = str(result)
        
        # æ‰“å°æœ€ç»ˆç»“æœï¼ˆé™åˆ¶è¾“å‡ºé•¿åº¦é¿å…é‡å¤ï¼‰
        print(f"\n[{self.name}] > Task completed.")
        print(f"\n=== [{self.name}] æœ€ç»ˆç»“æœ ===\n")
        # é™åˆ¶è¾“å‡ºé•¿åº¦ï¼Œé¿å…å¤§é‡é‡å¤å†…å®¹
        if len(output) > 1000:
            print(output[:500] + "\n... [å†…å®¹è¿‡é•¿ï¼Œå·²æˆªæ–­] ...")
        else:
            print(output)
        
        # å¼‚æ­¥æ›´æ–°æå–çš„çŸ¥è¯†
        if "messages" in result and self.config.knowledge_extraction_limit > 0:
            # å¯åŠ¨åå°çº¿ç¨‹æå–çŸ¥è¯†ï¼ˆéå®ˆæŠ¤çº¿ç¨‹ï¼‰
            knowledge_thread = threading.Thread(
                target=self._update_extracted_knowledge_sync,
                args=(result["messages"],),
                daemon=False,  # éå®ˆæŠ¤çº¿ç¨‹ï¼Œç¡®ä¿å®Œæˆ
                name=f"knowledge_extraction_{self.name}"
            )
            knowledge_thread.start()
            
            # æ·»åŠ åˆ°å…¨å±€è·Ÿè¸ªåˆ—è¡¨
            global _memory_update_threads
            _memory_update_threads.append(knowledge_thread)
        
        # è¿”å›æœ€åä¸€æ¡AIæ¶ˆæ¯å†…å®¹
        return str(output) if output else ""


def main():
    """ä¸»å‡½æ•° - å‘½ä»¤è¡Œå…¥å£
    
    æ”¯æŒçš„å‘½ä»¤è¡Œå‚æ•°ï¼š
    --memory: è®°å¿†çº§åˆ« (none/smart/pro)
    --session-id: ä¼šè¯IDï¼ˆç”¨äºæŒä¹…åŒ–è®°å¿†ï¼‰
    --max-tokens: æœ€å¤§ token é™åˆ¶
    --knowledge-file: çŸ¥è¯†æ–‡ä»¶è·¯å¾„
    --work-dir: å·¥ä½œç›®å½•
    --task: è¦æ‰§è¡Œçš„ä»»åŠ¡
    --llm-model: LLM æ¨¡å‹åç§°
    --llm-base-url: LLM API åŸºç¡€ URL
    --llm-api-key-env: LLM API å¯†é’¥ç¯å¢ƒå˜é‡å
    --llm-temperature: LLM æ¸©åº¦å‚æ•°
    
    ç¤ºä¾‹ï¼š
        # ä½¿ç”¨é»˜è®¤ DeepSeek
        python react_agent.py --task "åˆ›å»ºä¸€ä¸ªåšå®¢ç³»ç»Ÿ"
        
        # ä½¿ç”¨ OpenAI
        python react_agent.py --llm-model gpt-4-turbo-preview --llm-base-url https://api.openai.com/v1 --llm-api-key-env OPENAI_API_KEY --task "åˆ›å»ºä¸€ä¸ªåšå®¢ç³»ç»Ÿ"
    """
    import argparse
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description="Generic ReactAgent with Prior Knowledge Support")
    parser.add_argument("--memory", choices=["none", "smart", "pro"], 
                       default="smart", help="Memory level: none, smart, or pro (default: smart)")
    parser.add_argument("--session-id", type=str, 
                       default=None, help="Session ID for persistent memory")
    parser.add_argument("--max-tokens", type=int, default=30000,
                       help="Max token limit for smart memory (default: 30000)")
    parser.add_argument("--knowledge-file", type=str, 
                       default="å…ˆéªŒçŸ¥è¯†.md", 
                       help="Path to prior knowledge file (default: å…ˆéªŒçŸ¥è¯†.md)")
    parser.add_argument("--work-dir", type=str, 
                       default="output/generic_agent", 
                       help="Working directory (default: output/generic_agent)")
    parser.add_argument("--task", type=str, 
                       default="åˆ›å»ºä¸€ä¸ªç®€å•çš„ Hello World ç¨‹åº",
                       help="Task description (default: åˆ›å»ºä¸€ä¸ªç®€å•çš„ Hello World ç¨‹åº)")
    # LLM é…ç½®å‚æ•°
    parser.add_argument("--llm-model", type=str, 
                       default="deepseek-chat",
                       help="LLM model name (default: deepseek-chat)")
    parser.add_argument("--llm-base-url", type=str, 
                       default="https://api.deepseek.com/v1",
                       help="LLM API base URL (default: https://api.deepseek.com/v1)")
    parser.add_argument("--llm-api-key-env", type=str, 
                       default="DEEPSEEK_API_KEY",
                       help="Environment variable name for LLM API key (default: DEEPSEEK_API_KEY)")
    parser.add_argument("--llm-temperature", type=float, 
                       default=0,
                       help="LLM temperature (default: 0)")
    parser.add_argument("--context-window", type=int,
                       default=None,
                       help="Context window size in tokens (default: auto-detect based on model)")
    
    parser.add_argument("--show-memory-updates", action="store_true",
                       help="Show memory extraction notifications (default: True)")
    parser.add_argument("--no-show-memory-updates", action="store_true",
                       help="Hide memory extraction notifications")
    
    args = parser.parse_args()
    
    # æ ¹æ®memoryå‚æ•°æ˜ å°„åˆ°MemoryLevel
    memory_mapping = {
        "none": MemoryLevel.NONE,
        "smart": MemoryLevel.SMART,
        "pro": MemoryLevel.PRO
    }
    memory_level = memory_mapping[args.memory]
    
    if os.environ.get('DEBUG'):
        logger.info(f"Using Generic ReactAgent v4")
        logger.info(f"Memory level: {args.memory}")
        logger.info(f"Knowledge file: {args.knowledge_file}")
        logger.info(f"Working directory: {args.work_dir}")
        logger.info(f"LLM model: {args.llm_model}")
        logger.info(f"LLM base URL: {args.llm_base_url}")
        logger.info(f"LLM API key env: {args.llm_api_key_env}")
    
    # ä¸åˆ›å»ºå·¥ä½œç›®å½• - å·¥ä½œç›®å½•æ˜¯å¤–éƒ¨ä¸–ç•Œï¼Œåº”è¯¥å·²ç»å­˜åœ¨
    # å¦‚æœå·¥ä½œç›®å½•ä¸å­˜åœ¨ï¼Œåº”è¯¥æŠ¥é”™è€Œä¸æ˜¯åˆ›å»º
    if not Path(args.work_dir).exists():
        print(f"é”™è¯¯ï¼šå·¥ä½œç›®å½• '{args.work_dir}' ä¸å­˜åœ¨")
        print("å·¥ä½œç›®å½•ä»£è¡¨å¤–éƒ¨ä¸–ç•Œï¼ˆå¦‚é¡¹ç›®ä»£ç åº“ï¼‰ï¼Œå¿…é¡»é¢„å…ˆå­˜åœ¨")
        sys.exit(1)
    
    # åˆ›å»ºé…ç½®
    # å¤„ç† show_memory_updates å‚æ•°ï¼ˆé»˜è®¤ä¸º Trueï¼‰
    show_memory_updates = True
    if args.no_show_memory_updates:
        show_memory_updates = False
    elif args.show_memory_updates:
        show_memory_updates = True
    
    config = ReactAgentConfig(
        work_dir=args.work_dir,
        additional_config={},
        memory_level=memory_level,
        session_id=args.session_id,
        max_token_limit=args.max_tokens,
        knowledge_file=args.knowledge_file,
        llm_model=args.llm_model,
        llm_base_url=args.llm_base_url,
        llm_api_key_env=args.llm_api_key_env,
        llm_temperature=args.llm_temperature,
        context_window=args.context_window,
        show_memory_updates=show_memory_updates
    )
    
    try:
        # åˆ›å»º Agent
        agent = GenericReactAgent(config)
        if os.environ.get('DEBUG'):
            logger.info("Initialized Generic ReactAgent v4")
            logger.info(f"Starting task execution...")
        
        start_time = time.time()
        
        # æ‰§è¡Œä»»åŠ¡
        agent.execute_task(args.task)
        
        execution_time = time.time() - start_time
        
        # ç»Ÿè®¡
        print("\n" + "=" * 50)
        print("âœ… Task Execution Complete!")
        print("=" * 50)
        print(f"Agent: Generic ReactAgent v4")
        print(f"Work Directory: {args.work_dir}")
        print(f"\nMemory Configuration:")
        print(f"  - Level: {args.memory}")
        if args.memory == "smart":
            print(f"  - Token limit: {config.max_token_limit}")
        elif args.memory == "pro":
            print(f"  - Session ID: {config.session_id}")
            print(f"  - Database: {config.db_path}")
        print(f"\nExecution time: {execution_time:.2f}s")
        
        return 0
        
    except Exception as e:
        logger.error(f"Task execution failed: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
