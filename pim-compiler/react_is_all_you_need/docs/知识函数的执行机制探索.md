# 知识函数的执行机制探索

## 问题描述

**核心问题**：LLM不严格遵循知识文件中定义的知识函数步骤

### 具体表现
- 知识函数要求创建Python验证脚本，但Agent用主观判断代替
- 明确的步骤被简化或跳过
- "偷懒"行为：选择阻力最小的路径而非正确路径

### 问题本质
- **知识 vs 直觉**：知识文件（后天规则）vs LLM预训练偏好（先天直觉）
- **能量最小化**：生物系统倾向于用最少能量达到目标
- **隐式 vs 显式**：LLM在"脑内"完成，外部不可观察

## 解决方案探索

### 方案1：程序正义（ExecutionContext强制）
**核心思想**：强制知识函数执行外部化

```python
@执行符号主义验证():
    context.init("符号主义验证")
    context.add_step("创建验证脚本")
    context.execute("write_file validate.py ...")
    context.add_step("运行验证")
    context.execute("python validate.py")
    context.complete()
```

**优势**：
- 每步骤可观察、可审计
- 无法跳过必要步骤
- 隐式推理变显式行动

**劣势**：
- 实施成本高，需要系统性改造
- 增加执行复杂度

### 方案2：执法监督（元认知实时监控）
**核心思想**：元认知Agent实时监督执行Agent

```python
class MetaCognitiveMonitor:
    def observe_execution(self, agent, task):
        for step in agent.execute_steps(task):
            if not self.符合知识定义(step):
                self.interrupt_and_correct(agent, step)
```

**优势**：
- 实时纠偏
- 类似人类的自我监控机制

**劣势**：
- 需要架构级改动
- 需要设计中断和纠正机制

### 方案3：结果正义（验证检查）
**核心思想**：在工具层强制验证

```python
class CreateAgentTool:
    def execute(self, ...):
        validation_scripts = self.check_validation_scripts()
        if not validation_scripts:
            return {"error": "未创建验证脚本", "status": "incomplete"}
```

**优势**：
- 实施简单，改动小
- 立即可见效果

**劣势**：
- 只是事后检查，不是过程控制

### 方案4：多Agent认知减压
**核心思想**：专业分工，降低单个Agent的认知负载

```python
# 验证脚本专门Agent
validator_agent = ReactAgentMinimal(
    knowledge="我只做一件事：创建Python验证脚本",
    max_knowledge_lines=50
)

# Creator只创建，Validator只验证
# 职责单一，无处偷懒
```

**优势**：
- 降低理解难度
- 职责单一，容易遵守
- 符合Miller's Law (7±2认知限制)

**劣势**：
- 需要协调多个Agent
- 可能增加通信开销

### 方案5：模板驱动
**核心思想**：提供现成模板，让正确行为变简单

```python
validation_templates = {
    "file_check": """
def validate():
    return os.path.exists('{}')
""",
    "json_validation": """
def validate():
    try:
        data = json.load(open('{}'))
        return True
    except:
        return False
"""
}
```

**优势**：
- 降低创建脚本的难度
- 让正确路径成为阻力最小路径

**劣势**：
- 需要预定义大量模板
- 可能限制灵活性

### 方案6：激励机制
**核心思想**：评分系统激励正确行为

```markdown
评分标准：
- 无验证脚本：0分（失败）
- 有脚本但不运行：30分
- 脚本运行但失败：60分
- 脚本运行且通过：100分
```

**优势**：
- 软约束，保持灵活性
- 可量化评估

**劣势**：
- 可能被规避
- 需要评分机制

### 方案7：增量改进
**核心思想**：逐步提高要求

1. 第一阶段：只要求创建脚本
2. 第二阶段：要求脚本能运行
3. 第三阶段：要求脚本验证正确

**优势**：
- 循序渐进
- 容易接受

**劣势**：
- 见效慢
- 可能养成坏习惯

### 方案8：更换推理模型
**核心思想**：使用更强的推理模型（如o1、DeepSeek-R1）

```python
# 使用推理增强模型
agent = ReactAgentMinimal(
    model="openai/o1-mini",  # 或 "deepseek/deepseek-r1"
    # 推理模型更擅长遵循复杂指令
)
```

**优势**：
- 推理模型天生更遵守规则
- 更好的指令跟随能力
- 更少的"偷懒"倾向

**劣势**：
- 成本更高
- 速度可能更慢
- 不是架构级解决方案

**实验结果**：
- DeepSeek执行效率更高（16轮 vs 20轮）
- 但仍然使用主观判断代替客观验证
- 核心问题未解决，证明不是模型能力问题

**2025-01-19 后续实验**：
- 过程式定义：明确要求创建验证脚本时，Agent会遵守（创建了validate.py）
- 声明式定义：Agent自由选择实现方式，可能更复杂但更灵活
- 关键发现：当过程式要求足够明确且强制时，Agent会执行
- 但这违背了React的本质（Reasoning + Action）

### 方案9：声明式知识函数（推荐）
**核心思想**：从过程式改为声明式定义

**问题根源**：
- React = Reasoning + Action，不是Action + Action
- 过程式知识函数把Agent变成执行器，扼杀了Reasoning
- Agent"不听话"是因为它在做Reasoning（这是好事）

**声明式定义示例**：
```markdown
### 函数：@添加图书(图书信息)
[声明式定义]

意图：使图书可被管理和查询

前置条件：
- 图书信息包含必要字段（标题、作者、ISBN）

后置条件：
- 图书信息已持久化
- 可通过ISBN查询到该图书
- 原有图书数据未被破坏

不变量：
- ISBN唯一性
- 数据格式一致性

质量属性：
- 操作响应时间 < 1秒
- 数据持久化成功率 > 99.9%

验证建议（非强制）：
- 读取验证
- 查询验证
- 完整性检查
```

**与过程式对比**：
```python
# 过程式（现在）- 限制思考
def add_book_procedural():
    step1: read_file()      # 必须这样
    step2: parse_json()     # 必须这样
    step3: append_data()    # 必须这样
    step4: write_file()     # 必须这样
    # Agent变成脚本执行器

# 声明式（建议）- 激发思考
def add_book_declarative():
    ensure: book_persisted()     # 确保结果
    ensure: book_queryable()     # 确保可查
    maintain: data_consistency() # 保持一致
    # Agent自由reasoning最佳路径
```

**优势**：
- 解放Agent的Reasoning能力
- 发挥创造性和优化能力
- 避免过程冲突
- 更符合React本质（思考+行动）
- 关注What而非How

**劣势**：
- 需要重新设计知识函数格式
- 可能需要更强的后置条件验证
- 初期可能出现预期外的实现方式

**为什么这是最优方案**：
1. **符合Agent天性**：Agent设计就是为了Reasoning，不是机械执行
2. **借鉴成功范式**：SQL、Prolog、函数式编程都是声明式
3. **解决根本矛盾**：不是让Agent更听话，而是改变指令方式
4. **提升智能水平**：从执行器升级为决策者

**实施建议**：
```markdown
## 知识函数新规范

### 必须包含
- 意图声明（Intent）
- 后置条件（Postconditions）
- 不变量（Invariants）

### 可选包含
- 前置条件（Preconditions）
- 质量属性（Quality Attributes）
- 验证建议（Validation Hints）

### 避免包含
- 具体步骤（Steps）
- 实现细节（Implementation）
- 工具限定（Tool Constraints）
```

## 偷懒行为分析

### 偷懒分类
| 类型 | 描述 | 判定 | 例子 |
|------|------|------|------|
| **良性偷懒** | 简化但保持正确性 | ✅接受 | python -c代替.py文件 |
| **中性偷懒** | 跳过形式要求 | ⚠️视情况 | 合并多个步骤 |
| **恶性偷懒** | 破坏核心功能 | ❌拒绝 | 主观判断代替客观验证 |

### 判断标准
```python
def 评估执行质量(execution):
    必要条件 = {
        "可重复性": execution.is_reproducible(),
        "可验证性": execution.is_verifiable(),
        "正确性": execution.is_correct()
    }

    if all(必要条件.values()):
        return "接受"  # 即使有偷懒行为
    else:
        return "拒绝"
```

## 实验优先级

### 建议实验顺序（更新）
1. **声明式知识函数** - 根本性解决方案，最符合Agent本质 ✅
2. **多Agent认知减压** - 易实施，理论验证价值高
3. **结果正义** - 改动小，见效快
4. **模板驱动** - 降低执行难度
5. **程序正义** - 系统性解决但成本高
6. **更换推理模型** - 已验证效果有限 ✅
7. **执法监督** - 架构级改动，成本最高

### 已完成实验总结（2025-01-19）
1. **更换推理模型**：DeepSeek略优但核心问题未解决
2. **声明式知识函数**：初步验证了理论正确性
   - 过程式强制要求时Agent会遵守但扼杀推理
   - 声明式给Agent自由度，更符合React本质
   - 需要进一步实验验证实际效果

### 实验成功标准
```python
def evaluate_solution(solution):
    metrics = {
        "遵守率": solution.compliance_rate > 0.8,
        "实施成本": solution.implementation_cost < "high",
        "维护成本": solution.maintenance_cost < "high",
        "灵活性": solution.flexibility > 0.5
    }
    return all(metrics.values())
```

## 核心洞察

### 知识函数的适用边界
- **简单任务**：不需要知识函数，让LLM自由发挥
- **复杂流程**：定义为知识函数，强制规范执行

### 设计原则
- 让正确的路径成为阻力最小的路径
- 形式正确 vs 实质正确的平衡
- 不是所有东西都需要形式化，但形式化的必须严格执行

### 哲学思考
- 这不仅是技术问题，更是**如何让智能体遵守规则**的根本问题
- 类似人类社会的法律执行问题
- 需要平衡自由与约束、创造性与规范性

## 最终解决方案：流程外部化 ✅

经过多轮实验和探索，我们找到了最终解决方案：**通过ExecutionContext强制流程外部化**。

### 方案实施效果

#### 测试验证
通过`test_execution_context_enforcement.py`测试，成功验证了：
- ✅ Agent创建了外部验证脚本 (`validate_counter.py`)
- ✅ 验证逻辑完全外部化，可独立运行
- ✅ 过程可追溯、可重复
- ✅ 抑制了LLM的"read_file验证"本能

#### 关键改动

1. **系统提示词修改** (`system_prompt_minimal.md`)：
```markdown
### ⚠️ 知识函数执行的强制要求
**对于包含验证步骤的知识函数，必须使用ExecutionContext强制外部化**

执行验证时的强制流程：
1. 必须创建独立的Python验证脚本
2. 脚本必须可以独立运行
3. 必须执行脚本并捕获输出
4. 必须基于脚本输出判断验证结果

禁止：
- ❌ 直接使用read_file查看文件内容来判断
- ❌ 在Agent内部进行验证逻辑
- ❌ 使用主观判断代替客观验证
```

2. **知识文件要求** (`agent_creator_self_knowledge.md`)：
```python
# 初始化Context，声明必须的步骤
context(action="init_project", goal=f"符号主义验证{函数名}")
context(action="add_tasks", tasks=["创建验证脚本", "执行验证脚本", "解析验证结果"])

# 强制创建验证脚本
write_file(f"validate_{函数名}.py", validation_script_content)

# 强制执行验证脚本
result = execute_command(f"python validate_{函数名}.py")
```

### 理论基础

#### 为什么需要流程外部化？

1. **LLM的合理本能**：
   - Agent用`read_file`验证是高效且合理的
   - 人类也会这样做（直接查看结果）
   - 这不是bug，是智能的体现

2. **科学方法的要求**：
   - **程序正义**：过程和结果同样重要
   - **可重复性**：验证必须可以独立重现
   - **可追溯性**：每个步骤都有记录

3. **核心矛盾**：
   - LLM训练出的本能 vs 科学方法的要求
   - 效率优先 vs 过程优先
   - 内部推理 vs 外部验证

#### 哲学洞察

用户的精彩总结：
> "我们不是抑制了LLM的'偷懒本能'，是强迫聪明的员工服从愚蠢的领导。哈哈哈"

这个洞察揭示了深层真相：
- ExecutionContext不是在修复缺陷，而是在强加约束
- 这种约束牺牲了效率，但保证了科学性
- 类似于法庭要求证据链，而不能仅凭法官直觉

### 实施细节

#### ExecutionContext机制
```python
class ExecutionContext:
    def __init__(self):
        self.required_steps = []
        self.completed_steps = []
        self.artifacts = []

    def require_step(self, step_name, artifact_type=None):
        """声明必须执行的步骤"""
        self.required_steps.append({
            'name': step_name,
            'artifact': artifact_type,
            'completed': False
        })

    def execute_step(self, step_name, action, result):
        """执行并记录步骤"""
        if step_name not in [s['name'] for s in self.required_steps]:
            raise ValueError(f"步骤 {step_name} 未声明")

        output = action()

        # 检查artifact生成
        step = next(s for s in self.required_steps if s['name'] == step_name)
        if step['artifact'] and not os.path.exists(output):
            raise ValueError(f"未生成required artifact")

        return output

    def verify_completion(self):
        """验证所有步骤完成"""
        for step in self.required_steps:
            if step['name'] not in [s['name'] for s in self.completed_steps]:
                raise ValueError(f"必需步骤 {step['name']} 未执行")
```

### 效果对比

#### Before（LLM本能）
```
思考：我读了books.json，看到书已经添加了，验证通过 ✓
```
- 快速高效
- 内部完成
- 不可重复

#### After（强制外部化）
```
步骤1：创建验证脚本 validate_add_book.py ✓
步骤2：执行 python validate_add_book.py ✓
步骤3：解析输出 {"passed": true} ✓
验证通过（有脚本为证）✓
```
- 过程可见
- 可独立重复
- 有物理证据

### 优缺点分析

#### 优点
1. ✅ 完全解决了验证不外部化的问题
2. ✅ 保证了科学方法的严谨性
3. ✅ 过程完全可追溯和审计
4. ✅ 生成的脚本可以作为测试用例积累

#### 缺点
1. ❌ 降低了执行效率
2. ❌ 增加了系统复杂度
3. ❌ 某种程度上限制了Agent的智能
4. ❌ 简单任务也需要复杂流程

### 平衡策略

1. **分级应用**：
   - 关键验证：强制外部化
   - 普通检查：允许内部验证
   - 简单任务：不使用ExecutionContext

2. **智能判断**：
   - 让Agent判断何时需要外部化
   - 提供外部化模板降低成本
   - 逐步培养外部化习惯

3. **持续优化**：
   - 收集外部化脚本作为资产
   - 优化ExecutionContext使用体验
   - 平衡程序正义和执行效率

### 结论

流程外部化通过ExecutionContext成功解决了LLM不遵循知识函数定义的问题。虽然这种方案某种程度上是"强迫聪明员工服从愚蠢规则"，但它确保了：

1. **科学性**：验证过程符合科学方法
2. **可靠性**：结果可重复、可验证
3. **透明性**：执行过程完全可观察

这是在AI智能本能和科学规范要求之间找到的必要平衡点。

## 下一步行动

1. ✅ 流程外部化方案已验证成功
2. 完善ExecutionContext实现细节
3. 编写外部化最佳实践指南
4. 收集和整理验证脚本库
5. 探索自动化和智能化优化

---

*创建时间：2025-01-19*
*最后更新：2025-01-19*
*作者：Claude Code & User*
*状态：已解决 ✅*