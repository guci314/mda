#!/usr/bin/env python3
"""
Gemma 270M æœ¬åœ°è¿è¡Œæ¼”ç¤º
å±•ç¤ºå¦‚ä½•åœ¨æœ¬æœºè¿è¡ŒGoogle Gemma 270Mæ¨¡å‹

ä½œè€…: Assistant
æ—¥æœŸ: 2025-01-04
"""

import os
import time
import psutil
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from typing import Optional, List
import warnings
warnings.filterwarnings('ignore')

class Gemma270MDemo:
    """Gemma 270Mæ¨¡å‹æœ¬åœ°è¿è¡Œæ¼”ç¤ºç±»"""

    def __init__(self,
                 model_id: str = "unsloth/gemma-3-270m-it",  # ç¤¾åŒºç‰ˆæœ¬ï¼Œæ— éœ€Token
                 device: str = "auto",
                 quantize: bool = False):
        """
        åˆå§‹åŒ–Gemma 270Mæ¨¡å‹

        Args:
            model_id: HuggingFaceæ¨¡å‹ID
            device: è¿è¡Œè®¾å¤‡ ('cpu', 'cuda', 'mps', 'auto')
            quantize: æ˜¯å¦ä½¿ç”¨é‡åŒ–ä»¥å‡å°‘å†…å­˜å ç”¨
        """
        self.model_id = model_id
        self.device = self._get_device(device)
        self.quantize = quantize

        print(f"ğŸš€ æ­£åœ¨åŠ è½½Gemma 270Mæ¨¡å‹...")
        print(f"   æ¨¡å‹: {model_id}")
        print(f"   è®¾å¤‡: {self.device}")
        print(f"   é‡åŒ–: {'æ˜¯' if quantize else 'å¦'}")

        self.tokenizer = None
        self.model = None

    def _get_device(self, device: str) -> str:
        """è‡ªåŠ¨æ£€æµ‹æœ€ä½³è®¾å¤‡"""
        if device == "auto":
            if torch.cuda.is_available():
                return "cuda"
            elif torch.backends.mps.is_available():
                return "mps"
            else:
                return "cpu"
        return device

    def load_model(self):
        """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
        start_time = time.time()

        # åŠ è½½åˆ†è¯å™¨
        print("â³ åŠ è½½åˆ†è¯å™¨...")
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)

        # åŠ è½½æ¨¡å‹
        print("â³ åŠ è½½æ¨¡å‹æƒé‡...")
        if self.quantize and self.device == "cuda":
            # ä½¿ç”¨8-bité‡åŒ–ï¼ˆéœ€è¦bitsandbytesåº“ï¼‰
            try:
                from transformers import BitsAndBytesConfig
                quantization_config = BitsAndBytesConfig(
                    load_in_8bit=True,
                    bnb_8bit_compute_dtype=torch.float16
                )
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_id,
                    quantization_config=quantization_config,
                    device_map="auto"
                )
            except ImportError:
                print("âš ï¸ é‡åŒ–éœ€è¦å®‰è£…bitsandbytes: pip install bitsandbytes")
                self._load_standard_model()
        else:
            self._load_standard_model()

        load_time = time.time() - start_time
        self._print_model_info(load_time)

    def _load_standard_model(self):
        """æ ‡å‡†æ¨¡å‹åŠ è½½"""
        if self.device == "cpu":
            # CPUä¸Šä½¿ç”¨float32
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_id,
                torch_dtype=torch.float32
            )
        else:
            # GPUä¸Šä½¿ç”¨float16ä»¥èŠ‚çœå†…å­˜
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_id,
                torch_dtype=torch.float16
            )

        self.model = self.model.to(self.device)
        self.model.eval()

    def _print_model_info(self, load_time: float):
        """æ‰“å°æ¨¡å‹ä¿¡æ¯"""
        # è·å–æ¨¡å‹å‚æ•°é‡
        total_params = sum(p.numel() for p in self.model.parameters())

        # è·å–å†…å­˜ä½¿ç”¨
        if self.device == "cuda":
            memory_used = torch.cuda.memory_allocated() / 1024**3
            memory_info = f"{memory_used:.2f} GB (GPU)"
        else:
            process = psutil.Process()
            memory_used = process.memory_info().rss / 1024**3
            memory_info = f"{memory_used:.2f} GB (RAM)"

        print(f"\nâœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")
        print(f"   å‚æ•°é‡: {total_params/1e6:.1f}M")
        print(f"   å†…å­˜å ç”¨: {memory_info}")
        print(f"   åŠ è½½æ—¶é—´: {load_time:.2f}ç§’")

    def generate(self,
                 prompt: str,
                 max_length: int = 256,
                 temperature: float = 0.7,
                 top_p: float = 0.95,
                 stream: bool = True) -> str:
        """
        ç”Ÿæˆæ–‡æœ¬

        Args:
            prompt: è¾“å…¥æç¤º
            max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
            temperature: æ¸©åº¦å‚æ•°ï¼ˆ0-1ï¼Œè¶Šé«˜è¶Šéšæœºï¼‰
            top_p: nucleus samplingå‚æ•°
            stream: æ˜¯å¦æµå¼è¾“å‡º

        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬
        """
        if not self.model:
            raise ValueError("è¯·å…ˆè°ƒç”¨load_model()åŠ è½½æ¨¡å‹")

        # ç¼–ç è¾“å…¥
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        input_length = inputs.input_ids.shape[1]

        # ç”Ÿæˆå‚æ•°
        gen_kwargs = {
            "max_new_tokens": max_length,
            "temperature": temperature,
            "top_p": top_p,
            "do_sample": temperature > 0,
            "pad_token_id": self.tokenizer.pad_token_id,
            "eos_token_id": self.tokenizer.eos_token_id,
        }

        print(f"\nğŸ’­ ç”Ÿæˆä¸­...")
        start_time = time.time()

        if stream:
            return self._stream_generate(inputs, gen_kwargs, input_length)
        else:
            return self._batch_generate(inputs, gen_kwargs, input_length, start_time)

    def _batch_generate(self, inputs, gen_kwargs, input_length, start_time):
        """æ‰¹é‡ç”Ÿæˆï¼ˆä¸€æ¬¡æ€§è¿”å›ï¼‰"""
        with torch.no_grad():
            outputs = self.model.generate(inputs.input_ids, **gen_kwargs)

        # è§£ç è¾“å‡º
        generated_text = self.tokenizer.decode(
            outputs[0][input_length:],
            skip_special_tokens=True
        )

        gen_time = time.time() - start_time
        tokens_generated = outputs.shape[1] - input_length
        speed = tokens_generated / gen_time

        print(f"\nâ±ï¸ ç”Ÿæˆç»Ÿè®¡:")
        print(f"   Tokenæ•°: {tokens_generated}")
        print(f"   ç”Ÿæˆæ—¶é—´: {gen_time:.2f}ç§’")
        print(f"   é€Ÿåº¦: {speed:.1f} tokens/ç§’")

        return generated_text

    def _stream_generate(self, inputs, gen_kwargs, input_length):
        """æµå¼ç”Ÿæˆï¼ˆé€tokenè¾“å‡ºï¼‰"""
        from transformers import TextIteratorStreamer
        from threading import Thread

        streamer = TextIteratorStreamer(
            self.tokenizer,
            skip_prompt=True,
            skip_special_tokens=True
        )

        gen_kwargs["streamer"] = streamer

        # åœ¨å¦ä¸€ä¸ªçº¿ç¨‹ä¸­è¿è¡Œç”Ÿæˆ
        thread = Thread(target=self.model.generate,
                       args=(inputs.input_ids,),
                       kwargs=gen_kwargs)
        thread.start()

        # æµå¼è¾“å‡º
        generated_text = ""
        for new_text in streamer:
            print(new_text, end="", flush=True)
            generated_text += new_text

        thread.join()
        return generated_text

    def benchmark(self):
        """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print("\nğŸ“Š è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•...")

        test_prompts = [
            "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
            "å†™ä¸€ä¸ªPythonå‡½æ•°è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—",
            "è§£é‡Šé‡å­è®¡ç®—çš„åŸºæœ¬åŸç†",
        ]

        results = []
        for prompt in test_prompts:
            print(f"\næµ‹è¯•: {prompt[:30]}...")

            start_time = time.time()
            output = self.generate(prompt, max_length=100, stream=False)
            gen_time = time.time() - start_time

            tokens = len(self.tokenizer.encode(output))
            speed = tokens / gen_time

            results.append({
                "prompt": prompt[:30],
                "tokens": tokens,
                "time": gen_time,
                "speed": speed
            })

        # æ‰“å°ç»“æœ
        print("\nğŸ“ˆ åŸºå‡†æµ‹è¯•ç»“æœ:")
        print("-" * 60)
        for r in results:
            print(f"æç¤º: {r['prompt']}")
            print(f"  ç”ŸæˆTokenæ•°: {r['tokens']}")
            print(f"  ç”Ÿæˆæ—¶é—´: {r['time']:.2f}ç§’")
            print(f"  é€Ÿåº¦: {r['speed']:.1f} tokens/ç§’")

        avg_speed = sum(r['speed'] for r in results) / len(results)
        print(f"\nå¹³å‡é€Ÿåº¦: {avg_speed:.1f} tokens/ç§’")

    def interactive_chat(self):
        """äº¤äº’å¼å¯¹è¯æ¨¡å¼"""
        print("\nğŸ’¬ è¿›å…¥äº¤äº’å¼å¯¹è¯æ¨¡å¼")
        print("   è¾“å…¥'é€€å‡º'æˆ–'quit'ç»“æŸå¯¹è¯")
        print("   è¾“å…¥'æ¸…ç©º'æˆ–'clear'æ¸…ç©ºå¯¹è¯å†å²")
        print("-" * 60)

        conversation_history = []

        while True:
            user_input = input("\nğŸ‘¤ ä½ : ").strip()

            if user_input.lower() in ['é€€å‡º', 'quit', 'exit']:
                print("ğŸ‘‹ å†è§ï¼")
                break

            if user_input.lower() in ['æ¸…ç©º', 'clear']:
                conversation_history = []
                print("ğŸ—‘ï¸ å¯¹è¯å†å²å·²æ¸…ç©º")
                continue

            # æ„å»ºå¯¹è¯ä¸Šä¸‹æ–‡
            if conversation_history:
                context = "\n".join(conversation_history[-4:])  # ä¿ç•™æœ€è¿‘2è½®å¯¹è¯
                prompt = f"{context}\nç”¨æˆ·: {user_input}\nåŠ©æ‰‹:"
            else:
                prompt = f"ç”¨æˆ·: {user_input}\nåŠ©æ‰‹:"

            print("\nğŸ¤– Gemma: ", end="")
            response = self.generate(prompt, max_length=256, temperature=0.7)

            # æ›´æ–°å†å²
            conversation_history.append(f"ç”¨æˆ·: {user_input}")
            conversation_history.append(f"åŠ©æ‰‹: {response}")


def main():
    """ä¸»å‡½æ•°ï¼šæ¼”ç¤ºGemma 270Mçš„å„ç§åŠŸèƒ½"""

    print("=" * 60)
    print("ğŸŒŸ Google Gemma 270M æœ¬åœ°è¿è¡Œæ¼”ç¤º")
    print("=" * 60)

    # åˆå§‹åŒ–æ¨¡å‹
    demo = Gemma270MDemo(
        model_id="unsloth/gemma-3-270m-it",  # ä½¿ç”¨ç¤¾åŒºç‰ˆæœ¬ï¼Œæ— éœ€Token
        device="auto",  # è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
        quantize=False  # 270Mæ¨¡å‹å¾ˆå°ï¼Œé€šå¸¸ä¸éœ€è¦é‡åŒ–
    )

    # åŠ è½½æ¨¡å‹
    try:
        demo.load_model()
    except Exception as e:
        print(f"\nâŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print("\nè¯·ç¡®ä¿:")
        print("1. å®‰è£…äº†å¿…è¦çš„åº“: pip install transformers torch")
        print("2. æœ‰ç¨³å®šçš„ç½‘ç»œè¿æ¥ï¼ˆé¦–æ¬¡è¿è¡Œéœ€è¦ä¸‹è½½æ¨¡å‹ï¼‰")
        print("3. æœ‰è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´ï¼ˆçº¦550MBï¼‰")
        return

    # åŠŸèƒ½æ¼”ç¤ºèœå•
    while True:
        print("\n" + "=" * 60)
        print("ğŸ“‹ åŠŸèƒ½èœå•")
        print("=" * 60)
        print("1. å¿«é€Ÿæµ‹è¯• - ç”Ÿæˆä¸€ä¸ªç®€å•å›å¤")
        print("2. ä»£ç ç”Ÿæˆ - ç”ŸæˆPythonä»£ç ")
        print("3. æ€§èƒ½æµ‹è¯• - è¿è¡ŒåŸºå‡†æµ‹è¯•")
        print("4. äº¤äº’å¯¹è¯ - è¿›å…¥èŠå¤©æ¨¡å¼")
        print("5. è‡ªå®šä¹‰ç”Ÿæˆ - è‡ªå®šä¹‰å‚æ•°ç”Ÿæˆ")
        print("0. é€€å‡º")

        choice = input("\nè¯·é€‰æ‹©åŠŸèƒ½ (0-5): ").strip()

        if choice == "0":
            print("\nğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼")
            break

        elif choice == "1":
            print("\n--- å¿«é€Ÿæµ‹è¯• ---")
            prompt = "ä»‹ç»ä¸€ä¸‹Pythonç¼–ç¨‹è¯­è¨€çš„ä¸»è¦ç‰¹ç‚¹"
            print(f"æç¤º: {prompt}")
            print("\nå›å¤: ", end="")
            response = demo.generate(prompt, max_length=150)

        elif choice == "2":
            print("\n--- ä»£ç ç”Ÿæˆ ---")
            prompt = "å†™ä¸€ä¸ªPythonå‡½æ•°ï¼Œè®¡ç®—åˆ—è¡¨ä¸­æ‰€æœ‰æ•°å­—çš„å¹³å‡å€¼"
            print(f"æç¤º: {prompt}")
            print("\nç”Ÿæˆçš„ä»£ç :\n")
            response = demo.generate(prompt, max_length=200, temperature=0.3)

        elif choice == "3":
            demo.benchmark()

        elif choice == "4":
            demo.interactive_chat()

        elif choice == "5":
            print("\n--- è‡ªå®šä¹‰ç”Ÿæˆ ---")
            prompt = input("è¯·è¾“å…¥æç¤ºæ–‡æœ¬: ")
            max_length = int(input("æœ€å¤§ç”Ÿæˆé•¿åº¦ (é»˜è®¤256): ") or "256")
            temperature = float(input("æ¸©åº¦ 0-1 (é»˜è®¤0.7): ") or "0.7")

            print(f"\nç”Ÿæˆä¸­...")
            response = demo.generate(
                prompt,
                max_length=max_length,
                temperature=temperature
            )
            print(f"\nç”Ÿæˆç»“æœ:\n{response}")

        else:
            print("âš ï¸ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡è¯•")


if __name__ == "__main__":
    main()