#!/usr/bin/env python3
"""
Gemma 270M é«˜çº§æ¡ä»¶åå°„è®­ç»ƒç³»ç»Ÿ
å®ç°ç§‘å­¦çš„æ¨¡å¼å­¦ä¹ å’Œè®­ç»ƒæœºåˆ¶
"""

import re
import time
import json
import numpy as np
from typing import List, Tuple, Dict, Optional, Callable
from dataclasses import dataclass, field
from collections import Counter, defaultdict
import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForCausalLM


@dataclass
class TrainingExample:
    """è®­ç»ƒæ ·æœ¬"""
    input_text: str
    expected_response: str
    metadata: Dict = field(default_factory=dict)


@dataclass
class Pattern:
    """å­¦ä¹ åˆ°çš„æ¨¡å¼"""
    regex_pattern: Optional[str] = None
    keyword_set: List[str] = field(default_factory=list)
    embedding_centroid: Optional[torch.Tensor] = None
    similarity_threshold: float = 0.8
    confidence: float = 0.5


class AdvancedReflexTrainer:
    """é«˜çº§åå°„è®­ç»ƒå™¨"""

    def __init__(self, model_id: str = "unsloth/gemma-3-270m-it"):
        self.model = None
        self.tokenizer = None
        self.model_id = model_id
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

    def load_model(self):
        """åŠ è½½Gemmaæ¨¡å‹ç”¨äºè¯­ä¹‰åˆ†æ"""
        print("ğŸ“š åŠ è½½Gemmaæ¨¡å‹ç”¨äºè®­ç»ƒ...")
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_id,
            torch_dtype=torch.float16 if self.device != "cpu" else torch.float32
        ).to(self.device)
        self.model.eval()
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

    def train_from_examples(self,
                           examples: List[Tuple[str, str]],
                           name: str) -> Pattern:
        """ä»ç¤ºä¾‹ä¸­è®­ç»ƒæ¨¡å¼"""
        print(f"\nğŸ“ å¼€å§‹è®­ç»ƒåå°„: {name}")
        print(f"   è®­ç»ƒæ ·æœ¬æ•°: {len(examples)}")

        # è½¬æ¢ä¸ºTrainingExampleå¯¹è±¡
        training_data = [
            TrainingExample(inp, resp)
            for inp, resp in examples
        ]

        # 1. æå–å¤šå±‚ç‰¹å¾
        features = self._extract_features(training_data)

        # 2. å­¦ä¹ æ¨¡å¼
        pattern = self._learn_pattern(features, training_data)

        # 3. éªŒè¯æ¨¡å¼
        accuracy = self._validate_pattern(pattern, training_data)
        pattern.confidence = accuracy

        print(f"âœ… è®­ç»ƒå®Œæˆï¼Œå‡†ç¡®ç‡: {accuracy:.2%}")
        return pattern

    def _extract_features(self, examples: List[TrainingExample]) -> Dict:
        """æå–ç‰¹å¾"""
        features = {
            'keywords': [],
            'patterns': [],
            'embeddings': [],
            'lengths': [],
            'pos_patterns': []
        }

        for example in examples:
            # æå–å…³é”®è¯
            keywords = self._extract_keywords(example.input_text)
            features['keywords'].extend(keywords)

            # æå–æ¨¡å¼
            patterns = self._extract_patterns(example.input_text)
            features['patterns'].extend(patterns)

            # æå–è¯­ä¹‰åµŒå…¥ï¼ˆå¦‚æœæ¨¡å‹å·²åŠ è½½ï¼‰
            if self.model:
                embedding = self._get_embedding(example.input_text)
                features['embeddings'].append(embedding)

            # é•¿åº¦ç‰¹å¾
            features['lengths'].append(len(example.input_text.split()))

        return features

    def _extract_keywords(self, text: str) -> List[str]:
        """æå–å…³é”®è¯ï¼ˆç®€åŒ–ç‰ˆTF-IDFï¼‰"""
        # åˆ†è¯
        words = re.findall(r'\w+', text.lower())

        # è¿‡æ»¤åœç”¨è¯
        stopwords = {'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æˆ‘', 'ä½ ', 'ä»–',
                    'the', 'is', 'a', 'an', 'and', 'or', 'but'}
        keywords = [w for w in words if w not in stopwords and len(w) > 1]

        return keywords

    def _extract_patterns(self, text: str) -> List[str]:
        """æå–æ–‡æœ¬æ¨¡å¼"""
        patterns = []

        # N-gramæ¨¡å¼
        words = text.lower().split()
        for n in [1, 2, 3]:
            for i in range(len(words) - n + 1):
                ngram = ' '.join(words[i:i+n])
                patterns.append(ngram)

        # ç‰¹æ®Šæ¨¡å¼
        if re.search(r'\d+', text):
            patterns.append('<NUMBER>')
        if re.search(r'[A-Z][a-z]+', text):
            patterns.append('<PROPER_NOUN>')
        if '?' in text or 'ï¼Ÿ' in text:
            patterns.append('<QUESTION>')

        return patterns

    def _get_embedding(self, text: str) -> torch.Tensor:
        """è·å–æ–‡æœ¬çš„è¯­ä¹‰åµŒå…¥"""
        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            max_length=128
        ).to(self.device)

        with torch.no_grad():
            outputs = self.model(**inputs, output_hidden_states=True)
            # ä½¿ç”¨æœ€åä¸€å±‚éšè—çŠ¶æ€çš„å¹³å‡ä½œä¸ºåµŒå…¥
            last_hidden = outputs.hidden_states[-1]
            embedding = last_hidden.mean(dim=1).squeeze()

        return embedding

    def _learn_pattern(self, features: Dict,
                       examples: List[TrainingExample]) -> Pattern:
        """å­¦ä¹ æ¨¡å¼"""
        pattern = Pattern()

        # 1. å­¦ä¹ å…³é”®è¯æ¨¡å¼
        if features['keywords']:
            # æ‰¾å‡ºé«˜é¢‘å…³é”®è¯
            keyword_counter = Counter(features['keywords'])
            # é€‰æ‹©å‡ºç°åœ¨50%ä»¥ä¸Šæ ·æœ¬ä¸­çš„å…³é”®è¯
            threshold = len(examples) * 0.5
            common_keywords = [
                word for word, count in keyword_counter.items()
                if count >= threshold
            ]
            pattern.keyword_set = common_keywords

            # æ„å»ºæ­£åˆ™è¡¨è¾¾å¼
            if common_keywords:
                # åˆ›å»ºåŒ…å«æ‰€æœ‰å…³é”®è¯çš„æ­£åˆ™è¡¨è¾¾å¼
                pattern.regex_pattern = '|'.join(
                    re.escape(kw) for kw in common_keywords
                )

        # 2. å­¦ä¹ è¯­ä¹‰æ¨¡å¼ï¼ˆå¦‚æœæœ‰åµŒå…¥ï¼‰
        if features['embeddings']:
            # è®¡ç®—åµŒå…¥è´¨å¿ƒ
            embeddings = torch.stack(features['embeddings'])
            pattern.embedding_centroid = embeddings.mean(dim=0)

            # è®¡ç®—é˜ˆå€¼ï¼ˆåŸºäºæ ·æœ¬å†…ç›¸ä¼¼åº¦ï¼‰
            similarities = []
            for emb in features['embeddings']:
                sim = F.cosine_similarity(
                    emb.unsqueeze(0),
                    pattern.embedding_centroid.unsqueeze(0)
                ).item()
                similarities.append(sim)

            # è®¾ç½®é˜ˆå€¼ä¸ºæœ€å°ç›¸ä¼¼åº¦çš„90%
            pattern.similarity_threshold = min(similarities) * 0.9

        return pattern

    def _validate_pattern(self, pattern: Pattern,
                         examples: List[TrainingExample]) -> float:
        """éªŒè¯æ¨¡å¼å‡†ç¡®ç‡"""
        correct = 0
        total = len(examples)

        for example in examples:
            if self._match_pattern(pattern, example.input_text):
                correct += 1

        return correct / total if total > 0 else 0

    def _match_pattern(self, pattern: Pattern, text: str) -> bool:
        """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ¹é…æ¨¡å¼"""
        # 1. æ£€æŸ¥æ­£åˆ™è¡¨è¾¾å¼
        if pattern.regex_pattern:
            if re.search(pattern.regex_pattern, text.lower()):
                return True

        # 2. æ£€æŸ¥å…³é”®è¯
        if pattern.keyword_set:
            text_words = set(re.findall(r'\w+', text.lower()))
            if text_words.intersection(pattern.keyword_set):
                return True

        # 3. æ£€æŸ¥è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆå¦‚æœæœ‰æ¨¡å‹å’Œè´¨å¿ƒï¼‰
        if self.model and pattern.embedding_centroid is not None:
            embedding = self._get_embedding(text)
            similarity = F.cosine_similarity(
                embedding.unsqueeze(0),
                pattern.embedding_centroid.unsqueeze(0)
            ).item()
            if similarity >= pattern.similarity_threshold:
                return True

        return False


class IncrementalReflexLearner:
    """å¢é‡å­¦ä¹ ç³»ç»Ÿ"""

    def __init__(self, trainer: AdvancedReflexTrainer):
        self.trainer = trainer
        self.positive_examples = defaultdict(list)  # æ­£ä¾‹
        self.negative_examples = defaultdict(list)  # è´Ÿä¾‹
        self.patterns = {}  # å­¦ä¹ åˆ°çš„æ¨¡å¼
        self.performance_history = defaultdict(list)  # æ€§èƒ½å†å²

    def add_feedback(self, reflex_name: str, input_text: str,
                     response: str, is_correct: bool):
        """æ·»åŠ åé¦ˆæ ·æœ¬"""
        example = TrainingExample(input_text, response)

        if is_correct:
            self.positive_examples[reflex_name].append(example)
            print(f"âœ… æ·»åŠ æ­£ä¾‹åˆ° {reflex_name}")
        else:
            self.negative_examples[reflex_name].append(example)
            print(f"âŒ æ·»åŠ è´Ÿä¾‹åˆ° {reflex_name}")

        # å¦‚æœç§¯ç´¯äº†è¶³å¤Ÿçš„æ ·æœ¬ï¼Œé‡æ–°è®­ç»ƒ
        total_examples = (len(self.positive_examples[reflex_name]) +
                         len(self.negative_examples[reflex_name]))

        if total_examples >= 5 and total_examples % 5 == 0:
            print(f"ğŸ“Š æ ·æœ¬æ•°è¾¾åˆ°{total_examples}ï¼Œè§¦å‘é‡æ–°è®­ç»ƒ")
            self.retrain(reflex_name)

    def retrain(self, reflex_name: str):
        """é‡æ–°è®­ç»ƒåå°„"""
        positives = self.positive_examples[reflex_name]
        negatives = self.negative_examples[reflex_name]

        if len(positives) < 3:
            print(f"âš ï¸ æ­£ä¾‹å¤ªå°‘({len(positives)}ä¸ª)ï¼Œæš‚ä¸è®­ç»ƒ")
            return

        # ä»æ­£ä¾‹å­¦ä¹ æ¨¡å¼
        positive_pattern = self.trainer._learn_pattern(
            self.trainer._extract_features(positives),
            positives
        )

        # å¦‚æœæœ‰è´Ÿä¾‹ï¼Œè°ƒæ•´æ¨¡å¼ä»¥æ’é™¤å®ƒä»¬
        if negatives:
            # æ‰¾å‡ºåŒºåˆ†æ€§ç‰¹å¾
            positive_features = self._get_discriminative_features(
                positives, negatives
            )
            positive_pattern.keyword_set = positive_features

        # è¯„ä¼°æ–°æ¨¡å¼
        accuracy = self._evaluate_pattern(
            positive_pattern, positives, negatives
        )

        # è®°å½•æ€§èƒ½
        self.performance_history[reflex_name].append({
            'timestamp': time.time(),
            'accuracy': accuracy,
            'num_positive': len(positives),
            'num_negative': len(negatives)
        })

        # ä¿å­˜æ¨¡å¼
        self.patterns[reflex_name] = positive_pattern

        print(f"ğŸ¯ {reflex_name} é‡è®­ç»ƒå®Œæˆï¼Œå‡†ç¡®ç‡: {accuracy:.2%}")

    def _get_discriminative_features(self,
                                    positives: List[TrainingExample],
                                    negatives: List[TrainingExample]) -> List[str]:
        """æ‰¾å‡ºåŒºåˆ†æ­£è´Ÿä¾‹çš„ç‰¹å¾"""
        # æå–æ­£ä¾‹ç‰¹å¾
        positive_words = Counter()
        for ex in positives:
            words = self.trainer._extract_keywords(ex.input_text)
            positive_words.update(words)

        # æå–è´Ÿä¾‹ç‰¹å¾
        negative_words = Counter()
        for ex in negatives:
            words = self.trainer._extract_keywords(ex.input_text)
            negative_words.update(words)

        # æ‰¾å‡ºä¸»è¦å‡ºç°åœ¨æ­£ä¾‹ä¸­çš„è¯
        discriminative = []
        for word, pos_count in positive_words.items():
            neg_count = negative_words.get(word, 0)
            # æ­£ä¾‹ä¸­å‡ºç°é¢‘ç‡æ˜æ˜¾é«˜äºè´Ÿä¾‹
            if pos_count > neg_count * 2:
                discriminative.append(word)

        return discriminative

    def _evaluate_pattern(self, pattern: Pattern,
                         positives: List[TrainingExample],
                         negatives: List[TrainingExample]) -> float:
        """è¯„ä¼°æ¨¡å¼å‡†ç¡®ç‡"""
        correct = 0
        total = 0

        # æµ‹è¯•æ­£ä¾‹ï¼ˆåº”è¯¥åŒ¹é…ï¼‰
        for ex in positives:
            if self.trainer._match_pattern(pattern, ex.input_text):
                correct += 1
            total += 1

        # æµ‹è¯•è´Ÿä¾‹ï¼ˆä¸åº”è¯¥åŒ¹é…ï¼‰
        for ex in negatives:
            if not self.trainer._match_pattern(pattern, ex.input_text):
                correct += 1
            total += 1

        return correct / total if total > 0 else 0


class ActiveLearningReflex:
    """ä¸»åŠ¨å­¦ä¹ ç³»ç»Ÿï¼šä¸»åŠ¨è¯·æ±‚æ ‡æ³¨ä¸ç¡®å®šçš„æ ·æœ¬"""

    def __init__(self, trainer: AdvancedReflexTrainer):
        self.trainer = trainer
        self.uncertainty_threshold = 0.3
        self.query_queue = []  # å¾…æ ‡æ³¨é˜Ÿåˆ—
        self.labeled_data = defaultdict(list)  # å·²æ ‡æ³¨æ•°æ®

    def predict_with_uncertainty(self, input_text: str,
                                patterns: Dict[str, Pattern]) -> Tuple[Optional[str], float]:
        """é¢„æµ‹å¹¶è¿”å›ä¸ç¡®å®šæ€§"""
        matches = []

        # å¯¹æ‰€æœ‰æ¨¡å¼è®¡ç®—åŒ¹é…åº¦
        for name, pattern in patterns.items():
            if self.trainer._match_pattern(pattern, input_text):
                # è®¡ç®—ç½®ä¿¡åº¦
                confidence = self._calculate_confidence(pattern, input_text)
                matches.append((name, confidence))

        if not matches:
            # æ²¡æœ‰åŒ¹é…ï¼Œé«˜ä¸ç¡®å®šæ€§
            self.query_queue.append(input_text)
            return None, 1.0

        # æŒ‰ç½®ä¿¡åº¦æ’åº
        matches.sort(key=lambda x: x[1], reverse=True)
        best_match, best_confidence = matches[0]

        # è®¡ç®—ä¸ç¡®å®šæ€§
        if len(matches) > 1:
            # å¦‚æœæœ‰å¤šä¸ªåŒ¹é…ï¼Œä¸ç¡®å®šæ€§åŸºäºæœ€ä½³å’Œæ¬¡ä½³çš„å·®è·
            second_confidence = matches[1][1]
            uncertainty = 1.0 - (best_confidence - second_confidence)
        else:
            # åªæœ‰ä¸€ä¸ªåŒ¹é…ï¼Œä¸ç¡®å®šæ€§åŸºäºç½®ä¿¡åº¦
            uncertainty = 1.0 - best_confidence

        # å¦‚æœä¸ç¡®å®šæ€§å¤ªé«˜ï¼Œè¯·æ±‚æ ‡æ³¨
        if uncertainty > self.uncertainty_threshold:
            self.query_queue.append(input_text)
            print(f"â“ ä¸ç¡®å®šæ ·æœ¬å·²åŠ å…¥é˜Ÿåˆ—: {input_text[:30]}...")

        return best_match, uncertainty

    def _calculate_confidence(self, pattern: Pattern, text: str) -> float:
        """è®¡ç®—åŒ¹é…ç½®ä¿¡åº¦"""
        confidence = pattern.confidence  # åŸºç¡€ç½®ä¿¡åº¦

        # æ ¹æ®åŒ¹é…ç±»å‹è°ƒæ•´
        if pattern.regex_pattern and re.search(pattern.regex_pattern, text.lower()):
            confidence *= 1.2  # æ­£åˆ™åŒ¹é…åŠ åˆ†

        if pattern.keyword_set:
            text_words = set(re.findall(r'\w+', text.lower()))
            overlap = len(text_words.intersection(pattern.keyword_set))
            confidence *= (1 + overlap * 0.1)  # å…³é”®è¯é‡å åŠ åˆ†

        return min(confidence, 1.0)  # é™åˆ¶åœ¨0-1èŒƒå›´

    def request_labels(self, max_queries: int = 5):
        """è¯·æ±‚äººå·¥æ ‡æ³¨æœ€ä¸ç¡®å®šçš„æ ·æœ¬"""
        if not self.query_queue:
            print("ğŸ“­ æ²¡æœ‰å¾…æ ‡æ³¨çš„æ ·æœ¬")
            return

        # é€‰æ‹©æœ€ä¸ç¡®å®šçš„æ ·æœ¬
        queries = self.query_queue[:max_queries]
        self.query_queue = self.query_queue[max_queries:]

        print(f"\nğŸ·ï¸ è¯·æ ‡æ³¨ä»¥ä¸‹{len(queries)}ä¸ªæ ·æœ¬ï¼š")

        for i, query in enumerate(queries, 1):
            print(f"\n{i}. è¾“å…¥: {query}")
            label = input("   åº”è¯¥è§¦å‘å“ªä¸ªåå°„ï¼Ÿ(è¾“å…¥åç§°æˆ–'none'): ")

            if label and label != 'none':
                response = input("   æœŸæœ›çš„å“åº”: ")
                self.labeled_data[label].append(
                    TrainingExample(query, response)
                )
                print(f"   âœ… å·²æ ‡æ³¨ä¸º: {label}")

        return self.labeled_data


def demo_advanced_training():
    """æ¼”ç¤ºé«˜çº§è®­ç»ƒåŠŸèƒ½"""
    print("\n" + "="*50)
    print("ğŸ§ª é«˜çº§åå°„è®­ç»ƒæ¼”ç¤º")
    print("="*50)

    # åˆå§‹åŒ–è®­ç»ƒå™¨
    trainer = AdvancedReflexTrainer()

    # è®­ç»ƒç¤ºä¾‹1ï¼šé—®å€™åå°„
    greeting_examples = [
        ("ä½ å¥½", "æ‚¨å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿ"),
        ("æ—©ä¸Šå¥½", "æ—©ä¸Šå¥½ï¼ç¥æ‚¨æœ‰ç¾å¥½çš„ä¸€å¤©ï¼"),
        ("æ™šä¸Šå¥½", "æ™šä¸Šå¥½ï¼ä»Šå¤©è¿‡å¾—æ€ä¹ˆæ ·ï¼Ÿ"),
        ("å—¨", "å—¨ï¼å¾ˆé«˜å…´è§åˆ°æ‚¨ï¼"),
        ("æ‚¨å¥½", "æ‚¨å¥½ï¼è¯·é—®æœ‰ä»€ä¹ˆéœ€è¦å¸®åŠ©çš„å—ï¼Ÿ")
    ]

    print("\nè®­ç»ƒé—®å€™åå°„...")
    greeting_pattern = trainer.train_from_examples(greeting_examples, "greeting")

    # æµ‹è¯•æ³›åŒ–èƒ½åŠ›
    test_inputs = [
        "ä½ å¥½å•Š",  # å˜ä½“
        "æ—©å®‰",    # åŒä¹‰è¯
        "Hi",      # è‹±æ–‡
        "åƒé¥­äº†å—", # ä¸ç›¸å…³
    ]

    print("\næµ‹è¯•æ³›åŒ–èƒ½åŠ›:")
    for test in test_inputs:
        match = trainer._match_pattern(greeting_pattern, test)
        print(f"  '{test}' -> {'åŒ¹é…' if match else 'ä¸åŒ¹é…'}")

    # è®­ç»ƒç¤ºä¾‹2ï¼šæ•°å­¦è®¡ç®—åå°„
    math_examples = [
        ("1+1ç­‰äºå¤šå°‘", "è®©æˆ‘è®¡ç®—ä¸€ä¸‹ï¼š1+1=2"),
        ("100å‡å»50", "è®©æˆ‘è®¡ç®—ä¸€ä¸‹ï¼š100-50=50"),
        ("5ä¹˜ä»¥6", "è®©æˆ‘è®¡ç®—ä¸€ä¸‹ï¼š5*6=30"),
        ("è®¡ç®—10é™¤ä»¥2", "è®©æˆ‘è®¡ç®—ä¸€ä¸‹ï¼š10/2=5"),
        ("20åŠ 30çš„ç»“æœ", "è®©æˆ‘è®¡ç®—ä¸€ä¸‹ï¼š20+30=50")
    ]

    print("\nè®­ç»ƒæ•°å­¦åå°„...")
    math_pattern = trainer.train_from_examples(math_examples, "math")


def demo_incremental_learning():
    """æ¼”ç¤ºå¢é‡å­¦ä¹ """
    print("\n" + "="*50)
    print("ğŸ“ˆ å¢é‡å­¦ä¹ æ¼”ç¤º")
    print("="*50)

    trainer = AdvancedReflexTrainer()
    learner = IncrementalReflexLearner(trainer)

    # åˆå§‹è®­ç»ƒ
    print("\nåˆå§‹è®­ç»ƒé˜¶æ®µ:")
    learner.add_feedback("weather", "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·", "æŸ¥è¯¢å¤©æ°”ä¸­...", True)
    learner.add_feedback("weather", "å¤©æ°”é¢„æŠ¥", "æŸ¥è¯¢å¤©æ°”ä¸­...", True)
    learner.add_feedback("weather", "ä¼šä¸‹é›¨å—", "æŸ¥è¯¢å¤©æ°”ä¸­...", True)

    # æ·»åŠ è´Ÿä¾‹
    print("\næ·»åŠ è´Ÿä¾‹:")
    learner.add_feedback("weather", "ä½ å¥½", "æŸ¥è¯¢å¤©æ°”ä¸­...", False)
    learner.add_feedback("weather", "ç°åœ¨å‡ ç‚¹", "æŸ¥è¯¢å¤©æ°”ä¸­...", False)

    # è§¦å‘é‡è®­ç»ƒ
    learner.retrain("weather")

    # æŸ¥çœ‹æ€§èƒ½å†å²
    if "weather" in learner.performance_history:
        print("\næ€§èƒ½å†å²:")
        for record in learner.performance_history["weather"]:
            print(f"  å‡†ç¡®ç‡: {record['accuracy']:.2%} "
                  f"(æ­£ä¾‹: {record['num_positive']}, "
                  f"è´Ÿä¾‹: {record['num_negative']})")


def demo_active_learning():
    """æ¼”ç¤ºä¸»åŠ¨å­¦ä¹ """
    print("\n" + "="*50)
    print("ğŸ¯ ä¸»åŠ¨å­¦ä¹ æ¼”ç¤º")
    print("="*50)

    trainer = AdvancedReflexTrainer()
    active_learner = ActiveLearningReflex(trainer)

    # åˆ›å»ºä¸€äº›æ¨¡å¼
    patterns = {
        "greeting": Pattern(regex_pattern="ä½ å¥½|æ—©ä¸Šå¥½|æ™šä¸Šå¥½", confidence=0.9),
        "question": Pattern(regex_pattern="ä»€ä¹ˆ|æ€ä¹ˆ|ä¸ºä»€ä¹ˆ", confidence=0.8),
    }

    # æµ‹è¯•ä¸ç¡®å®šçš„è¾“å…¥
    test_inputs = [
        "ä½ å¥½å—",      # å¯èƒ½æ˜¯é—®å€™ä¹Ÿå¯èƒ½æ˜¯é—®é¢˜
        "æ—©",         # ä¸å®Œæ•´çš„é—®å€™
        "ä»€ä¹ˆæ—¶å€™",    # æ˜ç¡®çš„é—®é¢˜
        "å“ˆå–½",       # é—®å€™çš„å˜ä½“
        "è¿™æ˜¯å•¥"      # å£è¯­åŒ–çš„é—®é¢˜
    ]

    print("\né¢„æµ‹ç»“æœ:")
    for text in test_inputs:
        prediction, uncertainty = active_learner.predict_with_uncertainty(
            text, patterns
        )
        print(f"  '{text}' -> é¢„æµ‹: {prediction}, "
              f"ä¸ç¡®å®šæ€§: {uncertainty:.2f}")

    # è¯·æ±‚æ ‡æ³¨
    print("\nè¯·æ±‚äººå·¥æ ‡æ³¨:")
    active_learner.request_labels(max_queries=3)


def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("ğŸ§  Gemma 270M é«˜çº§åå°„è®­ç»ƒç³»ç»Ÿ")
    print("="*60)

    while True:
        print("\n" + "="*60)
        print("ğŸ“‹ åŠŸèƒ½èœå•")
        print("="*60)
        print("1. é«˜çº§è®­ç»ƒæ¼”ç¤º")
        print("2. å¢é‡å­¦ä¹ æ¼”ç¤º")
        print("3. ä¸»åŠ¨å­¦ä¹ æ¼”ç¤º")
        print("4. åŠ è½½æ¨¡å‹å¹¶è®­ç»ƒï¼ˆéœ€è¦Gemmaï¼‰")
        print("0. é€€å‡º")

        choice = input("\né€‰æ‹©åŠŸèƒ½ (0-4): ").strip()

        if choice == "0":
            break
        elif choice == "1":
            demo_advanced_training()
        elif choice == "2":
            demo_incremental_learning()
        elif choice == "3":
            demo_active_learning()
        elif choice == "4":
            print("\nåŠ è½½Gemmaæ¨¡å‹ä¸­...")
            trainer = AdvancedReflexTrainer()
            trainer.load_model()

            # ä½¿ç”¨è¯­ä¹‰åµŒå…¥è®­ç»ƒ
            examples = [
                ("æ‰“å¼€ç¯", "æ­£åœ¨æ‰“å¼€ç…§æ˜..."),
                ("å¼€ç¯", "æ­£åœ¨æ‰“å¼€ç…§æ˜..."),
                ("æŠŠç¯æ‰“å¼€", "æ­£åœ¨æ‰“å¼€ç…§æ˜..."),
                ("ç¯å…‰å¼€å¯", "æ­£åœ¨æ‰“å¼€ç…§æ˜...")
            ]

            pattern = trainer.train_from_examples(examples, "light_control")

            # æµ‹è¯•è¯­ä¹‰ç›¸ä¼¼çš„è¾“å…¥
            test = "è¯·å¼€ä¸€ä¸‹ç¯"
            match = trainer._match_pattern(pattern, test)
            print(f"\næµ‹è¯• '{test}' -> {'åŒ¹é…' if match else 'ä¸åŒ¹é…'}")
        else:
            print("æ— æ•ˆé€‰æ‹©")

    print("\nğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨é«˜çº§è®­ç»ƒç³»ç»Ÿï¼")


if __name__ == "__main__":
    main()