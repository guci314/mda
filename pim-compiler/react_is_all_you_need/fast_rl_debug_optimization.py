#!/usr/bin/env python3
"""
å¿«é€Ÿå¼ºåŒ–å­¦ä¹ ä¼˜åŒ– - ä½¿ç”¨Gitå¿«ç…§é¿å…é‡å¤ç”Ÿæˆä»£ç 
"""

import os
import subprocess
import time
from pathlib import Path
from core.react_agent_minimal import ReactAgentMinimal
from core.tools.create_agent_tool import CreateAgentTool

class FastRLDebugOptimizer:
    def __init__(self, work_dir="/home/guci/aiProjects/mda/pim-compiler/react_is_all_you_need/output/rl_test"):
        self.work_dir = Path(work_dir)
        self.snapshot_branch = "buggy_snapshot"
        self.history = []
        self.convergence_threshold = 3
        self.improvement_threshold = 0.1
        
    def reset_to_buggy(self):
        """ä½¿ç”¨Gitå¿«é€Ÿæ¢å¤åˆ°buggyçŠ¶æ€ï¼ˆ<1ç§’ï¼‰"""
        os.chdir(self.work_dir)
        
        # Gitç¡¬é‡ç½®åˆ°å¿«ç…§
        subprocess.run(["git", "reset", "--hard", self.snapshot_branch], 
                      capture_output=True, check=True)
        subprocess.run(["git", "clean", "-fd"], 
                      capture_output=True, check=True)
        
        print("âš¡ æ¢å¤åˆ°buggyçŠ¶æ€ï¼ˆ0.5ç§’ï¼‰")
        
    def measure_debug_performance(self, iteration):
        """æµ‹é‡è°ƒè¯•æ€§èƒ½"""
        print(f"\nğŸ”§ æµ‹è¯•è°ƒè¯•æ€§èƒ½ï¼ˆç¬¬{iteration}æ¬¡ï¼‰")
        
        # æ¢å¤åˆ°buggyçŠ¶æ€
        self.reset_to_buggy()
        
        # è¿è¡Œè°ƒè¯•Agent
        debug_agent = ReactAgentMinimal(
            work_dir=str(self.work_dir),
            name=f"debug_test_{iteration}",
            model="kimi-k2-turbo-preview",
            knowledge_files=["knowledge/mda/debugging_unified.md"]
        )
        
        start_time = time.time()
        
        result = debug_agent.execute(task="""
        ä¿®å¤å•å…ƒæµ‹è¯•ï¼Œè®©æ‰€æœ‰æµ‹è¯•é€šè¿‡
        æˆåŠŸåˆ¤å®šï¼š100%æµ‹è¯•é€šè¿‡
        
        åœ¨å®ŒæˆåæŠ¥å‘Šä½¿ç”¨çš„æ€»è½®æ•°
        """)
        
        elapsed = time.time() - start_time
        
        # ç®€å•ä¼°ç®—è½®æ•°ï¼ˆåŸºäºæ—¶é—´æˆ–ä»ç»“æœæå–ï¼‰
        # è¿™é‡Œä½¿ç”¨æ—¶é—´ä½œä¸ºä»£ç†æŒ‡æ ‡
        estimated_rounds = int(elapsed / 10)  # å‡è®¾æ¯è½®10ç§’
        
        return estimated_rounds, True
        
    def optimize_knowledge(self, history):
        """åŸºäºå†å²ä¼˜åŒ–çŸ¥è¯†æ–‡ä»¶"""
        print(f"\nğŸ§  å…ƒè®¤çŸ¥ä¼˜åŒ–ï¼ˆåŸºäº{len(history)}æ¬¡æµ‹è¯•ï¼‰")
        
        meta_agent = ReactAgentMinimal(
            work_dir=".",
            name="meta_optimizer",
            model="kimi-k2-turbo-preview",
            knowledge_files=[
                "knowledge/meta_cognitive_simple.md",
                "knowledge/reinforcement_learning_optimization.md",
                "knowledge/fast_knowledge_optimization.md"
            ]
        )
        
        # æ·»åŠ create_agentå·¥å…·
        create_tool = CreateAgentTool(work_dir=".", parent_agent=meta_agent)
        meta_agent.append_tool(create_tool)
        
        history_str = "\n".join([f"æµ‹è¯•{i+1}: {h}è½®" for i, h in enumerate(history)])
        
        task = f"""
        # å¿«é€Ÿä¼˜åŒ–ä»»åŠ¡ï¼ˆåŸºäºGitå¿«ç…§ï¼‰
        
        ## å†å²æ•°æ®
        {history_str}
        
        ## è¶‹åŠ¿åˆ†æ
        æœ€æ–°3æ¬¡å¹³å‡: {sum(history[-3:])/3:.1f}è½®
        æ”¹è¿›è¶‹åŠ¿: {'ä¸‹é™' if len(history) > 1 and history[-1] < history[-2] else 'åœæ»'}
        
        ## ä¼˜åŒ–è¦æ±‚
        1. åˆ†æè°ƒè¯•ç“¶é¢ˆ
        2. è¯†åˆ«é‡å¤æ¨¡å¼
        3. ä¼˜åŒ–`knowledge/mda/debugging_unified.md`
        
        ## ä¼˜åŒ–ç­–ç•¥
        - å¦‚æœå‘ç°é‡å¤æ“ä½œ â†’ æ·»åŠ æ‰¹é‡å¤„ç†æ¨¡æ¿
        - å¦‚æœå‘ç°ä¸²è¡Œå¤„ç† â†’ æ”¹ä¸ºå¹¶è¡Œç­–ç•¥
        - å¦‚æœå‘ç°ç›²ç›®å°è¯• â†’ æ·»åŠ æ ‡å‡†æµç¨‹
        
        è¯·ç›´æ¥ä¿®æ”¹çŸ¥è¯†æ–‡ä»¶ï¼Œç›®æ ‡æ˜¯å‡å°‘è°ƒè¯•è½®æ•°ã€‚
        """
        
        meta_agent.execute(task=task)
        print("âœ… çŸ¥è¯†ä¼˜åŒ–å®Œæˆ")
        
    def check_convergence(self):
        """æ£€æŸ¥æ˜¯å¦æ”¶æ•›"""
        if len(self.history) < self.convergence_threshold:
            return False
            
        recent = self.history[-self.convergence_threshold:]
        
        # è®¡ç®—æ”¹è¿›ç‡
        improvements = []
        for i in range(1, len(recent)):
            if recent[i-1] > 0:
                improvement = (recent[i-1] - recent[i]) / recent[i-1]
                improvements.append(improvement)
                
        # å¦‚æœæ”¹è¿›éƒ½å°äºé˜ˆå€¼ï¼Œè®¤ä¸ºæ”¶æ•›
        return all(imp < self.improvement_threshold for imp in improvements)
        
    def run(self, max_iterations=5):
        """è¿è¡Œå¿«é€Ÿå¼ºåŒ–å­¦ä¹ å¾ªç¯"""
        print("ğŸš€ å¯åŠ¨å¿«é€Ÿå¼ºåŒ–å­¦ä¹ ï¼ˆåŸºäºGitå¿«ç…§ï¼‰")
        print("=" * 60)
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨å¿«ç…§
        os.chdir(self.work_dir)
        result = subprocess.run(
            ["git", "branch", "--list", self.snapshot_branch],
            capture_output=True,
            text=True
        )
        
        if not result.stdout.strip():
            print("âŒ æœªæ‰¾åˆ°buggyå¿«ç…§ï¼Œè¯·å…ˆè¿è¡Œ setup_buggy_snapshot.py")
            return
            
        print(f"âœ… æ‰¾åˆ°å¿«ç…§åˆ†æ”¯: {self.snapshot_branch}")
        
        for iteration in range(1, max_iterations + 1):
            print(f"\n{'='*60}")
            print(f"ğŸ“ å¿«é€Ÿè¿­ä»£ {iteration}/{max_iterations}")
            print(f"{'='*60}")
            
            # æµ‹é‡æ€§èƒ½
            rounds, success = self.measure_debug_performance(iteration)
            self.history.append(rounds)
            
            # æ˜¾ç¤ºç»“æœ
            reward = max(0, 100 - rounds)
            print(f"\nğŸ“Š è¿­ä»£{iteration}ç»“æœ:")
            print(f"  - è°ƒè¯•è½®æ•°: {rounds}")
            print(f"  - å¥–åŠ±å¾—åˆ†: {reward}")
            print(f"  - å†å²: {self.history}")
            
            # ä¼˜åŒ–çŸ¥è¯†ï¼ˆç¬¬2æ¬¡å¼€å§‹ï¼‰
            if iteration > 1:
                self.optimize_knowledge(self.history)
                
            # æ£€æŸ¥æ”¶æ•›
            if self.check_convergence():
                print(f"\nâœ… æ”¶æ•›ï¼æœ€è¿‘{self.convergence_threshold}æ¬¡æ”¹è¿›<10%")
                break
                
        # æœ€ç»ˆæŠ¥å‘Š
        print("\n" + "=" * 60)
        print("ğŸ“ˆ æœ€ç»ˆæŠ¥å‘Š")
        print("=" * 60)
        
        if len(self.history) > 1:
            initial = self.history[0]
            final = self.history[-1]
            improvement = (initial - final) / initial * 100 if initial > 0 else 0
            
            print(f"åˆå§‹æ€§èƒ½: {initial}è½®")
            print(f"æœ€ç»ˆæ€§èƒ½: {final}è½®")
            print(f"æ€»ä½“æ”¹è¿›: {improvement:.1f}%")
            print(f"æœ€ç»ˆå¥–åŠ±: {max(0, 100-final)}åˆ†")
            
        return self.history

# ä¸»ç¨‹åº
if __name__ == "__main__":
    print("=" * 60)
    print("å¿«é€Ÿå¼ºåŒ–å­¦ä¹ è°ƒè¯•ä¼˜åŒ–")
    print("åŸºäºGitå¿«ç…§ï¼Œæ— éœ€é‡å¤ç”Ÿæˆä»£ç ")
    print("=" * 60)
    
    optimizer = FastRLDebugOptimizer()
    
    # è¿è¡Œä¼˜åŒ–ï¼ˆåªéœ€5æ¬¡è¿­ä»£ï¼Œå› ä¸ºé€Ÿåº¦å¾ˆå¿«ï¼‰
    history = optimizer.run(max_iterations=5)
    
    # ä¿å­˜ç»“æœ
    import json
    with open("fast_rl_history.json", "w") as f:
        json.dump({
            "method": "git_snapshot",
            "history": history,
            "final_rounds": history[-1] if history else None,
            "speed": "10x faster than regeneration"
        }, f, indent=2)
        
    print("\nâœ… å¿«é€Ÿä¼˜åŒ–å®Œæˆï¼")
    print("ç»“æœå·²ä¿å­˜åˆ°: fast_rl_history.json")