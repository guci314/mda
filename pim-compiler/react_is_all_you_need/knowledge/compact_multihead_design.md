# Compact多头注意力机制设计文档

## 当前状态（v1.0）

**设计日期**：2025-10-14
**状态**：理论验证通过，实践中待优化

## 核心设计

### 三头注意力架构

```
每条消息 → [Head1: 上级注意力（Authority）] → L0/L1/L3/L4
          → [Head2: 自我注意力（Experience）] → L2/L3/L4
          → [Head3: 环境注意力（Context）]   → L1/L2/L3/L4
          ↓
    Max-Pooling融合
          ↓
      最终层级（L0-L4）
```

### 理论基础

#### 1. 类比Transformer Multi-Head Attention
- **Transformer**: 8-16个头，隐式学习分工
- **Compact**: 3个头，显式设计分工
- **共同点**: 并行处理，多视角理解

#### 2. 归纳偏置（Inductive Bias）
- **来源维度**: 上级（用户）、自己（经验）、下级（环境）
- **假设**: 信息重要性与来源相关
- **灵感**: 面向对象编程的Object关系

#### 3. 融合策略：Max-Pooling
- **原理**: 保守策略，只要任一头认为重要就保留
- **原因**: 防止关键信息因"难以分类"而降级
- **对比**: Transformer用Average/Concat（信息聚合）

## 已验证特性 ✅

### 1. 多头并行工作
**测试**：`test_multihead_attention.py`
**结果**：三头复合消息同时输出L0+L1+L2
**证明**：不是顺序分类，是并行多头

### 2. Max-Pooling融合
**测试**：纠正（L0）+ 经验（L2）→ 最终L0
**结果**：取最高权重，不是平均
**证明**：使用Max而非Average

### 3. 语义理解而非规则
**测试**：`test_connectionism.py`
**结果**：100%识别未见过的纠正表达
**证明**：基于语义，不是关键词匹配

### 4. 复合信息处理
**测试**：单消息激活多个头
**结果**：信息可以同时是"纠正"+"配置"+"经验"
**证明**：不会因多义性丢失信息

## 当前归纳偏置的设计

### Head 1: 上级注意力（Authority Attention）

| 语义特征 | 输出层级 | 设计依据 |
|---------|---------|---------|
| 用户明确纠正 | L0 | 纠正=必须遵守 |
| 用户强制要求 | L0 | 指令=高权威 |
| 用户表达偏好 | L1 | 偏好=重要参考 |
| 普通用户对话 | L3 | 常规交互 |
| 无关闲聊 | L4 | 删除 |

**核心假设**：用户输入的权威性决定重要性

### Head 2: 自我注意力（Experience Attention）

| 语义特征 | 输出层级 | 设计依据 |
|---------|---------|---------|
| 问题解决方案 | L2 | 可复用经验 |
| 决策和方法论 | L2 | 算法知识 |
| 算法优化思路 | L2 | 改进模式 |
| 执行过程细节 | L3 | 非关键过程 |
| 无效尝试 | L4 | 删除 |

**核心假设**：经验的可复用性决定重要性

### Head 3: 环境注意力（Context Attention）

| 语义特征 | 输出层级 | 设计依据 |
|---------|---------|---------|
| 关键API配置 | L1 | 长期稳定配置 |
| 项目结构信息 | L1 | 基础依赖 |
| 工具使用经验 | L2 | 可复用知识 |
| 临时配置 | L3 | 短期有效 |
| 过时信息 | L4 | 删除 |

**核心假设**：配置的稳定性决定重要性

## 未来微调方向 🔧

### 1. 归纳偏置优化

#### 问题1：Head分工可能不够精确
**现象**：测试6中"记住这个配置"激活了Head1
**原因**："记住"被理解为指令（上级），而非配置（环境）
**可能方案**：
- 更细粒度的语义区分
- 添加第四个头（指令注意力）
- 调整Head1的语义范围

#### 问题2：L0-L4边界模糊
**现象**：L1和L2有时难以区分
**原因**："关键配置"和"配置经验"语义重叠
**可能方案**：
- 重新定义层级语义
- 合并L1和L2为"重要信息"
- 增加层级数量（L0-L6）

#### 问题3：Max-Pooling可能过于保守
**现象**：所有复合消息都输出L0
**原因**：Max总是取最高权重
**可能方案**：
- 加权Max（weighted-max）
- 动态阈值（adaptive threshold）
- 分层输出（L0主+L1/L2辅）

### 2. 自适应归纳偏置

#### 方案1：从用户反馈学习
```python
# 用户纠正压缩结果
user_feedback = {
    "message": "这条很重要，不应该删除",
    "expected_layer": "L0",
    "actual_layer": "L3"
}

# 自动调整Head权重
adjust_head_weights(user_feedback)
```

#### 方案2：元学习最优分层
```python
# 强化学习优化
for episode in episodes:
    compressed = compact(dialogue, current_bias)

    reward = (
        user_satisfaction +      # 用户满意度
        information_preservation - # 信息保留度
        compression_ratio        # 压缩率
    )

    current_bias = update_bias(reward)
```

#### 方案3：个性化偏置
```python
# 不同Agent有不同偏置
debugger_agent = {
    "head2_weight": 1.5,  # 更关注自己的经验
    "head3_weight": 0.8   # 较少关注环境
}

learner_agent = {
    "head1_weight": 1.5,  # 更关注用户指导
    "head2_weight": 1.2   # 更关注学习
}
```

### 3. 架构演进

#### 可能演进1：增加注意力头
```
当前：3头（上级、自我、环境）
未来：5头
  - Head1: 纠正注意力（专注用户纠正）
  - Head2: 指令注意力（专注用户指令）
  - Head3: 经验注意力（专注问题解决）
  - Head4: 配置注意力（专注环境配置）
  - Head5: 对话注意力（专注交互背景）
```

#### 可能演进2：分层注意力
```
L1注意力（粗粒度）：重要 vs 不重要
  ↓
L2注意力（细粒度）：L0/L1/L2/L3/L4
  ↓
最终输出
```

#### 可能演进3：神经符号混合
```
符号层（当前）：显式的三头设计
  +
神经层（未来）：可学习的注意力权重
  ↓
自适应压缩策略
```

## 评估指标

### 当前评估（手动）
- 压缩率：~50%
- 质量评分：98.7%（LLM-as-judge）
- 多头激活：100%（复合情况）

### 未来评估（自动）
1. **准确性**
   - 用户纠正保留率（目标：100%）
   - 关键配置保留率（目标：100%）
   - 经验复用成功率（需实际场景测试）

2. **效率**
   - 压缩率（目标：保持40-60%）
   - 压缩时间（目标：<5s）
   - 内存占用（目标：<1GB）

3. **泛化性**
   - 跨领域适应性
   - 多语言支持
   - 不同Agent类型适配

## 开放问题

### Q1: 三个头是否足够？
**当前**：基于Object理论设计
**疑问**：是否需要更多头覆盖更多语义维度？
**待验证**：实际使用中是否有漏网之鱼

### Q2: Max-Pooling是否最优？
**当前**：保守策略，避免信息丢失
**疑问**：是否导致L0过载？
**待验证**：其他融合策略（Weighted-Max, Soft-Max）

### Q3: 层级数量是否合理？
**当前**：5层（L0-L4）
**疑问**：是否太粗粒度或太细粒度？
**待验证**：3层或7层是否更好

### Q4: 如何处理模糊边界？
**当前**：依赖LLM语义理解
**疑问**：是否需要显式的边界判断规则？
**待验证**：混合方法（规则+语义）

## 实践建议

### 短期（当前使用）
1. ✅ 使用当前三头设计
2. ✅ 收集压缩失败案例
3. ✅ 记录用户纠正模式
4. ⚠️ 警惕L0过载问题

### 中期（1-3个月）
1. 分析失败案例，总结模式
2. 调整Head语义范围
3. 实验不同融合策略
4. A/B测试优化效果

### 长期（3-6个月）
1. 引入自适应学习
2. 个性化归纳偏置
3. 神经符号混合架构
4. 多Agent协同优化

## 版本历史

### v1.0 (2025-10-14)
- ✅ 建立三头注意力机制
- ✅ 定义Head1/2/3职责
- ✅ 实现Max-Pooling融合
- ✅ 验证多头并行工作
- ✅ 创建测试套件
- 📝 文档化设计决策

### v2.0 (未来)
- 🔧 根据实践反馈调整归纳偏置
- 🔧 优化融合策略
- 🔧 可能引入自适应学习
- 🔧 可能重构Head数量和职责

## 参考文献

1. **Transformer Multi-Head Attention**
   - Vaswani et al., "Attention Is All You Need" (2017)
   - 启发：并行多头处理

2. **归纳偏置设计**
   - Mitchell, "The Need for Biases in Learning Generalizations" (1980)
   - 指导：显式先验知识的价值

3. **香农信息论**
   - Shannon, "A Mathematical Theory of Communication" (1948)
   - 基础：重要性分层的理论依据

4. **元学习**
   - Finn et al., "Model-Agnostic Meta-Learning" (2017)
   - 未来：自适应优化方向

## 附录：测试文件

- `tests/test_compact_prompt.py` - 基础压缩质量测试
- `tests/test_connectionism.py` - 语义泛化能力测试
- `tests/test_multihead_attention.py` - 多头机制验证测试

---

**总结**：当前的三头注意力设计在理论上是正确的，已通过测试验证。归纳偏置基于合理假设（Object关系），但需要在实践中根据实际效果微调。设计保持开放和可演进，为未来优化留有空间。
