{
  "id": "tool_write_file_20250818_191250",
  "timestamp": "2025-08-18T19:12:50.496928",
  "event": "tool_write_file",
  "data": {
    "tool": "write_file",
    "arguments": {
      "file_path": "web_scraper.py",
      "content": "import requests\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urljoin, urlparse\nimport time\n\n\nclass WebScraper:\n    \"\"\"\n    一个简单的Web爬虫工具类\n    支持获取网页标题和链接\n    \"\"\"\n    \n    def __init__(self, delay=1):\n        \"\"\"\n        初始化爬虫\n        :param delay: 请求间隔时间（秒）\n        \"\"\"\n        self.delay = delay\n        self.session = requests.Session()\n        # 设置User-Agent避免被某些网站屏蔽\n        self.session.headers.update({\n            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n        })\n    \n    def get_page(self, url):\n        \"\"\"\n        获取网页内容\n        :param url: 目标网址\n        :return: BeautifulSoup对象\n        \"\"\"\n        try:\n            response = self.session.get(url, timeout=10)\n            response.raise_for_status()  # 检查请求是否成功\n            response.encoding = response.apparent_encoding  # 自动检测编码\n            soup = BeautifulSoup(response.text, 'html.parser')\n            time.sleep(self.delay)  # 延时避免请求过快\n            return soup\n        except requests.RequestException as e:\n            print(f\"请求错误: {e}\")\n            return None\n        except Exception as e:\n            print(f\"解析错误: {e}\")\n            return None\n    \n    def get_title(self, url):\n        \"\"\"\n        获取网页标题\n        :param url: 目标网址\n        :return: 网页标题字符串\n        \"\"\"\n        soup = self.get_page(url)\n        if soup:\n            title_tag = soup.find('title')\n            if title_tag:\n                return title_tag.get_text().strip()\n            else:\n                # 尝试获取H1标签作为备选标题\n                h1_tag = soup.find('h1')\n                if h1_tag:\n                    return h1_tag.get_text().strip()\n                return \"未找到标题\"\n        return None\n    \n    def get_links(self, url):\n        \"\"\"\n        获取网页中的所有链接\n        :param url: 目标网址\n        :return: 链接列表\n        \"\"\"\n        soup = self.get_page(url)\n        if not soup:\n            return []\n        \n        links = []\n        for link in soup.find_all('a', href=True):\n            href = link['href']\n            # 处理相对链接\n            absolute_url = urljoin(url, href)\n            links.append({\n                'text': link.get_text().strip(),\n                'url': absolute_url\n            })\n        return links\n    \n    def get_internal_links(self, url):\n        \"\"\"\n        获取网页中的内部链接（同域名）\n        :param url: 目标网址\n        :return: 内部链接列表\n        \"\"\"\n        links = self.get_links(url)\n        base_domain = urlparse(url).netloc\n        internal_links = []\n        \n        for link in links:\n            link_domain = urlparse(link['url']).netloc\n            if link_domain == base_domain:\n                internal_links.append(link)\n        \n        return internal_links\n    \n    def get_external_links(self, url):\n        \"\"\"\n        获取网页中的外部链接（不同域名）\n        :param url: 目标网址\n        :return: 外部链接列表\n        \"\"\"\n        links = self.get_links(url)\n        base_domain = urlparse(url).netloc\n        external_links = []\n        \n        for link in links:\n            link_domain = urlparse(link['url']).netloc\n            if link_domain and link_domain != base_domain:\n                external_links.append(link)\n        \n        return external_links"
    },
    "round": 1
  }
}