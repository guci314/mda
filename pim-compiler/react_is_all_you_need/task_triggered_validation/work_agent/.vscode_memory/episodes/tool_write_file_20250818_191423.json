{
  "id": "tool_write_file_20250818_191423",
  "timestamp": "2025-08-18T19:14:23.588176",
  "event": "tool_write_file",
  "data": {
    "tool": "write_file",
    "arguments": {
      "content": "import unittest\nfrom web_scraper import WebScraper\nfrom unittest.mock import Mock, patch\n\n\nclass TestWebScraper(unittest.TestCase):\n    \"\"\"\n    WebScraper类的单元测试\n    \"\"\"\n    \n    def setUp(self):\n        \"\"\"\n        测试前的准备工作\n        \"\"\"\n        self.scraper = WebScraper(delay=0)  # 设置延时为0以便快速测试\n    \n    def test_init(self):\n        \"\"\"\n        测试初始化\n        \"\"\"\n        self.assertIsInstance(self.scraper, WebScraper)\n        self.assertEqual(self.scraper.delay, 0)\n        self.assertIsNotNone(self.scraper.session)\n    \n    @patch('web_scraper.requests.Session.get')\n    def test_get_page_success(self, mock_get):\n        \"\"\"\n        测试成功获取页面\n        \"\"\"\n        # 模拟成功的HTTP响应\n        mock_response = Mock()\n        mock_response.text = '<html><title>Test</title></html>'\n        mock_response.apparent_encoding = 'utf-8'\n        mock_response.raise_for_status.return_value = None\n        mock_get.return_value = mock_response\n        \n        soup = self.scraper.get_page('http://example.com')\n        self.assertIsNotNone(soup)\n        self.assertEqual(soup.title.string, 'Test')\n    \n    @patch('web_scraper.requests.Session.get')\n    def test_get_page_failure(self, mock_get):\n        \"\"\"\n        测试获取页面失败的情况\n        \"\"\"\n        # 模拟请求异常\n        mock_get.side_effect = Exception(\"Network error\")\n        \n        soup = self.scraper.get_page('http://example.com')\n        self.assertIsNone(soup)\n    \n    @patch('web_scraper.WebScraper.get_page')\n    def test_get_title(self, mock_get_page):\n        \"\"\"\n        测试获取标题\n        \"\"\"\n        # 模拟BeautifulSoup对象\n        mock_soup = Mock()\n        mock_title = Mock()\n        mock_title.get_text.return_value = 'Test Title'\n        mock_soup.find.return_value = mock_title\n        mock_get_page.return_value = mock_soup\n        \n        title = self.scraper.get_title('http://example.com')\n        self.assertEqual(title, 'Test Title')\n    \n    @patch('web_scraper.WebScraper.get_page')\n    def test_get_links(self, mock_get_page):\n        \"\"\"\n        测试获取链接\n        \"\"\"\n        # 模拟BeautifulSoup对象\n        mock_soup = Mock()\n        \n        # 创建模拟的链接标签对象\n        mock_link1 = Mock()\n        mock_link1.__getitem__ = Mock(return_value='/page1')\n        mock_link1.get_text.return_value = 'Page 1'\n        mock_link1.attrs = {'href': '/page1'}\n        \n        mock_link2 = Mock()\n        mock_link2.__getitem__ = Mock(return_value='http://external.com/page2')\n        mock_link2.get_text.return_value = 'Page 2'\n        mock_link2.attrs = {'href': 'http://external.com/page2'}\n        \n        # 设置find_all方法的返回值\n        mock_soup.find_all.return_value = [mock_link1, mock_link2]\n        mock_get_page.return_value = mock_soup\n        \n        links = self.scraper.get_links('http://example.com')\n        self.assertEqual(len(links), 2)\n        self.assertEqual(links[0]['text'], 'Page 1')\n        self.assertEqual(links[0]['url'], 'http://example.com/page1')\n        self.assertEqual(links[1]['text'], 'Page 2')\n        self.assertEqual(links[1]['url'], 'http://external.com/page2')\n\n\nif __name__ == '__main__':\n    unittest.main()",
      "file_path": "test_scraper.py"
    },
    "round": 12
  }
}