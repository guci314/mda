{
  "id": "380f18b025ef959a",
  "timestamp": "2025-08-19T22:00:32.335135",
  "metadata": {
    "role": "tool",
    "round": 32,
    "importance": null
  },
  "views": {
    "FULL": {
      "content": {
        "role": "tool",
        "tool_call_id": "call_69eb0bc76d454a3f81fd39cc",
        "name": "read_file",
        "content": "#!/usr/bin/env python3\n\"\"\"\næœ€ç»ˆä¿®å¤ç‰ˆçš„4å±‚Agentè®°å¿†ç³»ç»Ÿæµ‹è¯•\nç¡®ä¿æ‰€æœ‰æ–‡ä»¶éƒ½å†™å…¥æ­£ç¡®çš„ç›®å½•\n\"\"\"\n\nimport os\nimport sys\nimport json\nfrom pathlib import Path\nfrom datetime import datetime\n\n# æ·»åŠ é¡¹ç›®è·¯å¾„\ncurrent_dir = Path(__file__).parent\nif str(current_dir) not in sys.path:\n    sys.path.insert(0, str(current_dir))\n\n# åŠ è½½.envæ–‡ä»¶\nenv_file = Path(__file__).parent.parent / \".env\"\nif env_file.exists():\n    from dotenv import load_dotenv\n    load_dotenv(env_file)\n    print(\"âœ… å·²åŠ è½½.envé…ç½®æ–‡ä»¶\")\n\n# å¯¼å…¥æ ¸å¿ƒæ¨¡å—\nfrom core.react_agent import ReactAgent  # type: ignore\nfrom core.memory_manager import MemoryMode  # type: ignore\n\n\ndef setup_4layer_agents(work_dir: Path):\n    \"\"\"è®¾ç½®4å±‚Agentç³»ç»Ÿ\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"ğŸ›ï¸ è®¾ç½®4å±‚Agentè®°å¿†ç³»ç»Ÿ\")\n    print(\"=\"*60)\n    \n    knowledge_base = Path(__file__).parent / \"knowledge\" / \"memory\" / \"agents\"\n    agents = {}\n    \n    # 1. å·¥ä½œAgent\n    print(\"\\n1ï¸âƒ£ åˆ›å»ºå·¥ä½œAgent...\")\n    agents['work'] = ReactAgent(\n        work_dir=str(work_dir / \"work_agent\"),\n        model=\"qwen/qwen3-coder\",\n        memory_mode=MemoryMode.BASIC,\n        max_rounds=10,\n        interface=\"å·¥ä½œAgent - æ‰§è¡Œå…·ä½“ä»»åŠ¡\"\n    )\n    print(\"  âœ… å·¥ä½œAgentå·²åˆ›å»º\")\n    \n    # 2. è§‚å¯Ÿè€…Agent\n    print(\"\\n2ï¸âƒ£ åˆ›å»ºè§‚å¯Ÿè€…Agent...\")\n    agents['observer'] = ReactAgent(\n        work_dir=str(work_dir / \"observer\"),\n        model=\"qwen/qwen3-coder\",\n        knowledge_files=[str(knowledge_base / \"memory_observer.md\")],\n        memory_mode=MemoryMode.FULL_ASYNC,\n        max_rounds=15,\n        interface=\"è§‚å¯Ÿè€…Agent - è§‚å¯Ÿå’Œè®°å½•\"\n    )\n    print(\"  âœ… è§‚å¯Ÿè€…Agentå·²åˆ›å»º\")\n    \n    # 3. æµ·é©¬ä½“Agent\n    print(\"\\n3ï¸âƒ£ åˆ›å»ºæµ·é©¬ä½“Agent...\")\n    agents['hippocampus'] = ReactAgent(\n        work_dir=str(work_dir / \"hippocampus\"),\n        model=\"qwen/qwen3-coder\",\n        knowledge_files=[str(knowledge_base / \"hippocampus.md\")],\n        memory_mode=MemoryMode.HYBRID,\n        max_rounds=20,  # å¢åŠ è½®æ•°ç¡®ä¿å®Œæˆ\n        interface=\"æµ·é©¬ä½“Agent - è®°å¿†å·©å›º\"\n    )\n    print(\"  âœ… æµ·é©¬ä½“Agentå·²åˆ›å»º\")\n    \n    # 4. å…ƒè®¤çŸ¥Agent\n    print(\"\\n4ï¸âƒ£ åˆ›å»ºå…ƒè®¤çŸ¥Agent...\")\n    agents['metacognition'] = ReactAgent(\n        work_dir=str(work_dir / \"metacognition\"),\n        model=\"qwen/qwen3-coder\",\n        knowledge_files=[str(knowledge_base / \"metacognition.md\")],\n        memory_mode=MemoryMode.FULL_ASYNC,\n        max_rounds=20,\n        interface=\"å…ƒè®¤çŸ¥Agent - ç³»ç»Ÿåæ€\"\n    )\n    print(\"  âœ… å…ƒè®¤çŸ¥Agentå·²åˆ›å»º\")\n    \n    return agents\n\n\ndef test_layer_by_layer(agents: dict, work_dir: Path):\n    \"\"\"é€å±‚æµ‹è¯•Agentç³»ç»Ÿ\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"ğŸ§ª é€å±‚æµ‹è¯•AgentåŠŸèƒ½\")\n    print(\"=\"*60)\n    \n    # Layer 1: å·¥ä½œAgentæ‰§è¡Œç®€å•ä»»åŠ¡\n    print(\"\\n### Layer 1: å·¥ä½œAgent\")\n    print(\"-\" * 40)\n    work_task = \"\"\"\nåˆ›å»ºä¸€ä¸ªç®€å•çš„è®¡ç®—å™¨æ¨¡å—ï¼š\n1. åˆ›å»ºcalculator.pyæ–‡ä»¶\n2. å®ç°åŠ å‡ä¹˜é™¤å››ä¸ªå‡½æ•°\n3. åˆ›å»ºtest_calculator.pyæµ‹è¯•æ–‡ä»¶\n\"\"\"\n    \n    print(\"æ‰§è¡Œä»»åŠ¡...\")\n    work_result = agents['work'].execute_task(work_task)\n    print(f\"âœ… å·¥ä½œå®Œæˆï¼Œæ‰§è¡Œäº†{agents['work'].stats['total_rounds']}è½®\")\n    \n    # ä¿å­˜å·¥ä½œè®°å½•ä¾›è§‚å¯Ÿè€…åˆ†æ\n    work_log = {\n        \"task\": work_task,\n        \"rounds\": agents['work'].stats['total_rounds'],\n        \"tool_calls\": agents['work'].stats['tool_calls'],\n        \"files_created\": agents['work'].stats.get('files_created', []),\n        \"timestamp\": datetime.now().isoformat()\n    }\n    \n    work_log_file = work_dir / \"work_log.json\"\n    with open(work_log_file, 'w', encoding='utf-8') as f:\n        json.dump(work_log, f, ensure_ascii=False, indent=2)\n    \n    # Layer 2: è§‚å¯Ÿè€…åˆ†æå·¥ä½œè®°å½•\n    print(\"\\n### Layer 2: è§‚å¯Ÿè€…Agent\")\n    print(\"-\" * 40)\n    \n    # æ˜ç¡®æŒ‡å®šè¾“å‡ºè·¯å¾„\n    observation_file = work_dir / \"observer\" / \"observation.json\"\n    \n    observe_task = f\"\"\"\nåˆ†æå·¥ä½œAgentçš„æ‰§è¡Œè®°å½•å¹¶ç”Ÿæˆè§‚å¯ŸæŠ¥å‘Šã€‚\n\nå·¥ä½œè®°å½•æ–‡ä»¶ï¼š{work_log_file}\n\né‡è¦è¦æ±‚ï¼š\n1. ä½¿ç”¨read_fileè¯»å–å·¥ä½œè®°å½•æ–‡ä»¶\n2. åˆ†ææ‰§è¡Œæ¨¡å¼\n3. å¿…é¡»å°†observation.jsonä¿å­˜åˆ°ï¼š{observation_file}\n\nä½¿ç”¨ä»¥ä¸‹Pythonä»£ç ä¿å­˜æ–‡ä»¶ï¼š\n```python\nimport json\nobservation_data = {{\n    \"task_type\": \"...\",\n    \"execution_efficiency\": {{...}},\n    \"tools_used\": {{...}},\n    \"key_findings\": [...]\n}}\njson_str = json.dumps(observation_data, ensure_ascii=False, indent=2)\n# ç„¶åä½¿ç”¨write_fileä¿å­˜åˆ°ï¼š{observation_file}\n```\n\nç¡®ä¿æ–‡ä»¶è·¯å¾„å®Œæ•´ï¼š{observation_file}\n\"\"\"\n    \n    print(\"å¼€å§‹è§‚å¯Ÿ...\")\n    observe_result = agents['observer'].execute_task(observe_task)\n    print(f\"âœ… è§‚å¯Ÿå®Œæˆï¼Œæ‰§è¡Œäº†{agents['observer'].stats['total_rounds']}è½®\")\n    \n    # å¦‚æœæ–‡ä»¶è¢«å†™åˆ°æ ¹ç›®å½•ï¼Œç§»åŠ¨å®ƒ\n    if (work_dir / \"observation.json\").exists() and not observation_file.exists():\n        import shutil\n        shutil.move(str(work_dir / \"observation.json\"), str(observation_file))\n        print(\"  ğŸ“ å·²å°†observation.jsonç§»åŠ¨åˆ°æ­£ç¡®ä½ç½®\")\n    \n    # Layer 3: æµ·é©¬ä½“å·©å›º\n    print(\"\\n### Layer 3: æµ·é©¬ä½“Agent\")\n    print(\"-\" * 40)\n    \n    consolidation_file = work_dir / \"hippocampus\" / \"consolidation.json\"\n    \n    consolidate_task = f\"\"\"\nå¯¹è§‚å¯ŸæŠ¥å‘Šè¿›è¡Œæ·±åº¦å·©å›ºã€‚\n\né‡è¦è¯´æ˜ï¼šè§‚å¯ŸæŠ¥å‘Šå¯èƒ½åœ¨ä»¥ä¸‹ä½ç½®ä¹‹ä¸€ï¼š\n1. {observation_file}\n2. {work_dir / \"observation.json\"}\n3. {work_dir / \"observer\" / \"observation.json\"}\n\nè¯·æŒ‰é¡ºåºå°è¯•è¯»å–ï¼Œç›´åˆ°æ‰¾åˆ°æ–‡ä»¶ã€‚\n\nä»»åŠ¡è¦æ±‚ï¼š\n1. æ‰¾åˆ°å¹¶è¯»å–observation.json\n2. æå–æ ¸å¿ƒæ¨¡å¼å’ŒçŸ¥è¯†\n3. å¿…é¡»å°†consolidation.jsonä¿å­˜åˆ°ï¼š{consolidation_file}\n\nä½¿ç”¨ä»¥ä¸‹ä»£ç ä¿å­˜ï¼š\n```python\nimport json\nconsolidation_data = {{\n    \"core_knowledge\": {{...}},\n    \"reusable_patterns\": {{...}},\n    \"lessons_learned\": {{...}}\n}}\njson_str = json.dumps(consolidation_data, ensure_ascii=False, indent=2)\n# ä½¿ç”¨write_fileä¿å­˜åˆ°ï¼š{consolidation_file}\n```\n\nç¡®ä¿è¾“å‡ºè·¯å¾„ï¼š{consolidation_file}\n\"\"\"\n    \n    print(\"å¼€å§‹å·©å›º...\")\n    consolidate_result = agents['hippocampus'].execute_task(consolidate_task)\n    print(f\"âœ… å·©å›ºå®Œæˆï¼Œæ‰§è¡Œäº†{agents['hippocampus'].stats['total_rounds']}è½®\")\n    \n    # å¦‚æœæ–‡ä»¶è¢«å†™åˆ°å…¶ä»–ä½ç½®ï¼Œç§»åŠ¨å®ƒ\n    for possible_path in [work_dir / \"consolidation.json\", \n                          work_dir / \"hippocampus\" / \"consolidation.json\"]:\n        if possible_path.exists() and possible_path != consolidation_file:\n            if not consolidation_file.exists():\n                import shutil\n                shutil.move(str(possible_path), str(consolidation_file))\n                print(\"  ğŸ“ å·²å°†consolidation.jsonç§»åŠ¨åˆ°æ­£ç¡®ä½ç½®\")\n                break\n    \n    # Layer 4: å…ƒè®¤çŸ¥åæ€\n    print(\"\\n### Layer 4: å…ƒè®¤çŸ¥Agent\")\n    print(\"-\" * 40)\n    \n    reflection_file = work_dir / \"metacognition\" / \"reflection.json\"\n    \n    reflect_task = f\"\"\"\nå¯¹æ•´ä¸ªè®°å¿†ç³»ç»Ÿè¿›è¡Œå…ƒè®¤çŸ¥åæ€ã€‚\n\nç³»ç»Ÿå·¥ä½œç›®å½•ï¼š{work_dir}\n\næ–‡ä»¶ä½ç½®è¯´æ˜ï¼š\n- observation.jsonä½äºï¼š{observation_file}\n- consolidation.jsonä½äºï¼š{consolidation_file}\n- work_log.jsonä½äºï¼š{work_log_file}\n\nä»»åŠ¡è¦æ±‚ï¼š\n1. è¯»å–ä¸Šè¿°æ‰€æœ‰æŠ¥å‘Šæ–‡ä»¶\n2. è¯„ä¼°å„å±‚Agentçš„è¡¨ç°\n3. å¿…é¡»å°†reflection.jsonä¿å­˜åˆ°ï¼š{reflection_file}\n\nä½¿ç”¨ä»¥ä¸‹ä»£ç ï¼š\n```python\nimport json\nreflection_data = {{\n    \"system_efficiency\": {{...}},\n    \"strengths\": [...],\n    \"weaknesses\": [...],\n    \"improvement_suggestions\": [...],\n    \"future_directions\": [...]\n}}\njson_str = json.dumps(reflection_data, ensure_ascii=False, indent=2)\n# ä½¿ç”¨write_fileä¿å­˜åˆ°ï¼š{reflection_file}\n```\n\nç¡®ä¿è¾“å‡ºåˆ°ï¼š{reflection_file}\n\"\"\"\n    \n    print(\"å¼€å§‹åæ€...\")\n    reflect_result = agents['metacognition'].execute_task(reflect_task)\n    print(f\"âœ… åæ€å®Œæˆï¼Œæ‰§è¡Œäº†{agents['metacognition'].stats['total_rounds']}è½®\")\n    \n    # å¦‚æœæ–‡ä»¶è¢«å†™åˆ°æ ¹ç›®å½•ï¼Œç§»åŠ¨å®ƒ\n    if (work_dir / \"reflection.json\").exists() and not reflection_file.exists():\n        import shutil\n        shutil.move(str(work_dir / \"reflection.json\"), str(reflection_file))\n        print(\"  ğŸ“ å·²å°†reflection.jsonç§»åŠ¨åˆ°æ­£ç¡®ä½ç½®\")\n    \n    return True\n\n\ndef check_results(work_dir: Path):\n    \"\"\"æ£€æŸ¥æµ‹è¯•ç»“æœ\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"ğŸ“Š æµ‹è¯•ç»“æœæ£€æŸ¥\")\n    print(\"=\"*60)\n    \n    # æ£€æŸ¥å„å±‚è¾“å‡º\n    layers = [\n        (\"å·¥ä½œAgent\", work_dir / \"work_agent\", [\"calculator.py\", \"test_calculator.py\"]),\n        (\"è§‚å¯Ÿè€…\", work_dir / \"observer\", [\"observation.json\"]),\n        (\"æµ·é©¬ä½“\", work_dir / \"hippocampus\", [\"consolidation.json\"]),\n        (\"å…ƒè®¤çŸ¥\", work_dir / \"metacognition\", [\"reflection.json\"])\n    ]\n    \n    all_good = True\n    for name, path, expected_files in layers:\n        print(f\"\\n{name}:\")\n        if path.exists():\n            files = list(path.iterdir())\n            print(f\"  ç›®å½•å­˜åœ¨ âœ…\")\n            print(f\"  æ–‡ä»¶æ•°é‡: {len([f for f in files if f.is_file()])}\")\n            \n            # æ£€æŸ¥é¢„æœŸæ–‡ä»¶\n            for expected in expected_files:\n                file_path = path / expected\n                if file_path.exists():\n                    print(f\"  {expected} âœ…\")\n                    # å¦‚æœæ˜¯JSONæ–‡ä»¶ï¼ŒéªŒè¯æ ¼å¼\n                    if expected.endswith('.json'):\n                        try:\n                            with open(file_path, 'r', encoding='utf-8') as f:\n                                data = json.load(f)\n                                print(f\"    JSONæ ¼å¼æœ‰æ•ˆ âœ…\")\n                                print(f\"    åŒ…å«{len(data)}ä¸ªé¡¶çº§é”®\")\n                        except json.JSONDecodeError as e:\n                            print(f\"    JSONæ ¼å¼é”™è¯¯ âŒ: {e}\")\n                            all_good = False\n                else:\n                    print(f\"  {expected} âŒ æœªæ‰¾åˆ°\")\n                    # æ£€æŸ¥æ˜¯å¦åœ¨å…¶ä»–ä½ç½®\n                    alt_path = work_dir / expected\n                    if alt_path.exists():\n                        print(f\"    ï¼ˆåœ¨æ ¹ç›®å½•æ‰¾åˆ°ï¼Œéœ€è¦ç§»åŠ¨ï¼‰\")\n                    all_good = False\n            \n            # æ£€æŸ¥è®°å¿†ç³»ç»Ÿ\n            memory_dir = path / \".vscode_memory\"\n            if memory_dir.exists():\n                memory_files = list(memory_dir.rglob(\"*.json\"))\n                print(f\"  è®°å¿†ç³»ç»Ÿ: âœ… ({len(memory_files)}ä¸ªæ–‡ä»¶)\")\n            else:\n                print(f\"  è®°å¿†ç³»ç»Ÿ: âŒ æœªæ¿€æ´»\")\n        else:\n            print(f\"  ç›®å½•ä¸å­˜åœ¨ âŒ\")\n            all_good = False\n    \n    # æ˜¾ç¤ºå„å±‚æŠ¥å‘Šå†…å®¹æ‘˜è¦\n    print(\"\\n\" + \"=\"*60)\n    print(\"ğŸ“ æŠ¥å‘Šå†…å®¹æ‘˜è¦\")\n    print(\"=\"*60)\n    \n    # è§‚å¯ŸæŠ¥å‘Š\n    obs_file = work_dir / \"observer\" / \"observation.json\"\n    if obs_file.exists():\n        try:\n            with open(obs_file, 'r', encoding='utf-8') as f:\n                obs_data = json.load(f)\n                print(\"\\nè§‚å¯ŸæŠ¥å‘Š:\")\n                print(f\"  ä»»åŠ¡ç±»å‹: {obs_data.get('task_type', 'N/A')}\")\n                print(f\"  å…³é”®å‘ç°: {len(obs_data.get('key_findings', []))}é¡¹\")\n        except:\n            pass\n    \n    # å·©å›ºæŠ¥å‘Š\n    cons_file = work_dir / \"hippocampus\" / \"consolidation.json\"\n    if cons_file.exists():\n        try:\n            with open(cons_file, 'r', encoding='utf-8') as f:\n                cons_data = json.load(f)\n                print(\"\\nå·©å›ºæŠ¥å‘Š:\")\n                print(f\"  æ ¸å¿ƒçŸ¥è¯†: {len(cons_data.get('core_knowledge', {}))}é¡¹\")\n                print(f\"  å¯å¤ç”¨æ¨¡å¼: {len(cons_data.get('reusable_patterns', {}))}é¡¹\")\n        except:\n            pass\n    \n    # åæ€æŠ¥å‘Š\n    ref_file = work_dir / \"metacognition\" / \"reflection.json\"\n    if ref_file.exists():\n        try:\n            with open(ref_file, 'r', encoding='utf-8') as f:\n                ref_data = json.load(f)\n                print(\"\\nåæ€æŠ¥å‘Š:\")\n                print(f\"  ç³»ç»Ÿä¼˜åŠ¿: {len(ref_data.get('strengths', []))}é¡¹\")\n                print(f\"  æ”¹è¿›å»ºè®®: {len(ref_data.get('improvement_suggestions', []))}é¡¹\")\n        except:\n            pass\n    \n    return all_good\n\n\ndef fix_file_locations(work_dir: Path):\n    \"\"\"ä¿®å¤æ–‡ä»¶ä½ç½® - å°†æ ¹ç›®å½•çš„æ–‡ä»¶ç§»åŠ¨åˆ°æ­£ç¡®ä½ç½®\"\"\"\n    fixes_made = []\n    \n    # å®šä¹‰æ–‡ä»¶æ˜ å°„\n    file_mappings = [\n        (\"observation.json\", work_dir / \"observer\" / \"observation.json\"),\n        (\"consolidation.json\", work_dir / \"hippocampus\" / \"consolidation.json\"),\n        (\"reflection.json\", work_dir / \"metacognition\" / \"reflection.json\")\n    ]\n    \n    for filename, target_path in file_mappings:\n        source_path = work_dir / filename\n        if source_path.exists() and not target_path.exists():\n            import shutil\n            target_path.parent.mkdir(parents=True, exist_ok=True)\n            shutil.move(str(source_path), str(target_path))\n            fixes_made.append(f\"{filename} â†’ {target_path.parent.name}/{filename}\")\n    \n    if fixes_made:\n        print(\"\\nğŸ“ æ–‡ä»¶ä½ç½®è‡ªåŠ¨ä¿®å¤:\")\n        for fix in fixes_made:\n            print(f\"  âœ… {fix}\")\n    \n    return len(fixes_made) > 0\n\n\ndef main():\n    \"\"\"ä¸»å‡½æ•°\"\"\"\n    work_dir = Path(__file__).parent / \"test_4layer_final\"\n    \n    # æ¸…ç†æ—§ç›®å½•\n    if work_dir.exists():\n        import shutil\n        shutil.rmtree(work_dir)\n    work_dir.mkdir(parents=True, exist_ok=True)\n    \n    try:\n        # è®¾ç½®Agent\n        agents = setup_4layer_agents(work_dir)\n        \n        # é€å±‚æµ‹è¯•\n        success = test_layer_by_layer(agents, work_dir)\n        \n        # è‡ªåŠ¨ä¿®å¤æ–‡ä»¶ä½ç½®\n        fix_file_locations(work_dir)\n        \n        # æ£€æŸ¥ç»“æœ\n        all_good = check_results(work_dir)\n        \n        # æœ€ç»ˆæŠ¥å‘Š\n        print(\"\\n\" + \"=\"*60)\n        if all_good:\n            print(\"ğŸ‰ 4å±‚Agentç³»ç»Ÿæµ‹è¯•å®Œå…¨æˆåŠŸï¼\")\n            print(\"\\néªŒè¯çš„åŠŸèƒ½ï¼š\")\n            print(\"  âœ… L1-å·¥ä½œAgentï¼šæ‰§è¡Œä»»åŠ¡\")\n            print(\"  âœ… L2-è§‚å¯Ÿè€…Agentï¼šåˆ†æè®°å½•\")\n            print(\"  âœ… L3-æµ·é©¬ä½“Agentï¼šå·©å›ºçŸ¥è¯†\")\n            print(\"  âœ… L4-å…ƒè®¤çŸ¥Agentï¼šç³»ç»Ÿåæ€\")\n            print(\"  âœ… æ‰€æœ‰æ–‡ä»¶åœ¨æ­£ç¡®ä½ç½®\")\n            print(\"  âœ… æ‰€æœ‰JSONæ ¼å¼æ­£ç¡®\")\n            print(\"  âœ… å„å±‚è®°å¿†ç³»ç»Ÿæ­£å¸¸å·¥ä½œ\")\n        else:\n            print(\"âš ï¸ æµ‹è¯•éƒ¨åˆ†æˆåŠŸ\")\n            print(\"\\nè‡ªåŠ¨ä¿®å¤å·²å°è¯•å°†æ–‡ä»¶ç§»åŠ¨åˆ°æ­£ç¡®ä½ç½®\")\n            print(\"è¯·æ£€æŸ¥ä¸Šè¿°è¾“å‡ºäº†è§£è¯¦æƒ…\")\n        print(\"=\"*60)\n        \n    except Exception as e:\n        print(f\"\\nâŒ æµ‹è¯•å¤±è´¥: {e}\")\n        import traceback\n        traceback.print_exc()\n\n\nif __name__ == \"__main__\":\n    print(f\"[{datetime.now()}] å¼€å§‹4å±‚Agentç³»ç»Ÿæµ‹è¯•ï¼ˆæœ€ç»ˆä¿®å¤ç‰ˆï¼‰...\")\n    \n    if not os.getenv(\"OPENROUTER_API_KEY\"):\n        print(\"âŒ è¯·è®¾ç½®OPENROUTER_API_KEYç¯å¢ƒå˜é‡\")\n        sys.exit(1)\n    \n    main()"
      },
      "tokens": 4797,
      "generated_at": "2025-08-19T22:00:32.335942"
    },
    "MINIMAL": {
      "content": {
        "marker": "[tool:result]"
      },
      "tokens": 5,
      "generated_at": "2025-08-19T22:00:32.335959"
    },
    "HIGH": {
      "content": {
        "role": "tool",
        "content": "#!/usr/bin/env python3\n\"\"\"\næœ€ç»ˆä¿®å¤ç‰ˆçš„4å±‚Agentè®°å¿†ç³»ç»Ÿæµ‹è¯•\nç¡®ä¿æ‰€æœ‰æ–‡ä»¶éƒ½å†™å…¥æ­£ç¡®çš„ç›®å½•\n\"\"\"\n\nimport os\nimport sys\nimport json\nfrom pathlib import Path\nfrom datetime import datetime\n\n# æ·»åŠ é¡¹ç›®è·¯å¾„\ncurrent_dir = Path(__file__).parent\nif str(current_dir) not in sys.path:\n    sys.path.insert(0, str(current_dir))\n\n# åŠ è½½.envæ–‡ä»¶\nenv_file = Path(__file__).parent.parent / \".env\"\nif env_file.exists():\n    from dotenv import load_dotenv\n    load_dotenv(env_file)\n    print(\"âœ… å·²åŠ è½½.envé…ç½®æ–‡ä»¶\")\n\n# å¯¼å…¥æ ¸å¿ƒæ¨¡å—\nfrom core.react_agent import ReactAgent  # type: ignore\nfrom core.memory_manager import MemoryMode  # type: ignore\n\n\ndef setup_4layer_agents\n...[éƒ¨åˆ†å†…å®¹çœç•¥]...\n= \"__main__\":\n    print(f\"[{datetime.now()}] å¼€å§‹4å±‚Agentç³»ç»Ÿæµ‹è¯•ï¼ˆæœ€ç»ˆä¿®å¤ç‰ˆï¼‰...\")\n    \n    if not os.getenv(\"OPENROUTER_API_KEY\"):\n        print(\"âŒ è¯·è®¾ç½®OPENROUTER_API_KEYç¯å¢ƒå˜é‡\")\n        sys.exit(1)\n    \n    main()"
      },
      "tokens": 324,
      "generated_at": "2025-08-19T22:00:32.341013"
    },
    "MEDIUM": {
      "content": {
        "role": "tool",
        "content_preview": "#!/usr/bin/env python3\n\"\"\"\næœ€ç»ˆä¿®å¤ç‰ˆçš„4å±‚Agentè®°å¿†ç³»ç»Ÿæµ‹è¯•\nç¡®ä¿æ‰€æœ‰æ–‡ä»¶éƒ½å†™å…¥æ­£ç¡®çš„ç›®å½•\n\"\"\"\n\nimport os\nimport sys\nimport json\nfrom pathlib import Path\nfrom datetime import datetime\n\n# æ·»åŠ é¡¹ç›®è·¯å¾„\ncurrent_dir = Path(__file__).parent\nif str(current_dir) not in sys.path:\n    sys.path.insert(0, str(current_dir))\n\n# åŠ è½½.envæ–‡ä»¶\nenv_file ..."
      },
      "tokens": 137,
      "generated_at": "2025-08-19T22:00:32.341052"
    },
    "LOW": {
      "content": {
        "role": "tool",
        "summary": "å·¥å…·æ‰§è¡Œå‡ºé”™"
      },
      "tokens": 16,
      "generated_at": "2025-08-19T22:00:32.341173"
    }
  }
}