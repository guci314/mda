# Compact提示词单元测试

## 目的

快速验证 `compact_prompt.md` 的有效性，缩短反馈周期。

## 设计原理

### 1. 测试数据集
构建5种典型对话场景：
- **用户纠正**：测试L0约束保留
- **配置信息**：测试L1配置精简
- **问题解决**：测试L2经验提取
- **冗余对话**：测试L4删除能力
- **综合场景**：测试整体压缩质量

### 2. 评估维度（LLM-as-judge）
使用另一个LLM评估压缩质量，6个维度各10分：
1. **约束保留度**（L0）- 用户纠正是否100%保留
2. **配置保留度**（L1）- 关键配置是否精简保留
3. **经验提取度**（L2）- 问题方案是否提取精华
4. **冗余消除度**（L4）- 无关内容是否删除
5. **结构清晰度** - 是否按L0-L3分层
6. **符合期望** - 是否满足测试用例期望

总分60分，≥70%为通过。

### 3. 自动化流程
```
测试用例 → 压缩API → 评估API → 生成报告
  (5个)      (快速)     (自动)     (JSON)
```

## 使用方法

### 1. 安装依赖
```bash
pip install requests
```

### 2. 设置API密钥
```bash
export DEEPSEEK_API_KEY=your_key_here
```

### 3. 运行测试
```bash
cd pim-compiler/react_is_all_you_need/tests
python test_compact_prompt.py
```

### 4. 查看结果
- 控制台输出：每个测试用例的压缩率和评分
- JSON报告：`compact_test_report.json`（包含详细结果）

## 输出示例

```
============================================================
🧪 Compact提示词单元测试
============================================================
  模型: deepseek-chat
  时间: 2025-01-13 10:30:00

============================================================
📋 测试用例: 用户纠正测试
============================================================
⏳ 正在压缩...
✅ 压缩完成
  - 原始长度: 450 字符
  - 压缩后长度: 180 字符
  - 压缩率: 40.0%
⏳ 正在评估...
✅ 评估完成
  - 总分: 52/60 (86.7%)
  - 详情: L0约束完整保留，标记清晰，冗余已删除

📄 压缩结果预览:
## L0 - 绝对约束
**[多次纠正]** 执行Python必须使用python3.12，不是python3

============================================================
📊 测试报告
============================================================

总体统计:
  - 测试用例数: 5
  - 平均压缩率: 45.2%
  - 平均评分: 81.5%

详细结果:
  ✅ 用户纠正测试
     压缩率: 40.0% | 评分: 86.7%
  ✅ 配置信息测试
     压缩率: 35.5% | 评分: 88.3%
  ✅ 问题解决测试
     压缩率: 50.2% | 评分: 75.0%
  ✅ 冗余消除测试
     压缩率: 85.0% | 评分: 90.0%
  ✅ 综合测试
     压缩率: 42.8% | 评分: 67.5%

============================================================
✅ 所有测试通过！(5/5)
============================================================

💾 详细报告已保存: compact_test_report.json
```

## 快速迭代流程

### 修改提示词后的验证流程：
```bash
# 1. 修改 compact_prompt.md
vim knowledge/minimal/system/compact_prompt.md

# 2. 运行测试（约30秒）
python tests/test_compact_prompt.py

# 3. 查看评分
# - 如果 ≥70%，提示词改进有效
# - 如果 <70%，继续调整

# 4. 查看详细报告（可选）
cat tests/compact_test_report.json | jq '.results[] | {name, score: .evaluation_score}'
```

## 添加自定义测试用例

在 `test_compact_prompt.py` 的 `run_test_suite()` 中添加：

```python
# 自定义测试用例
result_custom = self.test_case(
    name="自定义测试名称",
    dialogue_history=[
        {"role": "user", "content": "用户消息1"},
        {"role": "assistant", "content": "助手回复1"},
        # ... 更多对话
    ],
    description="Agent描述（用于注意力机制）",
    expectations={
        "l0_content": "L0应该包含什么",
        "l1_content": "L1应该包含什么",
        # ... 更多期望
    }
)
results.append(("自定义测试名称", result_custom))
```

## 成本估算

使用DeepSeek（¥1/百万tokens）：
- 每个测试用例：约2000 tokens（压缩+评估）
- 5个测试用例：约10000 tokens
- 单次运行成本：**约¥0.01**

## 优势

✅ **反馈快速**：30秒获得结果，vs 实践中数小时/天
✅ **成本低廉**：单次测试<¥0.02
✅ **可重复**：每次修改都能快速验证
✅ **可量化**：评分系统，不依赖主观判断
✅ **可追溯**：JSON报告保留所有历史结果

## 注意事项

1. **LLM评估的局限性**：
   - LLM-as-judge 可能有偏差
   - 建议：定期人工抽查验证

2. **测试数据的代表性**：
   - 当前5个测试用例覆盖主要场景
   - 建议：根据实际使用添加更多用例

3. **模型选择**：
   - 当前使用DeepSeek（便宜快速）
   - 建议：重要验证可用更强模型（如Claude）

## 进阶用法

### 对比不同版本的提示词

```python
# 1. 备份当前版本
cp compact_prompt.md compact_prompt_v1.md

# 2. 修改提示词
vim compact_prompt.md

# 3. 运行测试
python test_compact_prompt.py

# 4. 对比结果
# - 查看 compact_test_report.json
# - 对比评分和压缩率
```

### 批量测试多个版本

创建 `batch_test.py`：
```python
versions = ["v1", "v2", "v3"]
for v in versions:
    # 加载不同版本的提示词
    # 运行测试
    # 保存结果
```

## 贡献测试用例

如果你在实践中遇到特殊场景，欢迎贡献测试用例：
1. 在 `run_test_suite()` 中添加用例
2. 描述场景和期望
3. 验证通过后提交

## 许可

MIT License
