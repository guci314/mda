# LLM Provider Configuration
# Options: auto | gemini | anthropic | local
LLM_PROVIDER=auto

# Use LLM for all code generation (not just complex logic)
USE_LLM_FOR_ALL=false

# Gemini Configuration
# Get your API key from: https://aistudio.google.com/app/apikey
GOOGLE_AI_STUDIO_KEY=your-gemini-api-key-here

# Or use Vertex AI (requires Google Cloud project)
# VERTEX_AI_PROJECT=your-gcp-project-id

# Anthropic Configuration (Claude)
# Get your API key from: https://console.anthropic.com/
ANTHROPIC_API_KEY=your-anthropic-api-key-here

# Local LLM Configuration (Ollama)
# Only needed if using local LLM
LOCAL_LLM_URL=http://ollama:11434
LOCAL_LLM_MODEL=codellama:13b

# To use local LLM, run:
# docker compose --profile local-llm up